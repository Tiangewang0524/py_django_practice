2019-08-20 17:07:25 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 17:07:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 17:07:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'nba.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 17:07:25 [scrapy.extensions.telnet] INFO: Telnet Password: a8461f8a435b2c4e
2019-08-20 17:07:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 17:07:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 17:07:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 17:07:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 17:07:25 [scrapy.core.engine] INFO: Spider opened
2019-08-20 17:07:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 17:07:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 17:07:26 [root] INFO: {'player_contract': '4年2484万美元，2014年夏天签，2018年夏天到期，2016-17赛季、2017-18赛季球队选项；2017年10月以5年1.48亿美元提前续约，2018年夏天生效，2023年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '安德鲁-威金斯',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：2727万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '90公斤/199磅'}
2019-08-20 17:07:27 [root] INFO: {'player_contract': '5年1.39亿，2016年夏天签',
 'player_height': '2.01米/6尺7',
 'player_name': '德马尔-德罗赞',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：2773万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '4年2572万美元，2015年夏天签，2019年夏天到期。2018年夏天提前续约5年1.58亿美元，2019年夏天生效。',
 'player_height': '2.13米/7尺0',
 'player_name': '卡尔-安东尼-唐斯',
 'player_num': '32',
 'player_position': 'C-F',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '111公斤/244磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '3年5700万美元，2017年夏天签，2020年夏天到期，2019-2020赛季球员选项',
 'player_height': '1.88米/6尺2',
 'player_name': '杰夫-蒂格',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：1900万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '4年658万美元，2013年夏天签，2016年10月以4年6400万美元提前续约，2017年夏天生效，2021年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '戈尔吉-吉昂',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：1623万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '5年6160万美元，2017年11月签，2017年11月生效，2022年夏天到期，新合同包含将2017-18赛季薪金从157万美元提升至1670万美元',
 'player_height': '2.06米/6尺9',
 'player_name': '罗伯特-科温顿',
 'player_num': '33',
 'player_position': 'F',
 'player_salary': '本年薪金：1082万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '贾勒特-卡尔弗',
 'player_num': '23',
 'player_position': 'G-F',
 'player_salary': '本年薪金：581万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '3年1150万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '杰克-莱曼',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：365万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '4年1173万美元，2018年夏天签，2022年夏天到期，2020-21、2021-22赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '约什-奥科吉',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：253万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '97公斤/213磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '1年200万美元，2019年夏天签，2020年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '诺阿-冯莱',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：200万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '2年379万美元，2018年夏天签，2020年夏天到期，其中194万受保障，2019-20赛季球队选项且无保障',
 'player_height': '1.85米/6尺1',
 'player_name': '沙巴兹-内皮尔',
 'player_num': '13',
 'player_position': 'G',
 'player_salary': '本年薪金：185万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '2年316万美元，2018年夏天签，2020年夏天到期，其中151万受保障，2019-20赛季无保障',
 'player_height': '1.98米/6尺6',
 'player_name': '特雷韦恩-格雷厄姆',
 'player_num': '12',
 'player_position': 'F',
 'player_salary': '本年薪金：165万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '102公斤/226磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '1年160万美元，2019年夏天签，2020年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '乔丹-贝尔',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：160万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '2年226万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '凯塔-贝茨-迪奥普',
 'player_num': '31',
 'player_position': 'F',
 'player_salary': '本年薪金：142万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.91米/6尺3',
 'player_name': '贾里德-特里尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.96米/6尺5',
 'player_name': 'CJ-威廉姆斯',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '明尼苏达森林狼',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '4年8407万美元，2015年夏天签，2019年夏天到期，2018-19赛季球员选项；2017年10月以3年7230万美元提前续约，2018年夏天生效，2021年到期，如果2020年6月29日之前被裁',
 'player_height': '2.11米/6尺11',
 'player_name': '拉马库斯-阿尔德里奇',
 'player_num': '12',
 'player_position': 'F',
 'player_salary': '本年薪金：2600万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '2年3200万美元，2019年夏天签，2021年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '鲁迪-盖伊',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：1560万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '2年1100万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '特雷-莱尔斯',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：550万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '106公斤/234磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '4年1246万美元，2018年夏天签，2022年夏天到期，2020年和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '朗尼-沃克',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：276万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '93公斤/204磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '卢卡-沙马尼奇',
 'player_num': '19',
 'player_position': 'F',
 'player_salary': '本年薪金：268万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '103公斤/227磅'}
2019-08-20 17:07:28 [root] INFO: {'player_contract': '4年854万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '德里克-怀特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：195万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:29 [root] INFO: {'player_contract': '4年7095万美元，2017年夏天签，2021年夏天到期，15%交易保证金，2020-2021赛季球员选项',
 'player_height': '1.98米/6尺6',
 'player_name': '小蒂姆-哈达威',
 'player_num': '11',
 'player_position': 'F-G',
 'player_salary': '本年薪金：1815万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:30 [root] INFO: {'player_contract': '4年999万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项\n'
                    '2018年7月提前续约5年1.58亿美元，2019年夏天生效',
 'player_height': '1.98米/6尺6',
 'player_name': '德文-布克',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '93公斤/206磅'}
2019-08-20 17:07:31 [root] INFO: {'player_contract': '4年1.56亿美元，2018年夏天签，2022年到期，2021-22赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '勒布朗-詹姆斯',
 'player_num': '23',
 'player_position': 'F-G',
 'player_salary': '本年薪金：3744万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:32 [root] INFO: {'player_contract': '4年1.37亿美元，2018年夏天签，2022年夏天到期，2021年球员选项',
 'player_height': '2.06米/6尺9',
 'player_name': '保罗-乔治',
 'player_num': '21',
 'player_position': 'F',
 'player_salary': '本年薪金：3300万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:33 [root] INFO: {'player_contract': '4年1633万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '扎克-科林斯',
 'player_num': '33',
 'player_position': 'F-C',
 'player_salary': '本年薪金：424万美元',
 'player_team': '波特兰开拓者',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:34 [root] INFO: {'player_contract': '4年8800万美元，2019年夏天签，2023年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '哈里森-巴恩斯',
 'player_num': '40',
 'player_position': 'F',
 'player_salary': '本年薪金：1897万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:35 [root] INFO: {'player_contract': '5年2.01亿美元，2017年夏天签，2022年夏天到期，15%交易保证金',
 'player_height': '1.91米/6尺3',
 'player_name': '斯蒂芬-库里',
 'player_num': '30',
 'player_position': 'G',
 'player_salary': '本年薪金：4023万美元',
 'player_team': '金州勇士',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年4800万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '安德烈-伊格达拉',
 'player_num': '9',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1719万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年4800万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '考特尼-李',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：1276万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年3300万美元，2019年夏天签',
 'player_height': '2.11米/6尺11',
 'player_name': '德怀特-鲍威尔',
 'player_num': '7',
 'player_position': 'F-C',
 'player_salary': '本年薪金：956万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年3500万美元，2019年夏天签，2023年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '马克西-克勒贝尔',
 'player_num': '42',
 'player_position': 'F',
 'player_salary': '本年薪金：781万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年3246万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '卢卡-东契奇',
 'player_num': '77',
 'player_position': 'G-F',
 'player_salary': '本年薪金：768万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '103公斤/228磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1348万美元，2017年夏天签，2021年夏天到期，2019-20赛季、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '贾斯廷-杰克逊',
 'player_num': '44',
 'player_position': 'F',
 'player_salary': '本年薪金：328万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '三年 480万美元',
 'player_height': '2.03米/6尺8',
 'player_name': '以赛亚-罗比',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：150万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '104公斤/229磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年5000万美元，2016年夏天签，2020年夏天到期，2019-20赛季球员选项，15%的交易保证金',
 'player_height': '1.93米/6尺4',
 'player_name': '泰勒-约翰逊',
 'player_num': '16',
 'player_position': 'G',
 'player_salary': '本年薪金：1963万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年3000万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '凯利-乌布雷',
 'player_num': '3',
 'player_position': 'F',
 'player_salary': '本年薪金：1442万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年4037万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '德安德烈-艾顿',
 'player_num': '22',
 'player_position': 'C',
 'player_salary': '本年薪金：956万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '弗兰克-卡明斯基',
 'player_num': '8',
 'player_position': 'F-C',
 'player_salary': '本年薪金：476万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1075万美元，2016年夏天签，2020年夏天到期。',
 'player_height': '2.08米/6尺10',
 'player_name': '达里奥-沙里奇',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：348万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '101公斤/223磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年226万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '杰文-卡特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.98米/6尺6',
 'player_name': '乔治-金',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '5年1.3亿美元，2015年夏天签，2016年夏天生效，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.08米/6尺10',
 'player_name': '安东尼-戴维斯',
 'player_num': '3',
 'player_position': 'F-C',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '115公斤/253磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年3000万美元，2019年夏天签，2021年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '丹尼-格林',
 'player_num': '14',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1463万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年1600万美元，2019年夏天签，2021年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '肯塔维厄斯-考德威尔-波普',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：769万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年970万美元，2019年夏天签，2021年到期，2020年球员选项',
 'player_height': '1.88米/6尺2',
 'player_name': '埃弗里-布拉德利',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：476万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年820万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '贾维尔-麦基',
 'player_num': '7',
 'player_position': 'C',
 'player_salary': '本年薪金：394万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '122公斤/270磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年350万美元，2019年夏天签，2020年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '德马库斯-考辛斯',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：350万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '122公斤/270磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年600万美元，2019年夏天签，2021年到期',
 'player_height': '1.88米/6尺2',
 'player_name': '奎因-库克',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：292万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年550万美元，2019年夏天签，2021年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '亚历克斯-卡鲁索',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：264万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年260万美元，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '贾里德-杜德利',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：260万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年518万美元，2019年夏天签，2021年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '拉简-隆多',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：256万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年210万美元，2019年夏天签，2020年到期',
 'player_height': '1.93米/6尺4',
 'player_name': '特洛伊-丹尼尔斯',
 'player_num': '30',
 'player_position': 'G',
 'player_salary': '本年薪金：210万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年865万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '凯尔-库兹马',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：197万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年242万美元，2019年夏天签',
 'player_height': '1.93米/6尺4',
 'player_name': '塔伦-霍顿-塔克',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '双向合同，2019年夏天签',
 'player_height': '2.01米/6尺7',
 'player_name': '小扎克-诺维尔',
 'player_num': '17',
 'player_position': 'F-G',
 'player_salary': '本年薪金：万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年4000万美元，2019年夏天签，2022年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '帕特里克-贝弗利',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：1234万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年4200万，2016年夏天签，2020年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '莫里斯-哈克利斯',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：1101万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年2400万美元，2018年夏天签，2021年夏天到期',
 'player_height': '1.85米/6尺1',
 'player_name': '路易斯-威廉姆斯',
 'player_num': '40',
 'player_position': 'G',
 'player_salary': '本年薪金：800万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年2800万美元，2019年夏天签，2023年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '伊维察-祖巴茨',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：625万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年1200万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '蒙特雷兹-哈勒尔',
 'player_num': '1',
 'player_position': 'F-C',
 'player_salary': '本年薪金：600万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1569万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '杰罗姆-罗宾逊',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：357万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年260万美元加盟，2019年夏天签，2020年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '保罗-加索尔',
 'player_num': '16',
 'player_position': 'C-F',
 'player_salary': '本年薪金：260万美元',
 'player_team': '波特兰开拓者',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年260万美元，2019年夏天签，2020年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '安东尼-托利弗',
 'player_num': '43',
 'player_position': 'F',
 'player_salary': '本年薪金：260万美元',
 'player_team': '波特兰开拓者',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年607万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '斯卡尔-拉比西埃',
 'player_num': '17',
 'player_position': 'F',
 'player_salary': '本年薪金：234万美元',
 'player_team': '波特兰开拓者',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1017万美元，2018年夏天签，2022年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '安芬尼-西蒙斯',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：215万美元',
 'player_team': '波特兰开拓者',
 'player_weight': '83公斤/183磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年4100万美元，2019年夏天签，2022年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '德维恩-戴德蒙',
 'player_num': '0',
 'player_position': 'C',
 'player_salary': '本年薪金：1269万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年2500万美元，2019年夏天签，2021年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '特雷沃-阿里扎',
 'player_num': '9',
 'player_position': 'F',
 'player_salary': '本年薪金：1219万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年3700万美元，2019年夏天签，2022年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '科里-约瑟夫',
 'player_num': '35',
 'player_position': 'G',
 'player_salary': '本年薪金：1174万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年3613万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '马文-巴格利',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：856万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年2700万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '波格丹-波格丹诺维奇',
 'player_num': '88',
 'player_position': 'G',
 'player_salary': '本年薪金：853万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年2047万美元，2018年夏天签，2021年夏天到期，其中1332万受保障，2020-21赛季无保障',
 'player_height': '2.08米/6尺10',
 'player_name': '内马尼亚-别利察',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：682万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年2457万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '达龙-福克斯',
 'player_num': '24',
 'player_position': 'G',
 'player_salary': '本年薪金：639万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1589万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '巴迪-希尔德',
 'player_num': '22',
 'player_position': 'G',
 'player_salary': '本年薪金：486万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '97公斤/214磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.08米/6尺10',
 'player_name': '里乔恩-霍姆斯',
 'player_num': '3',
 'player_position': 'F-C',
 'player_salary': '本年薪金：476万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年615万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.83米/6尺0',
 'player_name': '约吉-费雷尔',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：315万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1062万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '哈里-贾尔斯',
 'player_num': '50',
 'player_position': 'F-C',
 'player_salary': '本年薪金：258万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年890万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '凯莱布-斯瓦尼根',
 'player_num': '41',
 'player_position': 'F',
 'player_salary': '本年薪金：203万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '5年1.9亿美元，2019年夏天签，2024年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '克莱-汤普森',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：3274万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1.17亿美元，2019年夏天签，2023年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '丹吉洛-拉塞尔',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：2728万美元',
 'player_team': '金州勇士',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1亿美元提前续约，2019年夏天签，2023-24赛季为球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '德雷蒙德-格林',
 'player_num': '2',
 'player_position': 'F',
 'player_salary': '本年薪金：1854万美元',
 'player_team': '金州勇士',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年1535万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '威利-考利-斯坦',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：470万美元',
 'player_team': '金州勇士',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '3年1500万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '凯文-卢尼',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：462万美元',
 'player_team': '金州勇士',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年232万美元，2019年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '亚历克-伯克斯',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：232万美元',
 'player_team': '金州勇士',
 'player_weight': '97公斤/214磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年403万美元，2019年夏天签',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-普尔',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：196万美元',
 'player_team': '金州勇士',
 'player_weight': '88公斤/194磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年922万美元，2018年夏天签，2022年夏天到期，其中2021年2022年俱乐部选项',
 'player_height': '1.98米/6尺6',
 'player_name': '雅各布-埃文斯',
 'player_num': '22',
 'player_position': 'G',
 'player_salary': '本年薪金：192万美元',
 'player_team': '金州勇士',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '4年909万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '奥马里-斯佩尔曼',
 'player_num': '28',
 'player_position': 'F',
 'player_salary': '本年薪金：190万美元',
 'player_team': '金州勇士',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '1年188万美元，2019年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '格伦-罗宾逊三世',
 'player_num': '6',
 'player_position': 'G-F',
 'player_salary': '本年薪金：188万美元',
 'player_team': '金州勇士',
 'player_weight': '101公斤/222磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '2年277万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '阿方索-麦金尼',
 'player_num': '32',
 'player_position': 'F',
 'player_salary': '本年薪金：142万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '阿伦-斯马伊拉吉奇',
 'player_num': '1',
 'player_position': 'F-C',
 'player_salary': '本年薪金：89万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/216磅'}
2019-08-20 17:07:36 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.01米/6尺7',
 'player_name': '马库斯-德里克森',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '金州勇士',
 'player_weight': '113公斤/249磅'}
2019-08-20 17:07:37 [root] INFO: {'player_contract': '2年2650万美元，2019年夏天签，2021年到期',
 'player_height': '1.93米/6尺4',
 'player_name': 'JJ-雷迪克',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：1292万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '5年2.06亿美元，2017年夏天签，2022年有球员选项，2023年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '拉塞尔-威斯布鲁克',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：3850万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '3年4500万美元，2019年夏天签，2022年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '约纳斯-瓦兰丘纳斯',
 'player_num': '17',
 'player_position': 'C',
 'player_salary': '本年薪金：1388万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '4年4800万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '所罗门-希尔',
 'player_num': '44',
 'player_position': 'F',
 'player_salary': '本年薪金：1276万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '4年5000万，2016年夏天签，2020年夏天到期，每年60万美元不易达到的激励奖金',
 'player_height': '2.11米/6尺11',
 'player_name': '迈尔斯-普拉姆利',
 'player_num': '25',
 'player_position': 'C',
 'player_salary': '本年薪金：1250万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '113公斤/249磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '4年3716万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '凯尔-安德森',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：907万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '新秀合同，2019年签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '贾-莫兰特',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：873万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '3年2400万美元，2019年夏天签，2022年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '泰厄斯-琼斯',
 'player_num': '21',
 'player_position': 'G',
 'player_salary': '本年薪金：800万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '5年3500万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '杰-克劳德',
 'player_num': '99',
 'player_position': 'F',
 'player_salary': '本年薪金：791万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '4年2712万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '约什-杰克逊',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：706万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '4年2928万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '小贾伦-杰克逊',
 'player_num': '13',
 'player_position': 'F',
 'player_salary': '本年薪金：693万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '110公斤/242磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '布兰登-克拉克',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：247万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '4年1109万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季无保障2020和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '格雷森-阿伦',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：243万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:38 [root] INFO: {'player_contract': '2年237万美元，2019年2月签，2020年夏天到期，2019-20赛季无保障',
 'player_height': '2.06米/6尺9',
 'player_name': '布鲁诺-卡博克洛',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：185万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:39 [root] INFO: {'player_contract': '2016年10月以4年8400万美元提前续约，2017年夏天生效，2021年夏天到期，每年25万不易达到的激励奖金',
 'player_height': '1.93米/6尺4',
 'player_name': '维克托-奥拉迪波',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：2100万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:40 [root] INFO: {'player_contract': '5年1.71亿美元，2017年夏天签，2022年夏天到，2021-22赛季球员选项',
 'player_height': '2.08米/6尺10',
 'player_name': '布雷克-格里芬',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：3423万美元',
 'player_team': '底特律活塞',
 'player_weight': '114公斤/251磅'}
2019-08-20 17:07:41 [root] INFO: {'player_contract': '5年1.13亿美元，2015年夏天签，2020年夏天到期。2018年7月，以4年1.2亿美元提前续约，2019年夏天生效，2023年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '凯文-乐福',
 'player_num': '0',
 'player_position': 'F-C',
 'player_salary': '本年薪金：2894万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '114公斤/251磅'}
2019-08-20 17:07:42 [root] INFO: {'player_contract': '4年1.06亿美元，2017年夏天签，2021年夏天到期，2020-2021赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '奥托-波特',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '5年1.78亿美元，2019年夏天签，2024年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '克里斯-米德尔顿',
 'player_num': '22',
 'player_position': 'F-G',
 'player_salary': '本年薪金：3068万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '106公斤/234磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '2年底薪，2019年签，2021年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '泰勒-莱登',
 'player_num': '32',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '蔡恩-威廉森',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：975万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '129公斤/285磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年 2247万美元合同，2017年夏天签',
 'player_height': '1.98米/6尺6',
 'player_name': '朗佐-鲍尔',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：872万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '4年2382万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '布兰登-英格拉姆',
 'player_num': '14',
 'player_position': 'F',
 'player_salary': '本年薪金：727万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '贾克森-海斯',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：486万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年500万美元，2017年夏天签',
 'player_height': '1.96米/6尺5',
 'player_name': '约什-哈特',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：193万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '2年327万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '贾利尔-奥卡福',
 'player_num': '8',
 'player_position': 'C',
 'player_salary': '本年薪金：170万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '125公斤/275磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季50.6万美元保障',
 'player_height': '1.91米/6尺3',
 'player_name': '弗兰克-杰克逊',
 'player_num': '15',
 'player_position': 'G',
 'player_salary': '本年薪金：162万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '2016年夏天在旧合同期内以4年1.18亿美元提前续约至2020年，2017年夏天再次以4年1.7亿美元提前续约至2023年，2022-23赛季球员选项',
 'player_height': '1.96米/6尺5',
 'player_name': '詹姆斯-哈登',
 'player_num': '13',
 'player_position': 'G',
 'player_salary': '本年薪金：3819万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '5年9000万美元，2018年夏天签，2023年夏天到期，其中8000万美元保障，每赛季火箭进入西决奖励100万美元，防守篮板率达到30%、罚球命中率达到65%各奖励50万美元',
 'player_height': '2.11米/6尺11',
 'player_name': '克林特-卡佩拉',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：1490万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '4年5289万美元，2016年夏天签，2020年夏天到期。',
 'player_height': '1.93米/6尺4',
 'player_name': '埃里克-戈登',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：1406万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '4年3188万美元，2017年夏天签，2021年夏天到期，2020-21赛季257万美元保障，2020年7月1日之前不被裁则转为全额保障',
 'player_height': '1.98米/6尺6',
 'player_name': 'PJ-塔克',
 'player_num': '17',
 'player_position': 'F',
 'player_salary': '本年薪金：835万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年1110万美元，2019年夏天签，2022年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '丹纽尔-豪斯',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：349万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '杰拉德-格林',
 'player_num': '14',
 'player_position': 'G-F',
 'player_salary': '本年薪金：256万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.16米/7尺1',
 'player_name': '泰森-钱德勒',
 'player_num': '19',
 'player_position': 'C',
 'player_salary': '本年薪金：256万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '2年底薪，2019年夏天签',
 'player_height': '1.93米/6尺4',
 'player_name': '奥斯汀-里弗斯',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：217万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '1年174万美元的非保障合同',
 'player_height': '2.03米/6尺8',
 'player_name': '安东尼-本内特',
 'player_num': '30',
 'player_position': 'F',
 'player_salary': '本年薪金：174万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年392万美元，2018年夏天签，2021年夏天到期',
 'player_height': '2.13米/7尺0',
 'player_name': '以赛亚-哈尔滕施泰因',
 'player_num': '55',
 'player_position': 'F-C',
 'player_salary': '本年薪金：142万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '113公斤/249磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年367万美元，2018年12月签，其中19-20赛季50%保障，20-21赛季为无保障合同，2021年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '加里-克拉克',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：141万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '1年89万美元',
 'player_height': '1.75米/5尺9',
 'player_name': '克里斯-克莱蒙斯',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '1年89万美元',
 'player_height': '1.96米/6尺5',
 'player_name': '威廉-麦克道尔-怀特',
 'player_num': '21',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '1年89万美元，2019年夏天签',
 'player_height': '1.85米/6尺1',
 'player_name': '沙莫里-庞兹',
 'player_num': '16',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '79公斤/174磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季无保障，2019年7月5日之前不被裁则转为全额保障 ',
 'player_height': '1.98米/6尺6',
 'player_name': '狄龙-布鲁克斯',
 'player_num': '24',
 'player_position': 'G-F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '3年395万美元，2017年夏天签，2020年夏天到期，2019-20赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '伊万-拉布',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '2年230万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '丹东尼-梅尔顿',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：135万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:43 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.06米/6尺9',
 'player_name': '渡边雄太',
 'player_num': '18',
 'player_position': 'G-F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '5年8478万，2013年夏天签，2014年夏天生效，2019年夏天到期；2017年夏天以4年1.7亿美元提前续约至2023年，2022-23赛季球员选项',
 'player_height': '1.93米/6尺4',
 'player_name': '约翰-沃尔',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：3780万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年8500万美元，2019年夏天签，2023年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '马尔科姆-布罗格登',
 'player_num': '7',
 'player_position': 'G',
 'player_salary': '本年薪金：1976万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2018年10月提前续约4年8000万美元，其中奖金部分为800万美元',
 'player_height': '2.11米/6尺11',
 'player_name': '迈尔斯-特纳',
 'player_num': '33',
 'player_position': 'C-F',
 'player_salary': '本年薪金：1750万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '110公斤/243磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2017年9月以4年4700万美元提前续约，2018年夏天生效，2022年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': 'TJ-沃伦',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：1081万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年3150万美元，2019年夏天签，2022年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '杰里米-兰姆',
 'player_num': '26',
 'player_position': 'G',
 'player_salary': '本年薪金：1000万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年2200万美元，2018年夏天签，2021年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '道格-麦克德莫特',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：733万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '99公斤/219磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '1年480万美元，2019年夏天签，2020年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '贾斯廷-霍勒迪',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：480万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年1118万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '多曼塔斯-萨博尼斯',
 'player_num': '11',
 'player_position': 'F',
 'player_salary': '本年薪金：353万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2年700万美元，2019年夏天签，2021年到期',
 'player_height': '1.88米/6尺2',
 'player_name': 'TJ-麦康奈尔',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：341万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2年577万美元，2019年夏天签',
 'player_height': '2.11米/6尺11',
 'player_name': '戈加-比塔泽',
 'player_num': '88',
 'player_position': 'C-F',
 'player_salary': '本年薪金：281万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年1158万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': 'TJ-利夫',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：281万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年1047万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.85米/6尺1',
 'player_name': '阿龙-霍勒迪',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：224万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年650万美元，2019年夏天签，2022年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '埃德蒙-萨姆纳',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：200万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2年225万美元，2018年夏天签，2020年夏天到期，其中包含84万的保证金',
 'player_height': '2.06米/6尺9',
 'player_name': '阿利兹-约翰逊 ',
 'player_num': '24',
 'player_position': 'F',
 'player_salary': '本年薪金：141万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '91公斤/201磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.98米/6尺6',
 'player_name': '达文-里德',
 'player_num': '32',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2019年夏天，签下Exhibit 10合同',
 'player_height': '2.03米/6尺8',
 'player_name': '杰基南-甘特',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '98公斤/216磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '5年1.3亿美元，2016年夏天签，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.11米/6尺11',
 'player_name': '安德烈-德拉蒙德',
 'player_num': '0',
 'player_position': 'C',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '底特律活塞',
 'player_weight': '127公斤/279磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '5年8000万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '雷吉-杰克逊',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：1809万美元',
 'player_team': '底特律活塞',
 'player_weight': '94公斤/208磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年4600万美元，2017年夏天签，2021年夏天到期，2020-2021赛季球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '托尼-斯内尔',
 'player_num': '17',
 'player_position': 'G',
 'player_salary': '本年薪金：1139万美元',
 'player_team': '底特律活塞',
 'player_weight': '98公斤/217磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年2100万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '兰斯顿-加洛韦',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：733万美元',
 'player_team': '底特律活塞',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2年1500万美元，2019年夏天签，2021年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '德里克-罗斯',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：731万美元',
 'player_team': '底特律活塞',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年1514万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '卢克-肯纳德',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：383万美元',
 'player_team': '底特律活塞',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '1年360万美元，2019年夏天签，2020年到期',
 'player_height': '2.08米/6尺10',
 'player_name': '马基夫-莫里斯',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：360万美元',
 'player_team': '底特律活塞',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年931万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '索恩-梅克',
 'player_num': '7',
 'player_position': 'C-F',
 'player_salary': '本年薪金：357万美元',
 'player_team': '底特律活塞',
 'player_weight': '101公斤/223磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '塞古-敦布亚',
 'player_num': '45',
 'player_position': 'F',
 'player_salary': '本年薪金：328万美元',
 'player_team': '底特律活塞',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '1年188万美元，2019年夏天签，2020年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '蒂姆-弗雷泽',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：188万美元',
 'player_team': '底特律活塞',
 'player_weight': '77公斤/170磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年392万美元，2018年夏天签，2021年夏天到期，2020-21赛季无保障，2020年7月10日前不被裁则全额保障',
 'player_height': '1.96米/6尺5',
 'player_name': '布鲁斯-布朗',
 'player_num': '6',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '底特律活塞',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年457万美元，2018年夏天签，2021年夏天到期，2019-20、2020-21赛季无保障，2019年7月5日、2020年7月5日前不被裁则当赛季转为全额保障',
 'player_height': '2.03米/6尺8',
 'player_name': '斯维亚托斯拉夫-米哈伊柳克',
 'player_num': '19',
 'player_position': 'F',
 'player_salary': '本年薪金：142万美元',
 'player_team': '底特律活塞',
 'player_weight': '87公斤/192磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '1年84万美元，2018年夏天签，2019年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '凯里-托马斯',
 'player_num': '13',
 'player_position': 'G',
 'player_salary': '本年薪金：84万美元',
 'player_team': '底特律活塞',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.85米/6尺1',
 'player_name': '卡林-卢卡斯',
 'player_num': '24',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '底特律活塞',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '5年8200万美元，2015年夏天签，2020年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '特里斯坦-汤普森',
 'player_num': '13',
 'player_position': 'C-F',
 'player_salary': '本年薪金：1854万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '108公斤/238磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '5年7000万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '布兰登-奈特',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：1564万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年5000万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-克拉克森',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：1344万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '88公斤/194磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年590万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项。\n'
                    '2018年10月提前续约4年4500万美元，2019年夏天生效',
 'player_height': '2.06米/6尺9',
 'player_name': '小拉里-南斯',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：1270万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年4400万美元，2015年10月签，2016年夏天生效，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '约翰-亨森',
 'player_num': '31',
 'player_position': 'C-F',
 'player_salary': '本年薪金：973万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '104公斤/229磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年3840万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '马修-戴拉维多瓦',
 'player_num': '18',
 'player_position': 'G',
 'player_salary': '本年薪金：960万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.88米/6尺2',
 'player_name': '达里厄斯-加兰',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：640万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '78公斤/173磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年2017万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '科林-塞克斯顿',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：476万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年832万美元，2017年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '切迪-奥斯曼',
 'player_num': '16',
 'player_position': 'F',
 'player_salary': '本年薪金：290万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '1年239万美元，2018年夏天签，2019年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '钱宁-弗莱',
 'player_num': '9',
 'player_position': 'F-C',
 'player_salary': '本年薪金：239万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年975万美元，2017年夏天签，2021年夏天到期，2019-2020、2020-2021赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '安特-日日奇',
 'player_num': '41',
 'player_position': 'C',
 'player_salary': '本年薪金：228万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '小凯文-波特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：129万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '97公斤/213磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.01米/6尺7',
 'player_name': '杰朗-布洛瑟姆盖姆',
 'player_num': '4',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '马基斯-克里斯',
 'player_num': '3',
 'player_position': 'F',
 'player_salary': '本年薪金：万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '106公斤/233磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年7800万美元，2018年夏天签，2022年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '扎克-拉文',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：1950万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年4100万美元，2019年夏天签',
 'player_height': '2.03米/6尺8',
 'player_name': '赛迪斯-杨',
 'player_num': '21',
 'player_position': 'F',
 'player_salary': '本年薪金：1301万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '100公斤/221磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年3000万美元，2019年夏天签，2022年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '托马什-萨托兰斯基',
 'player_num': '31',
 'player_position': 'G-F',
 'player_salary': '本年薪金：952万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年3200万美元，2017年夏天签，2021年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '克里斯蒂亚诺-费利西奥',
 'player_num': '6',
 'player_position': 'F-C',
 'player_salary': '本年薪金：816万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '121公斤/266磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年1749万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '克里斯-邓恩',
 'player_num': '32',
 'player_position': 'G',
 'player_salary': '本年薪金：535万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '科比-怀特',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：530万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年2039万美元，2017年夏天签，2021年夏天到期，2019-2020、2020-2021赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '劳里-马尔卡宁',
 'player_num': '24',
 'player_position': 'F',
 'player_salary': '本年薪金：530万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年2201万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '温德尔-卡特',
 'player_num': '34',
 'player_position': 'F',
 'player_salary': '本年薪金：520万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '117公斤/259磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年805万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '登泽尔-瓦伦丁',
 'player_num': '45',
 'player_position': 'G',
 'player_salary': '本年薪金：338万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '96公斤/212磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '3年900万美元，2019年夏天签',
 'player_height': '1.91米/6尺3',
 'player_name': '里安-阿尔奇迪亚科诺',
 'player_num': '51',
 'player_position': 'G',
 'player_salary': '本年薪金：300万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '4年1078万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '钱德勒-哈奇森',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：233万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '89公斤/196磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '1年162万美元，2018年夏天签，2019年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '卢克-科内特',
 'player_num': '2',
 'player_position': 'F-C',
 'player_salary': '本年薪金：162万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '2年294万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '安东尼奥-布莱克尼',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：159万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '89公斤/197磅'}
2019-08-20 17:07:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '丹尼尔-加福德',
 'player_num': '12',
 'player_position': 'F-C',
 'player_salary': '本年薪金：89万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '4年1亿美元，2019年夏天签，2023年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '尼古拉-武切维奇',
 'player_num': '9',
 'player_position': 'C',
 'player_salary': '本年薪金：2800万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '4年1亿美元。2017年夏天签，2021年夏天到期。',
 'player_height': '2.11米/6尺11',
 'player_name': '扬尼斯-阿德托昆博',
 'player_num': '34',
 'player_position': 'F',
 'player_salary': '本年薪金：2584万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '101公斤/222磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '5年7000万美元，2014年夏天签，2019年夏天到期',
 'player_height': '1.85米/6尺1',
 'player_name': '埃里克-布莱索',
 'player_num': '6',
 'player_position': 'G',
 'player_salary': '本年薪金：1562万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '4年5200万美元，2019年夏天签，2023年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '布鲁克-洛佩斯',
 'player_num': '11',
 'player_position': 'C',
 'player_salary': '本年薪金：1209万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '122公斤/268磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '3年2900万美元，2019年夏天签，2022年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '乔治-希尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：920万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '85公斤/188磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '3年2100万美元，2018年夏天签，2021年夏天到期，2020-21赛季无保障',
 'player_height': '2.08米/6尺10',
 'player_name': '艾森-伊利亚索瓦',
 'player_num': '77',
 'player_position': 'F',
 'player_salary': '本年薪金：700万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '4年1218万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': 'DJ-威尔森',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：296万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '4年1309万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '唐特-迪温琴佐',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：290万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '1年256万美元，2019年夏天签，2020年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '韦斯利-马修斯',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：256万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '2年336万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '帕特-康诺顿',
 'player_num': '24',
 'player_position': 'G-F',
 'player_salary': '本年薪金：172万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '2年350万美元，2019年夏天签，2021年到期',
 'player_height': '2.16米/7尺1',
 'player_name': '德拉甘-本德尔',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：167万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:45 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '斯特林-布朗',
 'player_num': '43',
 'player_position': 'G',
 'player_salary': '本年薪金：162万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:46 [root] INFO: {'player_contract': '5年1.2亿美元，2016年夏天签，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '尼古拉斯-巴图姆',
 'player_num': '5',
 'player_position': 'G-F',
 'player_salary': '本年薪金：2557万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:47 [root] INFO: {'player_contract': '4年1.42亿美元，2019年夏天签，2023年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '吉米-巴特勒',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：3274万美元',
 'player_team': '迈阿密热火',
 'player_weight': '105公斤/231磅'}
2019-08-20 17:07:48 [root] INFO: {'player_contract': '4年9444万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '钱德勒-帕森斯',
 'player_num': '25',
 'player_position': 'F',
 'player_salary': '本年薪金：2510万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:49 [root] INFO: {'player_contract': '5年1.8亿美元，2019年夏天签，2024年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '托拜厄斯-哈里斯',
 'player_num': '33',
 'player_position': 'F',
 'player_salary': '本年薪金：3103万美元',
 'player_team': '费城76人',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:50 [root] INFO: {'player_contract': '2018年12月以3年3435万美元提前续约，2019年夏天生效，2022年夏天到期，2021-22赛季为球员选项。',
 'player_height': '1.98米/6尺6',
 'player_name': '斯潘塞-丁威迪',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：1061万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年1亿美元，2017年夏天签，2020年夏天到期，7百万美元不易达到的激励奖金',
 'player_height': '1.83米/6尺0',
 'player_name': '凯尔-洛瑞',
 'player_num': '7',
 'player_position': 'G',
 'player_salary': '本年薪金：3300万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '5年1.27亿美元，2016年夏天签，2021年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '布拉德利-比尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '94公斤/207磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年1450万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '戴维斯-贝尔坦斯',
 'player_num': '17',
 'player_position': 'F',
 'player_salary': '本年薪金：700万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年1200万美元，2019年夏天签，2021年到期',
 'player_height': '1.83米/6尺0',
 'player_name': '伊斯梅尔-史密斯',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：585万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年408万美元，2018年夏天签，2020年夏天到期，2020-21赛季非保障',
 'player_height': '2.03米/6尺8',
 'player_name': '伊萨克-邦加',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年420万美元，2019年夏天签',
 'player_height': '1.88米/6尺2',
 'player_name': '贾斯廷-罗宾逊',
 'player_num': '52',
 'player_position': 'G',
 'player_salary': '本年薪金：90万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2022年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '阿德米拉尔-斯科菲尔德',
 'player_num': '9',
 'player_position': 'F',
 'player_salary': '本年薪金：89万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '109公斤/241磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-麦克雷',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年147万美元，2018年夏天签，2020年夏天到期，2019-20赛季非保障',
 'player_height': '1.96米/6尺5',
 'player_name': '杰梅里奥-琼斯',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：5万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '91公斤/201磅'}
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/cjwilliams-150398.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaredterrell-151075.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/keitabatesdiop-150967.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jordanbell-150492.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/treveongraham-150085.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shabazznapier-4910.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/noahvonleh-4965.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshokogie-150991.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年8000万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '阿龙-戈登',
 'player_num': '00',
 'player_position': 'F',
 'player_salary': '本年薪金：1986万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '5年8500万，2016年夏天签，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '埃文-富尼耶',
 'player_num': '10',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1700万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年5400万美元，2019年夏天签，2023年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '特伦斯-罗斯',
 'player_num': '8',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1250万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年3740万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '马克尔-富尔茨',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：975万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年2900万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '艾尔-法鲁克-阿米奴',
 'player_num': '2',
 'player_position': 'F',
 'player_salary': '本年薪金：925万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年2900万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.83米/6尺0',
 'player_name': 'DJ-奥古斯丁',
 'player_num': '14',
 'player_position': 'G',
 'player_salary': '本年薪金：725万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '83公斤/183磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年2232万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '乔纳森-艾萨克',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：580万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年2410万美元，2018年夏天签，2022年夏天到期，2020年和2021年球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '穆罕默德-班巴',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：570万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年600万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '肯-伯奇',
 'player_num': '24',
 'player_position': 'C-F',
 'player_salary': '本年薪金：300万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '1年202万美元，2019年夏天签，2020年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '迈克尔-卡特-威廉姆斯',
 'player_num': '7',
 'player_position': 'G',
 'player_salary': '本年薪金：202万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年405万美元，2017年夏天签，2020年夏天到期，2019-20赛季球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '韦斯利-艾旺杜',
 'player_num': '25',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年621万美元，2018年夏天签，2022年夏天到期，2021年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '梅尔文-弗雷泽',
 'player_num': '35',
 'player_position': 'F-G',
 'player_salary': '本年薪金：151万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '91公斤/201磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.93米/6尺4',
 'player_name': '特洛伊-科佩恩',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年5800万美元，2019年夏天签，2022年到期',
 'player_height': '1.88米/6尺2',
 'player_name': '特里-罗齐尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：1841万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1740万美元，2013年夏天签，2017年夏天到期，2016年10月以4年5600万美元续约，2017年夏天生效，2021年夏天到期',
 'player_height': '2.13米/7尺0',
 'player_name': '科迪-泽勒',
 'player_num': '40',
 'player_position': 'C',
 'player_salary': '本年薪金：1447万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': 'PJ-华盛顿',
 'player_num': '25',
 'player_position': 'F',
 'player_salary': '本年薪金：383万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '103公斤/228磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1572万美元，2018年夏天签，2022年夏天到期，2020-21赛季、2021-22赛季球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '迈尔斯-布里奇斯',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：375万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年586万美元，2016年夏天签，2020年夏天到期，2019-20赛季无保障',
 'player_height': '2.11米/6尺11',
 'player_name': '吉列尔莫-埃尔南戈麦斯',
 'player_num': '41',
 'player_position': 'C',
 'player_salary': '本年薪金：156万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '1年144万美元',
 'player_height': '2.13米/7尺0',
 'player_name': '托马斯-韦尔什',
 'player_num': '7',
 'player_position': 'C',
 'player_salary': '本年薪金：144万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季无保障，2019年8月1日前不被裁则转为全额保障',
 'player_height': '2.01米/6尺7',
 'player_name': '德维恩-培根',
 'player_num': '11',
 'player_position': 'G-F',
 'player_salary': '本年薪金：138万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年447万美元合同，2019年夏天签',
 'player_height': '1.98米/6尺6',
 'player_name': '科迪-马丁',
 'player_num': '31',
 'player_position': 'G',
 'player_salary': '本年薪金：117万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '87公斤/191磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.93米/6尺4',
 'player_name': '乔-奇利',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '5年9000万美元，2015年夏天签，2020年夏天到期，2019-20赛季球员选项',
 'player_height': '1.91米/6尺3',
 'player_name': '戈兰-德拉季奇',
 'player_num': '7',
 'player_position': 'G',
 'player_salary': '本年薪金：1922万美元',
 'player_team': '迈阿密热火',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年5912万美元，2017年夏天签，2021年夏天到期，2020-21赛季球员选项，94.6万可能拿到的激励奖金，其中2017-18赛季22万，另有其他体重控制奖金',
 'player_height': '2.06米/6尺9',
 'player_name': '詹姆斯-约翰逊',
 'player_num': '16',
 'player_position': 'F',
 'player_salary': '本年薪金：1513万美元',
 'player_team': '迈阿密热火',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1123万美元，2015年夏天签，2019年夏天到期，2018年10月以3年3900万美元提前续约，2019年夏天生效，2022年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '贾斯蒂斯-温斯洛',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：1300万美元',
 'player_team': '迈阿密热火',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年5200万美元，2017年夏天签，2021年夏天到期，470万美元不易达到的激励奖金，2017-18赛季需至少出战70场比赛可获得110万美元激励奖金',
 'player_height': '1.93米/6尺4',
 'player_name': '迪昂-韦特斯',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：1210万美元',
 'player_team': '迈阿密热火',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年5000万美元，2017年夏天签，2021年夏天到期，560万不易达到的激励奖金，剩余合同价值的5%或200万美元的交易金（两者取其小），2020-2021赛季球员选项',
 'player_height': '2.13米/7尺0',
 'player_name': '凯利-奥利尼克',
 'player_num': '9',
 'player_position': 'F',
 'player_salary': '本年薪金：1167万美元',
 'player_team': '迈阿密热火',
 'player_weight': '108公斤/238磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年4100万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '迈耶斯-伦纳德',
 'player_num': '0',
 'player_position': 'F-C',
 'player_salary': '本年薪金：1129万美元',
 'player_team': '迈阿密热火',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '泰勒-希罗',
 'player_num': '14',
 'player_position': 'G',
 'player_salary': '本年薪金：364万美元',
 'player_team': '迈阿密热火',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1402万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '巴姆-阿德巴约',
 'player_num': '13',
 'player_position': 'C-F',
 'player_salary': '本年薪金：345万美元',
 'player_team': '迈阿密热火',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年316万美元，2018年夏天签，2020年夏天到期，2019年无保证',
 'player_height': '2.01米/6尺7',
 'player_name': '小德里克-琼斯',
 'player_num': '5',
 'player_position': 'F-G',
 'player_salary': '本年薪金：151万美元',
 'player_team': '迈阿密热火',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2022年到期，2021年，2022年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': 'KZ-奥克帕拉',
 'player_num': '40',
 'player_position': 'F',
 'player_salary': '本年薪金：89万美元',
 'player_team': '迈阿密热火',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年7000万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '埃文-特纳',
 'player_num': '33',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1861万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年7483万美元，2016年夏天签，2020年夏天到期，2019-20赛季球员选项',
 'player_height': '1.98米/6尺6',
 'player_name': '阿伦-克拉布',
 'player_num': '12',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1850万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '德安德烈-亨特',
 'player_num': '5',
 'player_position': 'F-G',
 'player_salary': '本年薪金：706万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年1300万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '贾巴里-帕克',
 'player_num': '11',
 'player_position': 'F',
 'player_salary': '本年薪金：634万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年2653万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.88米/6尺2',
 'player_name': '特雷-杨',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：627万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年850万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '亚历克斯-莱恩',
 'player_num': '22',
 'player_position': 'C',
 'player_salary': '本年薪金：435万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '卡姆-雷迪什',
 'player_num': '20',
 'player_position': 'F-G',
 'player_salary': '本年薪金：424万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '92公斤/203磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1106万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '约翰-科林斯',
 'player_num': '3',
 'player_position': 'F-C',
 'player_salary': '本年薪金：269万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1190万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '凯文-赫尔特',
 'player_num': '95',
 'player_position': 'G',
 'player_salary': '本年薪金：264万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年731万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '德安德烈-本布里',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：260万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '1年256万美元合同，2019年夏天签',
 'player_height': '1.98米/6尺6',
 'player_name': '文斯-卡特',
 'player_num': '24',
 'player_position': 'G-F',
 'player_salary': '本年薪金：256万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年706万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '贾斯廷-安德森',
 'player_num': '0',
 'player_position': 'G-F',
 'player_salary': '本年薪金：252万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '103公斤/228磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年598万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '达米安-琼斯',
 'player_num': '22',
 'player_position': 'C',
 'player_salary': '本年薪金：231万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2017年10月以5年1.48亿美元提前续约，2018年夏天生效，2023年夏天到期；',
 'player_height': '2.13米/7尺0',
 'player_name': '乔尔-恩比德',
 'player_num': '21',
 'player_position': 'C',
 'player_salary': '本年薪金：2727万美元',
 'player_team': '费城76人',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2017年9月以4年4197万美元提前续约，2018年夏天生效，2022年夏天到期，2021-22赛季球员选项',
 'player_height': '1.98米/6尺6',
 'player_name': '约什-理查德森',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：1012万美元',
 'player_team': '费城76人',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1379万美元，2018年夏天签，2022年夏天到期，2020年和2021年球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '扎伊尔-史密斯',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：306万美元',
 'player_team': '费城76人',
 'player_weight': '90公斤/199磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年440万美元，2018年夏天签，2020年夏天到期，其中210万受保障，2019-2020赛季无保障',
 'player_height': '1.85米/6尺1',
 'player_name': '劳尔-内托',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：210万美元',
 'player_team': '费城76人',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年338万美元，2019年夏天签，2021年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '特雷-伯克',
 'player_num': '30',
 'player_position': 'G',
 'player_salary': '本年薪金：203万美元',
 'player_team': '费城76人',
 'player_weight': '87公斤/191磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年1600万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '乔-哈里斯',
 'player_num': '12',
 'player_position': 'G-F',
 'player_salary': '本年薪金：800万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '99公斤/219磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年空间中产合同，2019年夏天签，2021年到期,2020年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '加莱特-坦普尔',
 'player_num': '17',
 'player_position': 'G-F',
 'player_salary': '本年薪金：477万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年1075万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '托里恩-普林斯',
 'player_num': '30',
 'player_position': 'F',
 'player_salary': '本年薪金：348万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '1年256万美元，2019年夏天签，2020年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '威尔森-钱德勒',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：256万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年916万美元，2018年夏天签，2022年夏天到期，2020-21赛季以及2021-22赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '扎南-穆萨',
 'player_num': '10',
 'player_position': 'G-F',
 'player_salary': '本年薪金：191万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年底薪，2019年夏天签，2021年到期，2021年非保障',
 'player_height': '1.93米/6尺4',
 'player_name': '大卫-努瓦巴',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：167万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '95公斤/209磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年底薪，2019年夏天签，2021年到期，2020年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '西奥-平森',
 'player_num': '31',
 'player_position': 'G',
 'player_salary': '本年薪金：144万美元',
 'player_team': '布鲁克林篮网',
 'player_weight': '99公斤/218磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年6500万美元，2017年夏天签，2020年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '赛尔吉-伊巴卡',
 'player_num': '9',
 'player_position': 'C-F',
 'player_salary': '本年薪金：2327万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年750万美元，2019年夏天签，2021年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '斯坦利-约翰逊',
 'player_num': '43',
 'player_position': 'F',
 'player_salary': '本年薪金：365万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '4年610万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '帕斯卡尔-西亚卡姆',
 'player_num': '21',
 'player_position': 'F',
 'player_salary': '本年薪金：235万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '2年371万美元，2019年夏天签，2021年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '卡梅伦-佩恩',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：173万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '朗戴-霍利斯-杰弗森',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：173万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '97公斤/214磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '3年420万，2019年夏天签',
 'player_height': '1.93米/6尺4',
 'player_name': '马特-托马斯',
 'player_num': '24',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '89公斤/197磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2022年到期',
 'player_height': '2.08米/6尺10',
 'player_name': '德万-埃尔南德斯',
 'player_num': '13',
 'player_position': 'F',
 'player_salary': '本年薪金：89万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '106公斤/233磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.93米/6尺4',
 'player_name': '乔丹-劳埃德',
 'player_num': '33',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '多伦多猛龙',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:07:51 [root] INFO: {'player_contract': '1年144万美元，2019年夏天签',
 'player_height': '2.08米/6尺10',
 'player_name': '雷-斯波尔丁',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：144万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:07:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jacobevans-150974.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/marcusderrickson-151113.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alensmailagic-151760.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alfonzomckinnie-150858.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/glennrobinson-4912.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/omarispellman-150981.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/spencerdinwiddie-4955.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kylelowry-1244.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tobiasharris-3572.html> (referer: https://nba.hupu.com/players/76ers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jimmybutler-3583.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rayspalding-151025.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jordanloyd-151398.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chandlerparsons-3584.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dewanhernandez-151769.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/mattthomas-151844.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pascalsiakam-150219.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/sterlingbrown-150548.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/draganbender-150165.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rondaehollisjefferson-150014.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/cameronpayne-150052.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/stanleyjohnson-150021.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/sergeibaka-3055.html> (referer: https://nba.hupu.com/players/raptors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/theopinson-151199.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/nicolasbatum-3025.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/wilsonchandler-1424.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/davidnwaba-150427.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dzananmusa-150972.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garretttemple-3440.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/taureanprince-150188.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joeharris-4957.html> (referer: https://nba.hupu.com/players/nets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/treyburke-4833.html> (referer: https://nba.hupu.com/players/76ers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/raulneto-150063.html> (referer: https://nba.hupu.com/players/76ers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zhairesmith-150970.html> (referer: https://nba.hupu.com/players/76ers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joelembiid-4958.html> (referer: https://nba.hupu.com/players/76ers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshrichardson-150033.html> (referer: https://nba.hupu.com/players/76ers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/justinanderson-150017.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deandrebembry-150203.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/vincecarter-936.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/damianjones-150183.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/johncollins-150501.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/camreddish-151671.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kevinhuerter-151068.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alexlen-4823.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jabariparker-4914.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/evanturner-3450.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/traeyoung-150951.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deandrehunter-151703.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/allencrabbe-4845.html> (referer: https://nba.hupu.com/players/hawks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/derrickjones-150409.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/bamadebayo-150509.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kzokpala-151720.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tylerherro-151713.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/meyersleonard-3648.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kellyolynyk-4779.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dionwaiters-3641.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/justisewinslow-150032.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesjohnson-3321.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/gorandragic-3074.html> (referer: https://nba.hupu.com/players/heat)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joechealey-151387.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/codymartin-151759.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dwaynebacon-150477.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/thomaswelsh-151026.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/willyhernangomez-150041.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/terryrozier-150005.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesbridges-150440.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjwashington-151712.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/codyzeller-4842.html> (referer: https://nba.hupu.com/players/hornets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/troycaupain-150796.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/melvinfrazier-150950.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/wesiwundu-150474.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelcarterwilliams-4820.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonathanisaac-150435.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/djaugustin-3021.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/khembirch-4993.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/mohamedbamba-150935.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alfarouqaminu-3456.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/markellefultz-150426.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/terrenceross-3645.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/evanfournier-3657.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/aarongordon-4944.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/admiralschofield-151729.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jordanmcrae-151432.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/justinrobinson-152126.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaacbonga-150968.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/davisbertans-3596.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/bradleybeal-3640.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jemerriojones-151695.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/wesleymatthews-3408.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/patconnaughton-150050.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ishsmith-3516.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dontedivincenzo-150960.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ersanilyasova-1141.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericbledsoe-3466.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/djwilson-150466.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brooklopez-3013.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/giannisantetokounmpo-4802.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/nikolavucevic-3569.html> (referer: https://nba.hupu.com/players/magic)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/georgehill-3180.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/antonioblakeney-150634.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ryanarcidiacono-150266.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danielgafford-151732.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lukekornet-150789.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/wendellcarter-150957.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/laurimarkkanen-150439.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/denzelvalentine-150174.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chandlerhutchison-150976.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/cobywhite-151707.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/cristianofelicio-150065.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zachlavine-4911.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tomassatoransky-150359.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaronblossomgame-151439.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/thaddeusyoung-1237.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/krisdunn-150168.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/marquesechriss-150177.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kevinporter-151714.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/cediosman-150011.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dariusgarland-151704.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/antezizic-150194.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/channingfrye-1029.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/collinsexton-150952.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/larrynance-150030.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/matthewdellavedova-4789.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/johnhenson-3651.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jordanclarkson-4933.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tristanthompson-3557.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brucebrown-150941.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/timfrazier-5026.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/khyrithomas-150975.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonknight-3561.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/svimykhailiuk-150208.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kalinlucas-4867.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/thonmaker-150231.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/sekoudoumbouya-151705.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/markieffmorris-3566.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/derrickrose-3010.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lukekennard-150505.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/langstongalloway-4990.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andredrummond-3646.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tonysnell-4784.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/reggiejackson-3577.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tjmcconnell-150142.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alizejohnson-150939.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/davonreed-150545.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/edmondsumner-150467.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/aaronholiday-150943.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tjleaf-150499.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/gogabitadze-150936.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jakeenangant-151841.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/domantassabonis-150175.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jeremylamb-3649.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/justinholiday-3741.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/mylesturner-150026.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dougmcdermott-4927.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/johnwall-3449.html> (referer: https://nba.hupu.com/players/wizards)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tjwarren-4934.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/malcolmbrogdon-150206.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/khrismiddleton-3676.html> (referer: https://nba.hupu.com/players/bucks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ottoporter-4838.html> (referer: https://nba.hupu.com/players/bulls)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kevinlove-3004.html> (referer: https://nba.hupu.com/players/cavaliers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/blakegriffin-3344.html> (referer: https://nba.hupu.com/players/pistons)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/victoroladipo-4819.html> (referer: https://nba.hupu.com/players/pacers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tylerlydon-150493.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jordanpoole-151747.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alecburks-3565.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dangelorussell-150029.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kevonlooney-150023.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/draymondgreen-3672.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williecauleystein-150054.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/klaythompson-3564.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yogiferrell-150380.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/harrygilesiii-150468.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/calebswanigan-150490.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/richaunholmes-150046.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/buddyhield-150169.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/nemanjabjelica-150066.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deaaronfox-150434.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/bogdanbogdanovic-150528.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/marvinbagley-150922.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/trevorariza-558.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/coryjoseph-3582.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dewaynededmon-4849.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivicazubac-150201.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/montrezlharrell-150025.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/louwilliams-1128.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jeromerobinson-150992.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/skallabissiere-150172.html> (referer: https://nba.hupu.com/players/blazers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anferneesimons-150964.html> (referer: https://nba.hupu.com/players/blazers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/troydaniels-4884.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/mauriceharkless-3652.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonytolliver-3193.html> (referer: https://nba.hupu.com/players/blazers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/paugasol-15.html> (referer: https://nba.hupu.com/players/blazers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kylekuzma-150471.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/patrickbeverley-3393.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jordancaroline-152151.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/talenhortontucker-151715.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rajonrondo-1219.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jareddudley-1395.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/alexcaruso-150418.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/quinncook-150149.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/demarcuscousins-3453.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/javalemcgee-3038.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/averybradley-3467.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kentaviouscaldwellpope-4790.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonydavis-3638.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dannygreen-3356.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/georgeking-151027.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jevoncarter-150984.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deandreayton-150920.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankkaminsky-150010.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kellyoubre-150057.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dariosaric-4959.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tylerjohnson-4978.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahroby-151739.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/justinjackson-150511.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lukadoncic-150648.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/maxikleber-150704.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dwightpowell-4967.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lebronjames-650.html> (referer: https://nba.hupu.com/players/lakers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/stephencurry-3311.html> (referer: https://nba.hupu.com/players/warriors)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zachcollins-150461.html> (referer: https://nba.hupu.com/players/blazers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/harrisonbarnes-3644.html> (referer: https://nba.hupu.com/players/kings)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/courtneylee-3052.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/paulgeorge-3458.html> (referer: https://nba.hupu.com/players/clippers)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/devinbooker-150044.html> (referer: https://nba.hupu.com/players/suns)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/derrickwhite-150480.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lukasamanic-151734.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rudygay-1202.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jakelayman-150213.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonniewalker-150965.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/timhardaway-4814.html> (referer: https://nba.hupu.com/players/mavericks)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/treylyles-150059.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lamarcusaldridge-1209.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/robertcovington-4851.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarrettculver-151701.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:08:00 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-08-20 17:08:00 [root] INFO: {'player_contract': '3年6300万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '朱利叶斯-兰德尔',
 'player_num': '30',
 'player_position': 'F',
 'player_salary': '本年薪金：2000万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:08:00 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-08-20 17:08:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/karlanthonytowns-150037.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:08:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/demarderozan-3314.html> (referer: https://nba.hupu.com/players/spurs)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:08:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/gorguidieng-4813.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:08:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jeffteague-3309.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:08:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andrewwiggins-4956.html> (referer: https://nba.hupu.com/players/timberwolves)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 82, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 17:08:00 [root] INFO: {'player_contract': '2年3100万美元，2019年夏天签，2021年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '博比-波蒂斯',
 'player_num': '67',
 'player_position': 'F',
 'player_salary': '本年薪金：1512万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '112公斤/246磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '2年2000万美元，2019年夏天签',
 'player_height': '2.06米/6尺9',
 'player_name': '泰-吉布森',
 'player_num': '9',
 'player_position': 'F',
 'player_salary': '本年薪金：975万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '107公斤/236磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': 'RJ-巴雷特',
 'player_num': '2',
 'player_position': 'F-G',
 'player_salary': '本年薪金：783万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '92公斤/202磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '2年1600万美元，2019年夏天签，2021年到期',
 'player_height': '1.93米/6尺4',
 'player_name': '韦恩-艾灵顿',
 'player_num': '6',
 'player_position': 'G',
 'player_salary': '本年薪金：780万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '2年1600万美元，2019年夏天签，2021年到期',
 'player_height': '1.93米/6尺4',
 'player_name': '埃尔弗里德-佩顿',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：780万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '4年1869万美元，2017年夏天签，2021年夏天到期，2019-20赛季、2020-21赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '弗朗克-尼利基纳',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：486万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '2年低于空间中产特例签约，2019年夏天签，2021年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '雷吉-布洛克',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：470万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '4年1719万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '丹尼斯-史密斯',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：446万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '4年1855万美元，2018年夏天签，2022年夏天到期，其中2020年和2021年俱乐部选项',
 'player_height': '2.06米/6尺9',
 'player_name': '凯文-诺克斯',
 'player_num': '14',
 'player_position': 'F',
 'player_salary': '本年薪金：438万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '2年693万美元，2018年12月签，2020年夏天到期。2019-20赛季球队选项。',
 'player_height': '1.96米/6尺5',
 'player_name': '阿朗佐-特里尔',
 'player_num': '21',
 'player_position': 'G',
 'player_salary': '本年薪金：355万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '3年410万美元，2018年夏天签，2021年夏天到期，2019年无保证',
 'player_height': '1.96米/6尺5',
 'player_name': '丹伊恩-多特森',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：162万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:08:00 [root] INFO: {'player_contract': '3年471万美元，2018年夏天签，2021年夏天到期，2020年无保障，如果在6月29日之前没有宣布放弃，则继续执行',
 'player_height': '2.16米/7尺1',
 'player_name': '米切尔-罗宾逊',
 'player_num': '17',
 'player_position': 'C',
 'player_salary': '本年薪金：156万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '101公斤/223磅'}
2019-08-20 17:08:01 [root] INFO: {'player_contract': '4年1.41亿美元，2019年夏天签，2023年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '肯巴-沃克',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：3274万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '83公斤/184磅'}
2019-08-20 17:08:02 [root] INFO: {'player_contract': '3年9150万美元，2017年夏天签，2020年夏天到期，2019-2020赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '保罗-米尔萨普',
 'player_num': '4',
 'player_position': 'F',
 'player_salary': '本年薪金：3050万美元',
 'player_team': '丹佛掘金',
 'player_weight': '112公斤/246磅'}
2019-08-20 17:08:02 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-08-20 17:08:03 [root] INFO: {'player_contract': '5年1.53亿美元，2016年夏天签，2021年夏天到期',
 'player_height': '1.85米/6尺1',
 'player_name': '迈克-康利',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：3251万美元',
 'player_team': '犹他爵士',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1.6亿美元，2018年夏天签，2022年夏天到期',
 'player_height': '1.83米/6尺0',
 'player_name': '克里斯-保罗',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：3851万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2022年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '伊格纳斯-布拉兹代基斯',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：89万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.91米/6尺3',
 'player_name': '卡迪姆-阿伦',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.06米/6尺9',
 'player_name': '以赛亚-希克斯',
 'player_num': '13',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '纽约尼克斯',
 'player_weight': '110公斤/242磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1.28亿美元，2017年夏天签，2021年夏天到期，2020-21赛季球员选项，15%交易保证金',
 'player_height': '2.03米/6尺8',
 'player_name': '戈登-海沃德',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：3270万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '102公斤/226磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年5200万美元，2018年夏天签，2022年夏天到期，每年身体脂肪含量达到球队要求则有50万奖金',
 'player_height': '1.93米/6尺4',
 'player_name': '马库斯-斯马特',
 'player_num': '36',
 'player_position': 'G',
 'player_salary': '本年薪金：1205万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年3007万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '杰森-塔特姆',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：783万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年2140万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '杰伦-布朗',
 'player_num': '7',
 'player_position': 'F-G',
 'player_salary': '本年薪金：653万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '丹尼尔-泰斯',
 'player_num': '27',
 'player_position': 'F',
 'player_salary': '本年薪金：487万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '110公斤/243磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '埃内斯-坎特',
 'player_num': '11',
 'player_position': 'C',
 'player_salary': '本年薪金：476万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '罗密欧-兰福德',
 'player_num': '45',
 'player_position': 'G',
 'player_salary': '本年薪金：345万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '2年512万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '樊尚-普瓦里耶',
 'player_num': '17',
 'player_position': 'C',
 'player_salary': '本年薪金：250万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '107公斤/236磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '格兰特-威廉姆斯',
 'player_num': '12',
 'player_position': 'F',
 'player_salary': '本年薪金：237万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '107公斤/236磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年927万美元，2018年夏天签，2022年夏天到期,2020和2021年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '罗伯特-威廉姆斯',
 'player_num': '44',
 'player_position': 'C-F',
 'player_salary': '本年薪金：194万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '107公斤/237磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年604万美元，2017年夏天签，2021年夏天到期；2018-19赛季90万美元保障，2018年7月15日之前不被裁则转为全额保障，2019-20赛季无保障，2019年7月1日之前不被裁则转为全额',
 'player_height': '2.01米/6尺7',
 'player_name': '谢米-奥杰莱',
 'player_num': '37',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '1.93米/6尺4',
 'player_name': '布拉德-沃纳梅克',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：144万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.85米/6尺1',
 'player_name': '卡森-爱德华兹',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：120万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '1年90万美元，2019年夏签',
 'player_height': '2.31米/7尺7',
 'player_name': '塔科-法尔',
 'player_num': '50',
 'player_position': 'C',
 'player_salary': '本年薪金：90万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '131公斤/289磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.98米/6尺6',
 'player_name': 'PJ-多齐尔',
 'player_num': '28',
 'player_position': 'G-F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '波士顿凯尔特人',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '5年1.47亿美元，2018年夏天签，2023年夏天到期 ',
 'player_height': '2.13米/7尺0',
 'player_name': '尼古拉-约基奇',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：2743万美元',
 'player_team': '丹佛掘金',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年731万美元，2014年夏天签，2018年夏天到期，2016-17赛季、2017-18赛季球队选项（已执行），2017年10月以4年8400万美元提前续约，2018年夏天生效，2022年夏天到期，',
 'player_height': '1.93米/6尺4',
 'player_name': '加里-哈里斯',
 'player_num': '14',
 'player_position': 'G',
 'player_salary': '本年薪金：1784万美元',
 'player_team': '丹佛掘金',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年4100万美元，2017年夏天签，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '梅森-普拉姆利',
 'player_num': '24',
 'player_position': 'F-C',
 'player_salary': '本年薪金：1404万美元',
 'player_team': '丹佛掘金',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1509万美元，2018年夏天签，2022年夏天到期，其中2020-21、2021-22赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '小迈克尔-波特',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：339万美元',
 'player_team': '丹佛掘金',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年955万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '胡安-埃尔南戈麦斯',
 'player_num': '41',
 'player_position': 'F',
 'player_salary': '本年薪金：332万美元',
 'player_team': '丹佛掘金',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1456万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '多诺万-米切尔',
 'player_num': '45',
 'player_position': 'G',
 'player_salary': '本年薪金：363万美元',
 'player_team': '犹他爵士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年860万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '托尼-布拉德利',
 'player_num': '13',
 'player_position': 'C',
 'player_salary': '本年薪金：196万美元',
 'player_team': '犹他爵士',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年494万美元，2018年夏天签，2021年夏天到期，其中151万受保障，2019-2020、2020-21赛季无保障',
 'player_height': '2.03米/6尺8',
 'player_name': '乔治-尼昂',
 'player_num': '31',
 'player_position': 'F',
 'player_salary': '本年薪金：164万美元',
 'player_team': '犹他爵士',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季无保障',
 'player_height': '1.98米/6尺6',
 'player_name': '罗伊斯-奥尼尔',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '犹他爵士',
 'player_weight': '102公斤/226磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2022年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '米耶-奥尼',
 'player_num': '24',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '犹他爵士',
 'player_weight': '95公斤/209磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1.02亿美元，2017年夏天签，2021年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '伊曼纽尔-穆迪埃',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：万美元',
 'player_team': '犹他爵士',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年969万美元，2013年夏天签，2017年夏天到期，2016年10月以4年1亿美元提前续约，2017年夏天生效，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '史蒂文-亚当斯',
 'player_num': '12',
 'player_position': 'C',
 'player_salary': '本年薪金：2584万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年6477万美元，2017年夏天签，2020年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '达尼罗-加里纳利',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：2262万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年7500万，2013年夏天签，2016年10月以4年7000万美元提前续约，2017年夏天生效，2021年夏天到期，其中6200万美元受保障，200万美元不易达到的激励奖金',
 'player_height': '1.85米/6尺1',
 'player_name': '丹尼斯-施罗德',
 'player_num': '17',
 'player_position': 'G',
 'player_salary': '本年薪金：1550万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '78公斤/172磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年3000万美元，2017年夏天签，2020年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '安德烈-罗伯森',
 'player_num': '21',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1074万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1696万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '谢伊-吉尔杰斯-亚历山大',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：395万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '4年1032万美元，2017年夏天签，2021年夏天到期，2019-2020、2020-2021赛季球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '特伦斯-弗格森',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：248万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '达里厄斯-贝兹利',
 'player_num': '7',
 'player_position': 'F-G',
 'player_salary': '本年薪金：228万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '2年430万美元，2019年夏天签，2020年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '迈克-穆斯卡拉',
 'player_num': '33',
 'player_position': 'F-C',
 'player_salary': '本年薪金：202万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '1年202万美元，2019年夏天签，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '纳伦斯-诺埃尔',
 'player_num': '9',
 'player_position': 'F-C',
 'player_salary': '本年薪金：202万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '103公斤/228磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '三年530万，2019年夏天签',
 'player_height': '2.13米/7尺0',
 'player_name': '贾斯廷-巴顿',
 'player_num': '11',
 'player_position': 'C',
 'player_salary': '本年薪金：162万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年416万美元，2017年夏天签，2020年夏天到期。',
 'player_height': '1.98米/6尺6',
 'player_name': '阿卜杜勒-纳迪尔',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:08:04 [root] INFO: {'player_contract': '3年392万美元，2018年夏天签，2021年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '哈米杜-迪亚洛',
 'player_num': '15',
 'player_position': 'G',
 'player_salary': '本年薪金：141万美元',
 'player_team': '俄克拉荷马城雷霆',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:16:24 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 17:16:24 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 17:16:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'nba.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 17:16:24 [scrapy.extensions.telnet] INFO: Telnet Password: af2cd03dca344f2e
2019-08-20 17:16:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 17:16:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 17:16:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 17:16:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 17:16:25 [scrapy.core.engine] INFO: Spider opened
2019-08-20 17:16:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 17:16:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 17:16:26 [root] INFO: {'player_contract': '5年2.01亿美元，2017年夏天签，2022年夏天到期，15%交易保证金',
 'player_height': '1.91米/6尺3',
 'player_name': '斯蒂芬-库里',
 'player_num': '30',
 'player_position': 'G',
 'player_salary': '本年薪金：4023万美元',
 'player_team': '金州勇士',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:27 [root] INFO: {'player_contract': '5年2.06亿美元，2017年夏天签，2022年有球员选项，2023年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '拉塞尔-威斯布鲁克',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：3850万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:28 [root] INFO: {'player_contract': '4年1.56亿美元，2018年夏天签，2022年到期，2021-22赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '勒布朗-詹姆斯',
 'player_num': '23',
 'player_position': 'F-G',
 'player_salary': '本年薪金：3744万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年999万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项\n'
                    '2018年7月提前续约5年1.58亿美元，2019年夏天生效',
 'player_height': '1.98米/6尺6',
 'player_name': '德文-布克',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '93公斤/206磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '5年1.9亿美元，2019年夏天签，2024年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '克莱-汤普森',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：3274万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年1.17亿美元，2019年夏天签，2023年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '丹吉洛-拉塞尔',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：2728万美元',
 'player_team': '金州勇士',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年1亿美元提前续约，2019年夏天签，2023-24赛季为球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '德雷蒙德-格林',
 'player_num': '2',
 'player_position': 'F',
 'player_salary': '本年薪金：1854万美元',
 'player_team': '金州勇士',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年1535万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '威利-考利-斯坦',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：470万美元',
 'player_team': '金州勇士',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '3年1500万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '凯文-卢尼',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：462万美元',
 'player_team': '金州勇士',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年232万美元，2019年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '亚历克-伯克斯',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：232万美元',
 'player_team': '金州勇士',
 'player_weight': '97公斤/214磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年403万美元，2019年夏天签',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-普尔',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：196万美元',
 'player_team': '金州勇士',
 'player_weight': '88公斤/194磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年922万美元，2018年夏天签，2022年夏天到期，其中2021年2022年俱乐部选项',
 'player_height': '1.98米/6尺6',
 'player_name': '雅各布-埃文斯',
 'player_num': '22',
 'player_position': 'G',
 'player_salary': '本年薪金：192万美元',
 'player_team': '金州勇士',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年909万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '奥马里-斯佩尔曼',
 'player_num': '28',
 'player_position': 'F',
 'player_salary': '本年薪金：190万美元',
 'player_team': '金州勇士',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年188万美元，2019年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '格伦-罗宾逊三世',
 'player_num': '6',
 'player_position': 'G-F',
 'player_salary': '本年薪金：188万美元',
 'player_team': '金州勇士',
 'player_weight': '101公斤/222磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年277万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '阿方索-麦金尼',
 'player_num': '32',
 'player_position': 'F',
 'player_salary': '本年薪金：142万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2016年夏天在旧合同期内以4年1.18亿美元提前续约至2020年，2017年夏天再次以4年1.7亿美元提前续约至2023年，2022-23赛季球员选项',
 'player_height': '1.96米/6尺5',
 'player_name': '詹姆斯-哈登',
 'player_num': '13',
 'player_position': 'G',
 'player_salary': '本年薪金：3819万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '5年9000万美元，2018年夏天签，2023年夏天到期，其中8000万美元保障，每赛季火箭进入西决奖励100万美元，防守篮板率达到30%、罚球命中率达到65%各奖励50万美元',
 'player_height': '2.11米/6尺11',
 'player_name': '克林特-卡佩拉',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：1490万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年5289万美元，2016年夏天签，2020年夏天到期。',
 'player_height': '1.93米/6尺4',
 'player_name': '埃里克-戈登',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：1406万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '4年3188万美元，2017年夏天签，2021年夏天到期，2020-21赛季257万美元保障，2020年7月1日之前不被裁则转为全额保障',
 'player_height': '1.98米/6尺6',
 'player_name': 'PJ-塔克',
 'player_num': '17',
 'player_position': 'F',
 'player_salary': '本年薪金：835万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '3年1110万美元，2019年夏天签，2022年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '丹纽尔-豪斯',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：349万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '杰拉德-格林',
 'player_num': '14',
 'player_position': 'G-F',
 'player_salary': '本年薪金：256万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.16米/7尺1',
 'player_name': '泰森-钱德勒',
 'player_num': '19',
 'player_position': 'C',
 'player_salary': '本年薪金：256万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年底薪，2019年夏天签',
 'player_height': '1.93米/6尺4',
 'player_name': '奥斯汀-里弗斯',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：217万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年174万美元的非保障合同',
 'player_height': '2.03米/6尺8',
 'player_name': '安东尼-本内特',
 'player_num': '30',
 'player_position': 'F',
 'player_salary': '本年薪金：174万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '3年392万美元，2018年夏天签，2021年夏天到期',
 'player_height': '2.13米/7尺0',
 'player_name': '以赛亚-哈尔滕施泰因',
 'player_num': '55',
 'player_position': 'F-C',
 'player_salary': '本年薪金：142万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '113公斤/249磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '3年367万美元，2018年12月签，其中19-20赛季50%保障，20-21赛季为无保障合同，2021年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '加里-克拉克',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：141万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '5年1.3亿美元，2015年夏天签，2016年夏天生效，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.08米/6尺10',
 'player_name': '安东尼-戴维斯',
 'player_num': '3',
 'player_position': 'F-C',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '115公斤/253磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年3000万美元，2019年夏天签，2021年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '丹尼-格林',
 'player_num': '14',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1463万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年1600万美元，2019年夏天签，2021年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '肯塔维厄斯-考德威尔-波普',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：769万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年970万美元，2019年夏天签，2021年到期，2020年球员选项',
 'player_height': '1.88米/6尺2',
 'player_name': '埃弗里-布拉德利',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：476万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年820万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '贾维尔-麦基',
 'player_num': '7',
 'player_position': 'C',
 'player_salary': '本年薪金：394万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '122公斤/270磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年350万美元，2019年夏天签，2020年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '德马库斯-考辛斯',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：350万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '122公斤/270磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年600万美元，2019年夏天签，2021年到期',
 'player_height': '1.88米/6尺2',
 'player_name': '奎因-库克',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：292万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '2年550万美元，2019年夏天签，2021年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '亚历克斯-卡鲁索',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：264万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:16:29 [root] INFO: {'player_contract': '1年260万美元，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '贾里德-杜德利',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：260万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '2016年10月以4年8400万美元提前续约，2017年夏天生效，2021年夏天到期，每年25万不易达到的激励奖金',
 'player_height': '1.93米/6尺4',
 'player_name': '维克托-奥拉迪波',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：2100万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '4年5000万美元，2016年夏天签，2020年夏天到期，2019-20赛季球员选项，15%的交易保证金',
 'player_height': '1.93米/6尺4',
 'player_name': '泰勒-约翰逊',
 'player_num': '16',
 'player_position': 'G',
 'player_salary': '本年薪金：1963万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '2年3000万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '凯利-乌布雷',
 'player_num': '3',
 'player_position': 'F',
 'player_salary': '本年薪金：1442万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '4年4037万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '德安德烈-艾顿',
 'player_num': '22',
 'player_position': 'C',
 'player_salary': '本年薪金：956万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '弗兰克-卡明斯基',
 'player_num': '8',
 'player_position': 'F-C',
 'player_salary': '本年薪金：476万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '4年1075万美元，2016年夏天签，2020年夏天到期。',
 'player_height': '2.08米/6尺10',
 'player_name': '达里奥-沙里奇',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：348万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '101公斤/223磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '2年226万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '杰文-卡特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:30 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.98米/6尺6',
 'player_name': '乔治-金',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:31 [root] INFO: {'player_contract': '3年4800万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '安德烈-伊格达拉',
 'player_num': '9',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1719万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:32 [root] INFO: {'player_contract': '4年1.37亿美元，2018年夏天签，2022年夏天到期，2021年球员选项',
 'player_height': '2.06米/6尺9',
 'player_name': '保罗-乔治',
 'player_num': '21',
 'player_position': 'F',
 'player_salary': '本年薪金：3300万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:33 [root] INFO: {'player_contract': '4年7095万美元，2017年夏天签，2021年夏天到期，15%交易保证金，2020-2021赛季球员选项',
 'player_height': '1.98米/6尺6',
 'player_name': '小蒂姆-哈达威',
 'player_num': '11',
 'player_position': 'F-G',
 'player_salary': '本年薪金：1815万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:34 [root] INFO: {'player_contract': '5年1.39亿，2016年夏天签',
 'player_height': '2.01米/6尺7',
 'player_name': '德马尔-德罗赞',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：2773万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2年2650万美元，2019年夏天签，2021年到期',
 'player_height': '1.93米/6尺4',
 'player_name': 'JJ-雷迪克',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：1292万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '阿伦-斯马伊拉吉奇',
 'player_num': '1',
 'player_position': 'F-C',
 'player_salary': '本年薪金：89万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/216磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.01米/6尺7',
 'player_name': '马库斯-德里克森',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '金州勇士',
 'player_weight': '113公斤/249磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '1年89万美元',
 'player_height': '1.75米/5尺9',
 'player_name': '克里斯-克莱蒙斯',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '82公斤/180磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '1年89万美元',
 'player_height': '1.96米/6尺5',
 'player_name': '威廉-麦克道尔-怀特',
 'player_num': '21',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '1年89万美元，2019年夏天签',
 'player_height': '1.85米/6尺1',
 'player_name': '沙莫里-庞兹',
 'player_num': '16',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '79公斤/174磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2年518万美元，2019年夏天签，2021年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '拉简-隆多',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：256万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '84公斤/186磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '1年210万美元，2019年夏天签，2020年到期',
 'player_height': '1.93米/6尺4',
 'player_name': '特洛伊-丹尼尔斯',
 'player_num': '30',
 'player_position': 'G',
 'player_salary': '本年薪金：210万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '4年865万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '凯尔-库兹马',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：197万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2年242万美元，2019年夏天签',
 'player_height': '1.93米/6尺4',
 'player_name': '塔伦-霍顿-塔克',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：89万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '双向合同，2019年夏天签',
 'player_height': '2.01米/6尺7',
 'player_name': '小扎克-诺维尔',
 'player_num': '17',
 'player_position': 'F-G',
 'player_salary': '本年薪金：万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '4年8500万美元，2019年夏天签，2023年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '马尔科姆-布罗格登',
 'player_num': '7',
 'player_position': 'G',
 'player_salary': '本年薪金：1976万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2018年10月提前续约4年8000万美元，其中奖金部分为800万美元',
 'player_height': '2.11米/6尺11',
 'player_name': '迈尔斯-特纳',
 'player_num': '33',
 'player_position': 'C-F',
 'player_salary': '本年薪金：1750万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '110公斤/243磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2017年9月以4年4700万美元提前续约，2018年夏天生效，2022年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': 'TJ-沃伦',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：1081万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '3年3150万美元，2019年夏天签，2022年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '杰里米-兰姆',
 'player_num': '26',
 'player_position': 'G',
 'player_salary': '本年薪金：1000万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '3年2200万美元，2018年夏天签，2021年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '道格-麦克德莫特',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：733万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '99公斤/219磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '1年480万美元，2019年夏天签，2020年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '贾斯廷-霍勒迪',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：480万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '4年1118万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '多曼塔斯-萨博尼斯',
 'player_num': '11',
 'player_position': 'F',
 'player_salary': '本年薪金：353万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2年700万美元，2019年夏天签，2021年到期',
 'player_height': '1.88米/6尺2',
 'player_name': 'TJ-麦康奈尔',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：341万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '2年577万美元，2019年夏天签',
 'player_height': '2.11米/6尺11',
 'player_name': '戈加-比塔泽',
 'player_num': '88',
 'player_position': 'C-F',
 'player_salary': '本年薪金：281万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '4年1158万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': 'TJ-利夫',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：281万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '4年1047万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.85米/6尺1',
 'player_name': '阿龙-霍勒迪',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：224万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:35 [root] INFO: {'player_contract': '3年650万美元，2019年夏天签，2022年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '埃德蒙-萨姆纳',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：200万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:36 [root] INFO: {'player_contract': '5年1.13亿美元，2015年夏天签，2020年夏天到期。2018年7月，以4年1.2亿美元提前续约，2019年夏天生效，2023年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '凯文-乐福',
 'player_num': '0',
 'player_position': 'F-C',
 'player_salary': '本年薪金：2894万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '114公斤/251磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '3年4500万美元，2019年夏天签，2022年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '约纳斯-瓦兰丘纳斯',
 'player_num': '17',
 'player_position': 'C',
 'player_salary': '本年薪金：1388万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年4800万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '所罗门-希尔',
 'player_num': '44',
 'player_position': 'F',
 'player_salary': '本年薪金：1276万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年5000万，2016年夏天签，2020年夏天到期，每年60万美元不易达到的激励奖金',
 'player_height': '2.11米/6尺11',
 'player_name': '迈尔斯-普拉姆利',
 'player_num': '25',
 'player_position': 'C',
 'player_salary': '本年薪金：1250万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '113公斤/249磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年3716万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '凯尔-安德森',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：907万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '新秀合同，2019年签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '贾-莫兰特',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：873万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '3年2400万美元，2019年夏天签，2022年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '泰厄斯-琼斯',
 'player_num': '21',
 'player_position': 'G',
 'player_salary': '本年薪金：800万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '5年3500万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '杰-克劳德',
 'player_num': '99',
 'player_position': 'F',
 'player_salary': '本年薪金：791万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年2712万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '约什-杰克逊',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：706万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年2928万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '小贾伦-杰克逊',
 'player_num': '13',
 'player_position': 'F',
 'player_salary': '本年薪金：693万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '110公斤/242磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '布兰登-克拉克',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：247万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年1109万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季无保障2020和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '格雷森-阿伦',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：243万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '3年4000万美元，2019年夏天签，2022年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '帕特里克-贝弗利',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：1234万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年4200万，2016年夏天签，2020年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '莫里斯-哈克利斯',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：1101万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '3年2400万美元，2018年夏天签，2021年夏天到期',
 'player_height': '1.85米/6尺1',
 'player_name': '路易斯-威廉姆斯',
 'player_num': '40',
 'player_position': 'G',
 'player_salary': '本年薪金：800万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年2800万美元，2019年夏天签，2023年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '伊维察-祖巴茨',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：625万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '2年1200万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '蒙特雷兹-哈勒尔',
 'player_num': '1',
 'player_position': 'F-C',
 'player_salary': '本年薪金：600万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年1569万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '杰罗姆-罗宾逊',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：357万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年4800万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '考特尼-李',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：1276万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '3年3300万美元，2019年夏天签',
 'player_height': '2.11米/6尺11',
 'player_name': '德怀特-鲍威尔',
 'player_num': '7',
 'player_position': 'F-C',
 'player_salary': '本年薪金：956万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年3500万美元，2019年夏天签，2023年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '马克西-克勒贝尔',
 'player_num': '42',
 'player_position': 'F',
 'player_salary': '本年薪金：781万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年3246万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '卢卡-东契奇',
 'player_num': '77',
 'player_position': 'G-F',
 'player_salary': '本年薪金：768万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '103公斤/228磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '4年1348万美元，2017年夏天签，2021年夏天到期，2019-20赛季、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '贾斯廷-杰克逊',
 'player_num': '44',
 'player_position': 'F',
 'player_salary': '本年薪金：328万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:37 [root] INFO: {'player_contract': '三年 480万美元',
 'player_height': '2.03米/6尺8',
 'player_name': '以赛亚-罗比',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：150万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '104公斤/229磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '4年1.06亿美元，2017年夏天签，2021年夏天到期，2020-2021赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '奥托-波特',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '4年8407万美元，2015年夏天签，2019年夏天到期，2018-19赛季球员选项；2017年10月以3年7230万美元提前续约，2018年夏天生效，2021年到期，如果2020年6月29日之前被裁',
 'player_height': '2.11米/6尺11',
 'player_name': '拉马库斯-阿尔德里奇',
 'player_num': '12',
 'player_position': 'F',
 'player_salary': '本年薪金：2600万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '2年3200万美元，2019年夏天签，2021年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '鲁迪-盖伊',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：1560万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '2年1100万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '特雷-莱尔斯',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：550万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '106公斤/234磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '4年1246万美元，2018年夏天签，2022年夏天到期，2020年和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '朗尼-沃克',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：276万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '93公斤/204磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '卢卡-沙马尼奇',
 'player_num': '19',
 'player_position': 'F',
 'player_salary': '本年薪金：268万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '103公斤/227磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '4年854万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '德里克-怀特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：195万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '蔡恩-威廉森',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：975万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '129公斤/285磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '3年 2247万美元合同，2017年夏天签',
 'player_height': '1.98米/6尺6',
 'player_name': '朗佐-鲍尔',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：872万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '4年2382万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '布兰登-英格拉姆',
 'player_num': '14',
 'player_position': 'F',
 'player_salary': '本年薪金：727万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '贾克森-海斯',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：486万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '3年500万美元，2017年夏天签',
 'player_height': '1.96米/6尺5',
 'player_name': '约什-哈特',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：193万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '2年327万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '贾利尔-奥卡福',
 'player_num': '8',
 'player_position': 'C',
 'player_salary': '本年薪金：170万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '125公斤/275磅'}
2019-08-20 17:16:38 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季50.6万美元保障',
 'player_height': '1.91米/6尺3',
 'player_name': '弗兰克-杰克逊',
 'player_num': '15',
 'player_position': 'G',
 'player_salary': '本年薪金：162万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:38 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-08-20 17:16:39 [root] INFO: {'player_contract': '5年1.71亿美元，2017年夏天签，2022年夏天到，2021-22赛季球员选项',
 'player_height': '2.08米/6尺10',
 'player_name': '布雷克-格里芬',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：3423万美元',
 'player_team': '底特律活塞',
 'player_weight': '114公斤/251磅'}
2019-08-20 17:16:39 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-08-20 17:16:40 [root] INFO: {'player_contract': '5年1.78亿美元，2019年夏天签，2024年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '克里斯-米德尔顿',
 'player_num': '22',
 'player_position': 'F-G',
 'player_salary': '本年薪金：3068万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '106公斤/234磅'}
2019-08-20 17:16:41 [root] INFO: {'player_contract': '5年8478万，2013年夏天签，2014年夏天生效，2019年夏天到期；2017年夏天以4年1.7亿美元提前续约至2023年，2022-23赛季球员选项',
 'player_height': '1.93米/6尺4',
 'player_name': '约翰-沃尔',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：3780万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:42 [root] INFO: {'player_contract': '4年1亿美元，2019年夏天签，2023年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '尼古拉-武切维奇',
 'player_num': '9',
 'player_position': 'C',
 'player_salary': '本年薪金：2800万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '118公斤/260磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '5年1.2亿美元，2016年夏天签，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '尼古拉斯-巴图姆',
 'player_num': '5',
 'player_position': 'G-F',
 'player_salary': '本年薪金：2557万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '2年225万美元，2018年夏天签，2020年夏天到期，其中包含84万的保证金',
 'player_height': '2.06米/6尺9',
 'player_name': '阿利兹-约翰逊 ',
 'player_num': '24',
 'player_position': 'F',
 'player_salary': '本年薪金：141万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '91公斤/201磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.98米/6尺6',
 'player_name': '达文-里德',
 'player_num': '32',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '2019年夏天，签下Exhibit 10合同',
 'player_height': '2.03米/6尺8',
 'player_name': '杰基南-甘特',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '98公斤/216磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '5年8200万美元，2015年夏天签，2020年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '特里斯坦-汤普森',
 'player_num': '13',
 'player_position': 'C-F',
 'player_salary': '本年薪金：1854万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '108公斤/238磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '5年7000万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '布兰登-奈特',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：1564万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '4年5000万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-克拉克森',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：1344万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '88公斤/194磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '4年590万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项。\n'
                    '2018年10月提前续约4年4500万美元，2019年夏天生效',
 'player_height': '2.06米/6尺9',
 'player_name': '小拉里-南斯',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：1270万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '4年4400万美元，2015年10月签，2016年夏天生效，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '约翰-亨森',
 'player_num': '31',
 'player_position': 'C-F',
 'player_salary': '本年薪金：973万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '104公斤/229磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '4年3840万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '马修-戴拉维多瓦',
 'player_num': '18',
 'player_position': 'G',
 'player_salary': '本年薪金：960万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '90公斤/198磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.88米/6尺2',
 'player_name': '达里厄斯-加兰',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：640万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '78公斤/173磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '4年2017万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '科林-塞克斯顿',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：476万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '3年832万美元，2017年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '切迪-奥斯曼',
 'player_num': '16',
 'player_position': 'F',
 'player_salary': '本年薪金：290万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '98公斤/215磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '1年239万美元，2018年夏天签，2019年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '钱宁-弗莱',
 'player_num': '9',
 'player_position': 'F-C',
 'player_salary': '本年薪金：239万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '4年975万美元，2017年夏天签，2021年夏天到期，2019-2020、2020-2021赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '安特-日日奇',
 'player_num': '41',
 'player_position': 'C',
 'player_salary': '本年薪金：228万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '小凯文-波特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：129万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '97公斤/213磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.01米/6尺7',
 'player_name': '杰朗-布洛瑟姆盖姆',
 'player_num': '4',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:43 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '马基斯-克里斯',
 'player_num': '3',
 'player_position': 'F',
 'player_salary': '本年薪金：万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '106公斤/233磅'}
2019-08-20 17:16:43 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1.42亿美元，2019年夏天签，2023年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '吉米-巴特勒',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：3274万美元',
 'player_team': '迈阿密热火',
 'player_weight': '105公斤/231磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年237万美元，2019年2月签，2020年夏天到期，2019-20赛季无保障',
 'player_height': '2.06米/6尺9',
 'player_name': '布鲁诺-卡博克洛',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：185万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季无保障，2019年7月5日之前不被裁则转为全额保障 ',
 'player_height': '1.98米/6尺6',
 'player_name': '狄龙-布鲁克斯',
 'player_num': '24',
 'player_position': 'G-F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年395万美元，2017年夏天签，2020年夏天到期，2019-20赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '伊万-拉布',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年230万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '丹东尼-梅尔顿',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：135万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.06米/6尺9',
 'player_name': '渡边雄太',
 'player_num': '18',
 'player_position': 'G-F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年7800万美元，2018年夏天签，2022年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '扎克-拉文',
 'player_num': '8',
 'player_position': 'G',
 'player_salary': '本年薪金：1950万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年4100万美元，2019年夏天签',
 'player_height': '2.03米/6尺8',
 'player_name': '赛迪斯-杨',
 'player_num': '21',
 'player_position': 'F',
 'player_salary': '本年薪金：1301万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '100公斤/221磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年3000万美元，2019年夏天签，2022年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '托马什-萨托兰斯基',
 'player_num': '31',
 'player_position': 'G-F',
 'player_salary': '本年薪金：952万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年3200万美元，2017年夏天签，2021年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '克里斯蒂亚诺-费利西奥',
 'player_num': '6',
 'player_position': 'F-C',
 'player_salary': '本年薪金：816万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '121公斤/266磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1749万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '克里斯-邓恩',
 'player_num': '32',
 'player_position': 'G',
 'player_salary': '本年薪金：535万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '科比-怀特',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：530万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '84公斤/185磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年2039万美元，2017年夏天签，2021年夏天到期，2019-2020、2020-2021赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '劳里-马尔卡宁',
 'player_num': '24',
 'player_position': 'F',
 'player_salary': '本年薪金：530万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年2201万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '温德尔-卡特',
 'player_num': '34',
 'player_position': 'F',
 'player_salary': '本年薪金：520万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '117公斤/259磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年805万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '登泽尔-瓦伦丁',
 'player_num': '45',
 'player_position': 'G',
 'player_salary': '本年薪金：338万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '96公斤/212磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年900万美元，2019年夏天签',
 'player_height': '1.91米/6尺3',
 'player_name': '里安-阿尔奇迪亚科诺',
 'player_num': '51',
 'player_position': 'G',
 'player_salary': '本年薪金：300万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1078万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '钱德勒-哈奇森',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：233万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '89公斤/196磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年162万美元，2018年夏天签，2019年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '卢克-科内特',
 'player_num': '2',
 'player_position': 'F-C',
 'player_salary': '本年薪金：162万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '113公斤/250磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年294万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '安东尼奥-布莱克尼',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：159万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '89公斤/197磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '5年1.3亿美元，2016年夏天签，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.11米/6尺11',
 'player_name': '安德烈-德拉蒙德',
 'player_num': '0',
 'player_position': 'C',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '底特律活塞',
 'player_weight': '127公斤/279磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '5年8000万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '雷吉-杰克逊',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：1809万美元',
 'player_team': '底特律活塞',
 'player_weight': '94公斤/208磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年4600万美元，2017年夏天签，2021年夏天到期，2020-2021赛季球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '托尼-斯内尔',
 'player_num': '17',
 'player_position': 'G',
 'player_salary': '本年薪金：1139万美元',
 'player_team': '底特律活塞',
 'player_weight': '98公斤/217磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年2100万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '兰斯顿-加洛韦',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：733万美元',
 'player_team': '底特律活塞',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年1500万美元，2019年夏天签，2021年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '德里克-罗斯',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：731万美元',
 'player_team': '底特律活塞',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1514万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '卢克-肯纳德',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：383万美元',
 'player_team': '底特律活塞',
 'player_weight': '91公斤/200磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年360万美元，2019年夏天签，2020年到期',
 'player_height': '2.08米/6尺10',
 'player_name': '马基夫-莫里斯',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：360万美元',
 'player_team': '底特律活塞',
 'player_weight': '111公斤/245磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年931万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '索恩-梅克',
 'player_num': '7',
 'player_position': 'C-F',
 'player_salary': '本年薪金：357万美元',
 'player_team': '底特律活塞',
 'player_weight': '101公斤/223磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '塞古-敦布亚',
 'player_num': '45',
 'player_position': 'F',
 'player_salary': '本年薪金：328万美元',
 'player_team': '底特律活塞',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年188万美元，2019年夏天签，2020年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '蒂姆-弗雷泽',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：188万美元',
 'player_team': '底特律活塞',
 'player_weight': '77公斤/170磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年392万美元，2018年夏天签，2021年夏天到期，2020-21赛季无保障，2020年7月10日前不被裁则全额保障',
 'player_height': '1.96米/6尺5',
 'player_name': '布鲁斯-布朗',
 'player_num': '6',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '底特律活塞',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年457万美元，2018年夏天签，2021年夏天到期，2019-20、2020-21赛季无保障，2019年7月5日、2020年7月5日前不被裁则当赛季转为全额保障',
 'player_height': '2.03米/6尺8',
 'player_name': '斯维亚托斯拉夫-米哈伊柳克',
 'player_num': '19',
 'player_position': 'F',
 'player_salary': '本年薪金：142万美元',
 'player_team': '底特律活塞',
 'player_weight': '87公斤/192磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年84万美元，2018年夏天签，2019年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '凯里-托马斯',
 'player_num': '13',
 'player_position': 'G',
 'player_salary': '本年薪金：84万美元',
 'player_team': '底特律活塞',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.85米/6尺1',
 'player_name': '卡林-卢卡斯',
 'player_num': '24',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '底特律活塞',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1亿美元。2017年夏天签，2021年夏天到期。',
 'player_height': '2.11米/6尺11',
 'player_name': '扬尼斯-阿德托昆博',
 'player_num': '34',
 'player_position': 'F',
 'player_salary': '本年薪金：2584万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '101公斤/222磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '5年7000万美元，2014年夏天签，2019年夏天到期',
 'player_height': '1.85米/6尺1',
 'player_name': '埃里克-布莱索',
 'player_num': '6',
 'player_position': 'G',
 'player_salary': '本年薪金：1562万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年5200万美元，2019年夏天签，2023年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '布鲁克-洛佩斯',
 'player_num': '11',
 'player_position': 'C',
 'player_salary': '本年薪金：1209万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '122公斤/268磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年2900万美元，2019年夏天签，2022年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '乔治-希尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：920万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '85公斤/188磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年2100万美元，2018年夏天签，2021年夏天到期，2020-21赛季无保障',
 'player_height': '2.08米/6尺10',
 'player_name': '艾森-伊利亚索瓦',
 'player_num': '77',
 'player_position': 'F',
 'player_salary': '本年薪金：700万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '107公斤/235磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1218万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': 'DJ-威尔森',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：296万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1309万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '唐特-迪温琴佐',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：290万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年256万美元，2019年夏天签，2020年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '韦斯利-马修斯',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：256万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年336万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '帕特-康诺顿',
 'player_num': '24',
 'player_position': 'G-F',
 'player_salary': '本年薪金：172万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年350万美元，2019年夏天签，2021年到期',
 'player_height': '2.16米/7尺1',
 'player_name': '德拉甘-本德尔',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：167万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '斯特林-布朗',
 'player_num': '43',
 'player_position': 'G',
 'player_salary': '本年薪金：162万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '104公斤/230磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '5年1.27亿美元，2016年夏天签，2021年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '布拉德利-比尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '94公斤/207磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年1450万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '戴维斯-贝尔坦斯',
 'player_num': '17',
 'player_position': 'F',
 'player_salary': '本年薪金：700万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年1200万美元，2019年夏天签，2021年到期',
 'player_height': '1.83米/6尺0',
 'player_name': '伊斯梅尔-史密斯',
 'player_num': '5',
 'player_position': 'G',
 'player_salary': '本年薪金：585万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '79公斤/175磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年408万美元，2018年夏天签，2020年夏天到期，2020-21赛季非保障',
 'player_height': '2.03米/6尺8',
 'player_name': '伊萨克-邦加',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年420万美元，2019年夏天签',
 'player_height': '1.88米/6尺2',
 'player_name': '贾斯廷-罗宾逊',
 'player_num': '52',
 'player_position': 'G',
 'player_salary': '本年薪金：90万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2022年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '阿德米拉尔-斯科菲尔德',
 'player_num': '9',
 'player_position': 'F',
 'player_salary': '本年薪金：89万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '109公斤/241磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-麦克雷',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '81公斤/179磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年147万美元，2018年夏天签，2020年夏天到期，2019-20赛季非保障',
 'player_height': '1.96米/6尺5',
 'player_name': '杰梅里奥-琼斯',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：5万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '91公斤/201磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年8000万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '阿龙-戈登',
 'player_num': '00',
 'player_position': 'F',
 'player_salary': '本年薪金：1986万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '5年8500万，2016年夏天签，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '埃文-富尼耶',
 'player_num': '10',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1700万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年5400万美元，2019年夏天签，2023年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '特伦斯-罗斯',
 'player_num': '8',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1250万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '88公斤/195磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年3740万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '马克尔-富尔茨',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：975万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年2900万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '艾尔-法鲁克-阿米奴',
 'player_num': '2',
 'player_position': 'F',
 'player_salary': '本年薪金：925万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年2900万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.83米/6尺0',
 'player_name': 'DJ-奥古斯丁',
 'player_num': '14',
 'player_position': 'G',
 'player_salary': '本年薪金：725万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '83公斤/183磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年2232万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '乔纳森-艾萨克',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：580万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年2410万美元，2018年夏天签，2022年夏天到期，2020年和2021年球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '穆罕默德-班巴',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：570万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '2年600万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '肯-伯奇',
 'player_num': '24',
 'player_position': 'C-F',
 'player_salary': '本年薪金：300万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年202万美元，2019年夏天签，2020年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '迈克尔-卡特-威廉姆斯',
 'player_num': '7',
 'player_position': 'G',
 'player_salary': '本年薪金：202万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年405万美元，2017年夏天签，2020年夏天到期，2019-20赛季球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '韦斯利-艾旺杜',
 'player_num': '25',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '93公斤/205磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年621万美元，2018年夏天签，2022年夏天到期，2021年球队选项',
 'player_height': '1.98米/6尺6',
 'player_name': '梅尔文-弗雷泽',
 'player_num': '35',
 'player_position': 'F-G',
 'player_salary': '本年薪金：151万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '91公斤/201磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.93米/6尺4',
 'player_name': '特洛伊-科佩恩',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '95公斤/210磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年5800万美元，2019年夏天签，2022年到期',
 'player_height': '1.88米/6尺2',
 'player_name': '特里-罗齐尔',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：1841万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1740万美元，2013年夏天签，2017年夏天到期，2016年10月以4年5600万美元续约，2017年夏天生效，2021年夏天到期',
 'player_height': '2.13米/7尺0',
 'player_name': '科迪-泽勒',
 'player_num': '40',
 'player_position': 'C',
 'player_salary': '本年薪金：1447万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': 'PJ-华盛顿',
 'player_num': '25',
 'player_position': 'F',
 'player_salary': '本年薪金：383万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '103公斤/228磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年1572万美元，2018年夏天签，2022年夏天到期，2020-21赛季、2021-22赛季球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '迈尔斯-布里奇斯',
 'player_num': '0',
 'player_position': 'F',
 'player_salary': '本年薪金：375万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '102公斤/225磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '4年586万美元，2016年夏天签，2020年夏天到期，2019-20赛季无保障',
 'player_height': '2.11米/6尺11',
 'player_name': '吉列尔莫-埃尔南戈麦斯',
 'player_num': '41',
 'player_position': 'C',
 'player_salary': '本年薪金：156万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '109公斤/240磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '1年144万美元',
 'player_height': '2.13米/7尺0',
 'player_name': '托马斯-韦尔什',
 'player_num': '7',
 'player_position': 'C',
 'player_salary': '本年薪金：144万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '116公斤/255磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季无保障，2019年8月1日前不被裁则转为全额保障',
 'player_height': '2.01米/6尺7',
 'player_name': '德维恩-培根',
 'player_num': '11',
 'player_position': 'G-F',
 'player_salary': '本年薪金：138万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '100公斤/220磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '3年447万美元合同，2019年夏天签',
 'player_height': '1.98米/6尺6',
 'player_name': '科迪-马丁',
 'player_num': '31',
 'player_position': 'G',
 'player_salary': '本年薪金：117万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '87公斤/191磅'}
2019-08-20 17:16:44 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.93米/6尺4',
 'player_name': '乔-奇利',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：7万美元',
 'player_team': '夏洛特黄蜂',
 'player_weight': '86公斤/190磅'}
2019-08-20 17:16:45 [root] INFO: {'player_contract': '4年9444万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '钱德勒-帕森斯',
 'player_num': '25',
 'player_position': 'F',
 'player_salary': '本年薪金：2510万美元',
 'player_team': '亚特兰大老鹰',
 'player_weight': '104公斤/230磅'}
2019-08-20 19:16:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:16:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:16:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:16:12 [scrapy.extensions.telnet] INFO: Telnet Password: 9c2cee4d7cfa2814
2019-08-20 19:16:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:16:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:16:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:16:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:16:12 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:16:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:16:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:16:13 [root] INFO: {'player_contract': '4年8800万美元，2019年夏天签，2023年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '哈里森-巴恩斯',
 'player_num': '40',
 'player_position': 'F',
 'player_salary': '本年薪金：1897万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '102公斤/225磅'}
2019-08-20 19:16:14 [root] INFO: {'player_contract': '3年4800万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '安德烈-伊格达拉',
 'player_num': '9',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1719万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '5年1.39亿，2016年夏天签',
 'player_height': '2.01米/6尺7',
 'player_name': '德马尔-德罗赞',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：2773万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '3年4100万美元，2019年夏天签，2022年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '德维恩-戴德蒙',
 'player_num': '0',
 'player_position': 'C',
 'player_salary': '本年薪金：1269万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '111公斤/245磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '2年2500万美元，2019年夏天签，2021年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '特雷沃-阿里扎',
 'player_num': '9',
 'player_position': 'F',
 'player_salary': '本年薪金：1219万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '3年3700万美元，2019年夏天签，2022年到期',
 'player_height': '1.91米/6尺3',
 'player_name': '科里-约瑟夫',
 'player_num': '35',
 'player_position': 'G',
 'player_salary': '本年薪金：1174万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '86公斤/190磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年3613万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '2.11米/6尺11',
 'player_name': '马文-巴格利',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：856万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '107公斤/235磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '3年2700万美元，2017年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '波格丹-波格丹诺维奇',
 'player_num': '88',
 'player_position': 'G',
 'player_salary': '本年薪金：853万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '3年2047万美元，2018年夏天签，2021年夏天到期，其中1332万受保障，2020-21赛季无保障',
 'player_height': '2.08米/6尺10',
 'player_name': '内马尼亚-别利察',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：682万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '102公斤/225磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年2457万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '达龙-福克斯',
 'player_num': '24',
 'player_position': 'G',
 'player_salary': '本年薪金：639万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '79公斤/175磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年1589万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '1.93米/6尺4',
 'player_name': '巴迪-希尔德',
 'player_num': '22',
 'player_position': 'G',
 'player_salary': '本年薪金：486万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '97公斤/214磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.08米/6尺10',
 'player_name': '里乔恩-霍姆斯',
 'player_num': '3',
 'player_position': 'F-C',
 'player_salary': '本年薪金：476万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '111公斤/245磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '2年615万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.83米/6尺0',
 'player_name': '约吉-费雷尔',
 'player_num': '20',
 'player_position': 'G',
 'player_salary': '本年薪金：315万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '82公斤/180磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年1062万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '哈里-贾尔斯',
 'player_num': '50',
 'player_position': 'F-C',
 'player_salary': '本年薪金：258万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '3年4500万美元，2019年夏天签，2022年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '约纳斯-瓦兰丘纳斯',
 'player_num': '17',
 'player_position': 'C',
 'player_salary': '本年薪金：1388万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '116公斤/255磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年4800万美元，2016年夏天签，2020年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '所罗门-希尔',
 'player_num': '44',
 'player_position': 'F',
 'player_salary': '本年薪金：1276万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '102公斤/225磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年5000万，2016年夏天签，2020年夏天到期，每年60万美元不易达到的激励奖金',
 'player_height': '2.11米/6尺11',
 'player_name': '迈尔斯-普拉姆利',
 'player_num': '25',
 'player_position': 'C',
 'player_salary': '本年薪金：1250万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '113公斤/249磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年3716万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '凯尔-安德森',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：907万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '104公斤/230磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '新秀合同，2019年签，2023年到期，2022年，2023年球队选项',
 'player_height': '1.91米/6尺3',
 'player_name': '贾-莫兰特',
 'player_num': '12',
 'player_position': 'G',
 'player_salary': '本年薪金：873万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '79公斤/175磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '3年2400万美元，2019年夏天签，2022年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '泰厄斯-琼斯',
 'player_num': '21',
 'player_position': 'G',
 'player_salary': '本年薪金：800万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '88公斤/195磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '5年3500万美元，2015年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '杰-克劳德',
 'player_num': '99',
 'player_position': 'F',
 'player_salary': '本年薪金：791万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '107公斤/235磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年2712万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '约什-杰克逊',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：706万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年2928万美元，2018年夏天签，2022年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '小贾伦-杰克逊',
 'player_num': '13',
 'player_position': 'F',
 'player_salary': '本年薪金：693万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '110公斤/242磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '布兰登-克拉克',
 'player_num': '15',
 'player_position': 'F',
 'player_salary': '本年薪金：247万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '4年1109万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季无保障2020和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '格雷森-阿伦',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：243万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:15 [root] INFO: {'player_contract': '2年237万美元，2019年2月签，2020年夏天到期，2019-20赛季无保障',
 'player_height': '2.06米/6尺9',
 'player_name': '布鲁诺-卡博克洛',
 'player_num': '5',
 'player_position': 'F',
 'player_salary': '本年薪金：185万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '5年2.01亿美元，2017年夏天签，2022年夏天到期，15%交易保证金',
 'player_height': '1.91米/6尺3',
 'player_name': '斯蒂芬-库里',
 'player_num': '30',
 'player_position': 'G',
 'player_salary': '本年薪金：4023万美元',
 'player_team': '金州勇士',
 'player_weight': '86公斤/190磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '4年8407万美元，2015年夏天签，2019年夏天到期，2018-19赛季球员选项；2017年10月以3年7230万美元提前续约，2018年夏天生效，2021年到期，如果2020年6月29日之前被裁',
 'player_height': '2.11米/6尺11',
 'player_name': '拉马库斯-阿尔德里奇',
 'player_num': '12',
 'player_position': 'F',
 'player_salary': '本年薪金：2600万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '118公斤/260磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '2年3200万美元，2019年夏天签，2021年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '鲁迪-盖伊',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：1560万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '104公斤/230磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '2年1100万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '特雷-莱尔斯',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：550万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '106公斤/234磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '4年1246万美元，2018年夏天签，2022年夏天到期，2020年和2021年球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '朗尼-沃克',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：276万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '93公斤/204磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '卢卡-沙马尼奇',
 'player_num': '19',
 'player_position': 'F',
 'player_salary': '本年薪金：268万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '103公斤/227磅'}
2019-08-20 19:16:17 [root] INFO: {'player_contract': '4年854万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '1.96米/6尺5',
 'player_name': '德里克-怀特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：195万美元',
 'player_team': '圣安东尼奥马刺',
 'player_weight': '91公斤/200磅'}
2019-08-20 19:16:18 [root] INFO: {'player_contract': '4年1.56亿美元，2018年夏天签，2022年到期，2021-22赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '勒布朗-詹姆斯',
 'player_num': '23',
 'player_position': 'F-G',
 'player_salary': '本年薪金：3744万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '113公斤/250磅'}
2019-08-20 19:16:19 [root] INFO: {'player_contract': '2年2650万美元，2019年夏天签，2021年到期',
 'player_height': '1.93米/6尺4',
 'player_name': 'JJ-雷迪克',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：1292万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 19:16:20 [root] INFO: {'player_contract': '4年7095万美元，2017年夏天签，2021年夏天到期，15%交易保证金，2020-2021赛季球员选项',
 'player_height': '1.98米/6尺6',
 'player_name': '小蒂姆-哈达威',
 'player_num': '11',
 'player_position': 'F-G',
 'player_salary': '本年薪金：1815万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:20 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-08-20 19:16:20 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-08-20 19:16:21 [root] INFO: {'player_contract': '4年1.37亿美元，2018年夏天签，2022年夏天到期，2021年球员选项',
 'player_height': '2.06米/6尺9',
 'player_name': '保罗-乔治',
 'player_num': '21',
 'player_position': 'F',
 'player_salary': '本年薪金：3300万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:22 [root] INFO: {'player_contract': '4年999万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项\n'
                    '2018年7月提前续约5年1.58亿美元，2019年夏天生效',
 'player_height': '1.98米/6尺6',
 'player_name': '德文-布克',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '93公斤/206磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '5年2.06亿美元，2017年夏天签，2022年有球员选项，2023年夏天到期',
 'player_height': '1.91米/6尺3',
 'player_name': '拉塞尔-威斯布鲁克',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：3850万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '91公斤/200磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年890万美元，2017年夏天签，2021年夏天到期，2019-20、2020-21赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '凯莱布-斯瓦尼根',
 'player_num': '41',
 'player_position': 'F',
 'player_salary': '本年薪金：203万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '113公斤/250磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年底薪，2019年签，2021年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '泰勒-莱登',
 'player_num': '32',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '萨克拉门托国王',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季无保障，2019年7月5日之前不被裁则转为全额保障 ',
 'player_height': '1.98米/6尺6',
 'player_name': '狄龙-布鲁克斯',
 'player_num': '24',
 'player_position': 'G-F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '102公斤/225磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年395万美元，2017年夏天签，2020年夏天到期，2019-20赛季球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '伊万-拉布',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：162万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年230万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.93米/6尺4',
 'player_name': '丹东尼-梅尔顿',
 'player_num': '0',
 'player_position': 'G',
 'player_salary': '本年薪金：135万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '86公斤/190磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.06米/6尺9',
 'player_name': '渡边雄太',
 'player_num': '18',
 'player_position': 'G-F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '孟菲斯灰熊',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '5年1.9亿美元，2019年夏天签，2024年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '克莱-汤普森',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：3274万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年1.17亿美元，2019年夏天签，2023年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '丹吉洛-拉塞尔',
 'player_num': '23',
 'player_position': 'G',
 'player_salary': '本年薪金：2728万美元',
 'player_team': '金州勇士',
 'player_weight': '88公斤/195磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年1亿美元提前续约，2019年夏天签，2023-24赛季为球员选项',
 'player_height': '2.01米/6尺7',
 'player_name': '德雷蒙德-格林',
 'player_num': '2',
 'player_position': 'F',
 'player_salary': '本年薪金：1854万美元',
 'player_team': '金州勇士',
 'player_weight': '104公斤/230磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年1535万美元，2015年夏天签，2019年夏天到期，2017-18赛季、2018-19赛季球队选项',
 'player_height': '2.13米/7尺0',
 'player_name': '威利-考利-斯坦',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：470万美元',
 'player_team': '金州勇士',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年1500万美元，2019年夏天签，2022年到期',
 'player_height': '2.06米/6尺9',
 'player_name': '凯文-卢尼',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：462万美元',
 'player_team': '金州勇士',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '1年232万美元，2019年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '亚历克-伯克斯',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：232万美元',
 'player_team': '金州勇士',
 'player_weight': '97公斤/214磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年403万美元，2019年夏天签',
 'player_height': '1.96米/6尺5',
 'player_name': '乔丹-普尔',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：196万美元',
 'player_team': '金州勇士',
 'player_weight': '88公斤/194磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年922万美元，2018年夏天签，2022年夏天到期，其中2021年2022年俱乐部选项',
 'player_height': '1.98米/6尺6',
 'player_name': '雅各布-埃文斯',
 'player_num': '22',
 'player_position': 'G',
 'player_salary': '本年薪金：192万美元',
 'player_team': '金州勇士',
 'player_weight': '95公斤/210磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年909万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '奥马里-斯佩尔曼',
 'player_num': '28',
 'player_position': 'F',
 'player_salary': '本年薪金：190万美元',
 'player_team': '金州勇士',
 'player_weight': '111公斤/245磅'}
2019-08-20 19:16:23 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-08-20 19:16:23 [root] INFO: {'player_contract': '5年1.3亿美元，2015年夏天签，2016年夏天生效，2021年夏天到期，2020-21赛季球员选项',
 'player_height': '2.08米/6尺10',
 'player_name': '安东尼-戴维斯',
 'player_num': '3',
 'player_position': 'F-C',
 'player_salary': '本年薪金：2709万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '115公斤/253磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年3000万美元，2019年夏天签，2021年到期',
 'player_height': '1.98米/6尺6',
 'player_name': '丹尼-格林',
 'player_num': '14',
 'player_position': 'G-F',
 'player_salary': '本年薪金：1463万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年1600万美元，2019年夏天签，2021年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '肯塔维厄斯-考德威尔-波普',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：769万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年970万美元，2019年夏天签，2021年到期，2020年球员选项',
 'player_height': '1.88米/6尺2',
 'player_name': '埃弗里-布拉德利',
 'player_num': '11',
 'player_position': 'G',
 'player_salary': '本年薪金：476万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '82公斤/180磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年820万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '贾维尔-麦基',
 'player_num': '7',
 'player_position': 'C',
 'player_salary': '本年薪金：394万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '122公斤/270磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '1年350万美元，2019年夏天签，2020年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '德马库斯-考辛斯',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：350万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '122公斤/270磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年600万美元，2019年夏天签，2021年到期',
 'player_height': '1.88米/6尺2',
 'player_name': '奎因-库克',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：292万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '81公斤/179磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年550万美元，2019年夏天签，2021年到期',
 'player_height': '1.96米/6尺5',
 'player_name': '亚历克斯-卡鲁索',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：264万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '84公斤/186磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '1年260万美元，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '贾里德-杜德利',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：260万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '102公斤/225磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年518万美元，2019年夏天签，2021年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '拉简-隆多',
 'player_num': '9',
 'player_position': 'G',
 'player_salary': '本年薪金：256万美元',
 'player_team': '洛杉矶湖人',
 'player_weight': '84公斤/186磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.01米/6尺7',
 'player_name': '蔡恩-威廉森',
 'player_num': '1',
 'player_position': 'F',
 'player_salary': '本年薪金：975万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '129公斤/285磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年 2247万美元合同，2017年夏天签',
 'player_height': '1.98米/6尺6',
 'player_name': '朗佐-鲍尔',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：872万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年2382万美元，2016年夏天签，2020年夏天到期，2018-19赛季、2019-20赛季球队选项',
 'player_height': '2.06米/6尺9',
 'player_name': '布兰登-英格拉姆',
 'player_num': '14',
 'player_position': 'F',
 'player_salary': '本年薪金：727万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '86公斤/190磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '贾克森-海斯',
 'player_num': '10',
 'player_position': 'F',
 'player_salary': '本年薪金：486万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年500万美元，2017年夏天签',
 'player_height': '1.96米/6尺5',
 'player_name': '约什-哈特',
 'player_num': '3',
 'player_position': 'G',
 'player_salary': '本年薪金：193万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年327万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.11米/6尺11',
 'player_name': '贾利尔-奥卡福',
 'player_num': '8',
 'player_position': 'C',
 'player_salary': '本年薪金：170万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '125公斤/275磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年381万美元，2017年夏天签，2020年夏天到期，2019-20赛季50.6万美元保障',
 'player_height': '1.91米/6尺3',
 'player_name': '弗兰克-杰克逊',
 'player_num': '15',
 'player_position': 'G',
 'player_salary': '本年薪金：162万美元',
 'player_team': '新奥尔良鹈鹕',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年4800万美元，2016年夏天签，2020年夏天到期',
 'player_height': '1.96米/6尺5',
 'player_name': '考特尼-李',
 'player_num': '1',
 'player_position': 'G',
 'player_salary': '本年薪金：1276万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '91公斤/200磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年3300万美元，2019年夏天签',
 'player_height': '2.11米/6尺11',
 'player_name': '德怀特-鲍威尔',
 'player_num': '7',
 'player_position': 'F-C',
 'player_salary': '本年薪金：956万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年3500万美元，2019年夏天签，2023年到期',
 'player_height': '2.11米/6尺11',
 'player_name': '马克西-克勒贝尔',
 'player_num': '42',
 'player_position': 'F',
 'player_salary': '本年薪金：781万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年3246万美元，2018年夏天签，2022年夏天到期，2020和2021年球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '卢卡-东契奇',
 'player_num': '77',
 'player_position': 'G-F',
 'player_salary': '本年薪金：768万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '103公斤/228磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年1348万美元，2017年夏天签，2021年夏天到期，2019-20赛季、2020-21赛季球队选项',
 'player_height': '2.03米/6尺8',
 'player_name': '贾斯廷-杰克逊',
 'player_num': '44',
 'player_position': 'F',
 'player_salary': '本年薪金：328万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '95公斤/210磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '三年 480万美元',
 'player_height': '2.03米/6尺8',
 'player_name': '以赛亚-罗比',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：150万美元',
 'player_team': '达拉斯独行侠',
 'player_weight': '104公斤/229磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年4000万美元，2019年夏天签，2022年到期',
 'player_height': '1.85米/6尺1',
 'player_name': '帕特里克-贝弗利',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：1234万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '84公斤/185磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年4200万，2016年夏天签，2020年夏天到期',
 'player_height': '2.06米/6尺9',
 'player_name': '莫里斯-哈克利斯',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：1101万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年2400万美元，2018年夏天签，2021年夏天到期',
 'player_height': '1.85米/6尺1',
 'player_name': '路易斯-威廉姆斯',
 'player_num': '40',
 'player_position': 'G',
 'player_salary': '本年薪金：800万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '79公斤/175磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年2800万美元，2019年夏天签，2023年夏天到期',
 'player_height': '2.16米/7尺1',
 'player_name': '伊维察-祖巴茨',
 'player_num': '5',
 'player_position': 'C',
 'player_salary': '本年薪金：625万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年1200万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '蒙特雷兹-哈勒尔',
 'player_num': '1',
 'player_position': 'F-C',
 'player_salary': '本年薪金：600万美元',
 'player_team': '洛杉矶快船',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年5000万美元，2016年夏天签，2020年夏天到期，2019-20赛季球员选项，15%的交易保证金',
 'player_height': '1.93米/6尺4',
 'player_name': '泰勒-约翰逊',
 'player_num': '16',
 'player_position': 'G',
 'player_salary': '本年薪金：1963万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '84公斤/186磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年3000万美元，2019年夏天签，2021年夏天到期',
 'player_height': '2.01米/6尺7',
 'player_name': '凯利-乌布雷',
 'player_num': '3',
 'player_position': 'F',
 'player_salary': '本年薪金：1442万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年4037万美元，2018年夏天签，2022年夏天到期，2020-21赛季及2021-22赛季球队选项',
 'player_height': '2.16米/7尺1',
 'player_name': '德安德烈-艾顿',
 'player_num': '22',
 'player_position': 'C',
 'player_salary': '本年薪金：956万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '118公斤/260磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年1000万美元，2019年夏天签，2021年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '弗兰克-卡明斯基',
 'player_num': '8',
 'player_position': 'F-C',
 'player_salary': '本年薪金：476万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年1075万美元，2016年夏天签，2020年夏天到期。',
 'player_height': '2.08米/6尺10',
 'player_name': '达里奥-沙里奇',
 'player_num': '20',
 'player_position': 'F',
 'player_salary': '本年薪金：348万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '101公斤/223磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年226万美元，2018年夏天签，2020年夏天到期',
 'player_height': '1.88米/6尺2',
 'player_name': '杰文-卡特',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：142万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '91公斤/200磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '双向合同',
 'player_height': '1.98米/6尺6',
 'player_name': '乔治-金',
 'player_num': '8',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '菲尼克斯太阳',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2016年夏天在旧合同期内以4年1.18亿美元提前续约至2020年，2017年夏天再次以4年1.7亿美元提前续约至2023年，2022-23赛季球员选项',
 'player_height': '1.96米/6尺5',
 'player_name': '詹姆斯-哈登',
 'player_num': '13',
 'player_position': 'G',
 'player_salary': '本年薪金：3819万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '100公斤/220磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '5年9000万美元，2018年夏天签，2023年夏天到期，其中8000万美元保障，每赛季火箭进入西决奖励100万美元，防守篮板率达到30%、罚球命中率达到65%各奖励50万美元',
 'player_height': '2.11米/6尺11',
 'player_name': '克林特-卡佩拉',
 'player_num': '15',
 'player_position': 'C',
 'player_salary': '本年薪金：1490万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年5289万美元，2016年夏天签，2020年夏天到期。',
 'player_height': '1.93米/6尺4',
 'player_name': '埃里克-戈登',
 'player_num': '10',
 'player_position': 'G',
 'player_salary': '本年薪金：1406万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '4年3188万美元，2017年夏天签，2021年夏天到期，2020-21赛季257万美元保障，2020年7月1日之前不被裁则转为全额保障',
 'player_height': '1.98米/6尺6',
 'player_name': 'PJ-塔克',
 'player_num': '17',
 'player_position': 'F',
 'player_salary': '本年薪金：835万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '111公斤/245磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年1110万美元，2019年夏天签，2022年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '丹纽尔-豪斯',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：349万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.01米/6尺7',
 'player_name': '杰拉德-格林',
 'player_num': '14',
 'player_position': 'G-F',
 'player_salary': '本年薪金：256万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '93公斤/205磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '1年底薪，2019年夏天签，2020年到期',
 'player_height': '2.16米/7尺1',
 'player_name': '泰森-钱德勒',
 'player_num': '19',
 'player_position': 'C',
 'player_salary': '本年薪金：256万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '109公斤/240磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '2年底薪，2019年夏天签',
 'player_height': '1.93米/6尺4',
 'player_name': '奥斯汀-里弗斯',
 'player_num': '25',
 'player_position': 'G',
 'player_salary': '本年薪金：217万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '91公斤/200磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '1年174万美元的非保障合同',
 'player_height': '2.03米/6尺8',
 'player_name': '安东尼-本内特',
 'player_num': '30',
 'player_position': 'F',
 'player_salary': '本年薪金：174万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '107公斤/235磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年392万美元，2018年夏天签，2021年夏天到期',
 'player_height': '2.13米/7尺0',
 'player_name': '以赛亚-哈尔滕施泰因',
 'player_num': '55',
 'player_position': 'F-C',
 'player_salary': '本年薪金：142万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '113公斤/249磅'}
2019-08-20 19:16:23 [root] INFO: {'player_contract': '3年367万美元，2018年12月签，其中19-20赛季50%保障，20-21赛季为无保障合同，2021年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '加里-克拉克',
 'player_num': '6',
 'player_position': 'F',
 'player_salary': '本年薪金：141万美元',
 'player_team': '休斯顿火箭',
 'player_weight': '102公斤/225磅'}
2019-08-20 19:16:24 [root] INFO: {'player_contract': '5年1.13亿美元，2015年夏天签，2020年夏天到期。2018年7月，以4年1.2亿美元提前续约，2019年夏天生效，2023年夏天到期',
 'player_height': '2.08米/6尺10',
 'player_name': '凯文-乐福',
 'player_num': '0',
 'player_position': 'F-C',
 'player_salary': '本年薪金：2894万美元',
 'player_team': '克利夫兰骑士',
 'player_weight': '114公斤/251磅'}
2019-08-20 19:16:25 [root] INFO: {'player_contract': '4年1.06亿美元，2017年夏天签，2021年夏天到期，2020-2021赛季球员选项',
 'player_height': '2.03米/6尺8',
 'player_name': '奥托-波特',
 'player_num': '22',
 'player_position': 'F',
 'player_salary': '本年薪金：2725万美元',
 'player_team': '芝加哥公牛',
 'player_weight': '90公斤/198磅'}
2019-08-20 19:16:26 [root] INFO: {'player_contract': '5年1.78亿美元，2019年夏天签，2024年到期',
 'player_height': '2.03米/6尺8',
 'player_name': '克里斯-米德尔顿',
 'player_num': '22',
 'player_position': 'F-G',
 'player_salary': '本年薪金：3068万美元',
 'player_team': '密尔沃基雄鹿',
 'player_weight': '106公斤/234磅'}
2019-08-20 19:16:27 [root] INFO: {'player_contract': '5年8478万，2013年夏天签，2014年夏天生效，2019年夏天到期；2017年夏天以4年1.7亿美元提前续约至2023年，2022-23赛季球员选项',
 'player_height': '1.93米/6尺4',
 'player_name': '约翰-沃尔',
 'player_num': '2',
 'player_position': 'G',
 'player_salary': '本年薪金：3780万美元',
 'player_team': '华盛顿奇才',
 'player_weight': '95公斤/210磅'}
2019-08-20 19:16:28 [root] INFO: {'player_contract': '2016年10月以4年8400万美元提前续约，2017年夏天生效，2021年夏天到期，每年25万不易达到的激励奖金',
 'player_height': '1.93米/6尺4',
 'player_name': '维克托-奥拉迪波',
 'player_num': '4',
 'player_position': 'G',
 'player_salary': '本年薪金：2100万美元',
 'player_team': '印第安纳步行者',
 'player_weight': '95公斤/210磅'}
2019-08-20 19:16:29 [root] INFO: {'player_contract': '5年1.71亿美元，2017年夏天签，2022年夏天到，2021-22赛季球员选项',
 'player_height': '2.08米/6尺10',
 'player_name': '布雷克-格里芬',
 'player_num': '23',
 'player_position': 'F',
 'player_salary': '本年薪金：3423万美元',
 'player_team': '底特律活塞',
 'player_weight': '114公斤/251磅'}
2019-08-20 19:16:30 [root] INFO: {'player_contract': '4年1亿美元，2019年夏天签，2023年到期',
 'player_height': '2.13米/7尺0',
 'player_name': '尼古拉-武切维奇',
 'player_num': '9',
 'player_position': 'C',
 'player_salary': '本年薪金：2800万美元',
 'player_team': '奥兰多魔术',
 'player_weight': '118公斤/260磅'}
2019-08-20 19:16:30 [root] INFO: {'player_contract': '1年188万美元，2019年夏天签，2020年夏天到期',
 'player_height': '1.98米/6尺6',
 'player_name': '格伦-罗宾逊三世',
 'player_num': '6',
 'player_position': 'G-F',
 'player_salary': '本年薪金：188万美元',
 'player_team': '金州勇士',
 'player_weight': '101公斤/222磅'}
2019-08-20 19:16:30 [root] INFO: {'player_contract': '2年277万美元，2018年夏天签，2020年夏天到期',
 'player_height': '2.03米/6尺8',
 'player_name': '阿方索-麦金尼',
 'player_num': '32',
 'player_position': 'F',
 'player_salary': '本年薪金：142万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/215磅'}
2019-08-20 19:16:30 [root] INFO: {'player_contract': '新秀合同，2019年夏天签，2023年到期，2022年，2023年球队选项',
 'player_height': '2.08米/6尺10',
 'player_name': '阿伦-斯马伊拉吉奇',
 'player_num': '1',
 'player_position': 'F-C',
 'player_salary': '本年薪金：89万美元',
 'player_team': '金州勇士',
 'player_weight': '98公斤/216磅'}
2019-08-20 19:16:30 [root] INFO: {'player_contract': '双向合同',
 'player_height': '2.01米/6尺7',
 'player_name': '马库斯-德里克森',
 'player_num': '7',
 'player_position': 'F',
 'player_salary': '本年薪金：7万美元',
 'player_team': '金州勇士',
 'player_weight': '113公斤/249磅'}
2019-08-20 19:17:14 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:17:14 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:17:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:17:14 [scrapy.extensions.telnet] INFO: Telnet Password: 17735bce0ead75b4
2019-08-20 19:17:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:17:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:17:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:17:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:17:14 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:17:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:17:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:17:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:17:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:17:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 406240,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.679268,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 17, 18, 905428),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 17, 14, 226160)}
2019-08-20 19:17:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:20:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:20:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:20:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:20:34 [scrapy.extensions.telnet] INFO: Telnet Password: a766d4a49e563a1e
2019-08-20 19:20:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:20:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:20:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:20:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:20:34 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:20:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:20:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:20:39 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:20:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405803,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.659267,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 20, 39, 181883),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 20, 34, 522616)}
2019-08-20 19:20:39 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:22:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:22:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:22:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:22:39 [scrapy.extensions.telnet] INFO: Telnet Password: 54178a4fadabfcd1
2019-08-20 19:22:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:22:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:22:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:22:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:22:39 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:22:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:22:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:22:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 61, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:22:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 61, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:22:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 61, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:22:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:22:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33291,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.4822,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 22, 42, 907960),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/ValueError': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 22, 39, 425760)}
2019-08-20 19:22:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:23:02 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:23:02 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:23:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:23:02 [scrapy.extensions.telnet] INFO: Telnet Password: 56d53b03aae0268e
2019-08-20 19:23:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:23:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:23:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:23:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:23:02 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:23:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:23:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:23:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 61, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 61, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 61, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:05 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:23:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33291,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.466198,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 23, 5, 859272),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/ValueError': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 23, 2, 393074)}
2019-08-20 19:23:05 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:23:16 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:23:16 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:23:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:23:16 [scrapy.extensions.telnet] INFO: Telnet Password: 3a1b4e18682e8f43
2019-08-20 19:23:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:23:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:23:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:23:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:23:16 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:23:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:23:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:23:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 62, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 62, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 62, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:23:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33291,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.4892,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 23, 20, 218094),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/ValueError': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 23, 16, 728894)}
2019-08-20 19:23:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:23:38 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:23:38 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:23:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:23:38 [scrapy.extensions.telnet] INFO: Telnet Password: 33860ae7816d8b1e
2019-08-20 19:23:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:23:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:23:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:23:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:23:38 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:23:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:23:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:23:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 62, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 62, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 62, in parse_detail
    player_salary, player_url in zip_player:
ValueError: not enough values to unpack (expected 8, got 7)
2019-08-20 19:23:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:23:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33291,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.443197,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 23, 42, 157349),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/ValueError': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 23, 38, 714152)}
2019-08-20 19:23:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:24:37 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:24:37 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:24:37 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:24:37 [scrapy.extensions.telnet] INFO: Telnet Password: e3630308dbcd4097
2019-08-20 19:24:37 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:24:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:24:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:24:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:24:37 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:24:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:24:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:24:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:24:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405858,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.636265,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 24, 42, 472798),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 24, 37, 836533)}
2019-08-20 19:24:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:26:02 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:26:02 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:26:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:26:02 [scrapy.extensions.telnet] INFO: Telnet Password: 7bb824b7da5a430c
2019-08-20 19:26:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:26:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:26:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:26:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:26:03 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:26:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:26:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:26:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:26:07 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:26:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405470,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.70527,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 26, 7, 865683),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 26, 3, 160413)}
2019-08-20 19:26:07 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:28:10 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:28:10 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:28:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:28:10 [scrapy.extensions.telnet] INFO: Telnet Password: 82d04e7a9935503a
2019-08-20 19:28:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:28:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:28:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:28:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:28:10 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:28:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:28:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:28:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 76, in parse_detail
    print(item_detail.text)
AttributeError: 'Request' object has no attribute 'text'
2019-08-20 19:28:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 76, in parse_detail
    print(item_detail.text)
AttributeError: 'Request' object has no attribute 'text'
2019-08-20 19:28:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 76, in parse_detail
    print(item_detail.text)
AttributeError: 'Request' object has no attribute 'text'
2019-08-20 19:28:14 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:28:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33293,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.472199,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 28, 14, 314915),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/AttributeError': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 28, 10, 842716)}
2019-08-20 19:28:14 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:30:04 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:30:04 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:30:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:30:04 [scrapy.extensions.telnet] INFO: Telnet Password: 1a24f16549ad9169
2019-08-20 19:30:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:30:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:30:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:30:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:30:04 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:30:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:30:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:30:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 76, in parse_detail
    print(item_detail.get_body)
AttributeError: 'Request' object has no attribute 'get_body'
2019-08-20 19:30:07 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 76, in parse_detail
    print(item_detail.get_body)
AttributeError: 'Request' object has no attribute 'get_body'
2019-08-20 19:30:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 76, in parse_detail
    print(item_detail.get_body)
AttributeError: 'Request' object has no attribute 'get_body'
2019-08-20 19:30:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:30:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33293,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.453198,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 30, 8, 176428),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/AttributeError': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 30, 4, 723230)}
2019-08-20 19:30:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:31:28 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:31:28 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:31:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:31:28 [scrapy.extensions.telnet] INFO: Telnet Password: 1dd1575e7dd7eaeb
2019-08-20 19:31:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:31:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:31:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:31:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:31:28 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:31:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:31:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:31:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET https://nba.hupu.com/robots.txt>: DNS lookup failed: no results for hostname lookup: nba.hupu.com.
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: nba.hupu.com.
2019-08-20 19:31:42 [scrapy.core.scraper] ERROR: Error downloading <GET https://nba.hupu.com/players/rockets>
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: nba.hupu.com.
2019-08-20 19:31:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:31:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,
 'downloader/request_bytes': 1341,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'elapsed_time_seconds': 13.823791,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 31, 42, 587828),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 8, 20, 11, 31, 28, 764037)}
2019-08-20 19:31:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:32:13 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:32:13 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:32:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:32:13 [scrapy.extensions.telnet] INFO: Telnet Password: 242630451cbefaa4
2019-08-20 19:32:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:32:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:32:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:32:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:32:14 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:32:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:32:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:32:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:32:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:32:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 404922,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.768272,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 32, 18, 896904),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 32, 14, 128632)}
2019-08-20 19:32:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:57:46 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:57:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:57:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:57:46 [scrapy.extensions.telnet] INFO: Telnet Password: 8655a4bda76dada3
2019-08-20 19:57:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:57:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:57:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:57:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:57:46 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:57:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:57:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:57:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:57:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 404997,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.763272,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 57, 51, 28537),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 57, 46, 265265)}
2019-08-20 19:57:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:58:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:58:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:58:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:58:23 [scrapy.extensions.telnet] INFO: Telnet Password: 8c91ce3bf09e05fe
2019-08-20 19:58:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:58:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:58:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:58:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:58:23 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:58:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:58:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:58:28 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:58:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 404942,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.648266,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 58, 28, 467679),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 58, 23, 819413)}
2019-08-20 19:58:28 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 19:59:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 19:59:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 19:59:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 19:59:22 [scrapy.extensions.telnet] INFO: Telnet Password: 18dc5cdcdca81d8f
2019-08-20 19:59:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 19:59:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 19:59:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 19:59:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 19:59:23 [scrapy.core.engine] INFO: Spider opened
2019-08-20 19:59:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 19:59:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 19:59:27 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 19:59:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 404997,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.731271,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 11, 59, 27, 741069),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 11, 59, 23, 9798)}
2019-08-20 19:59:27 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 20:01:55 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 20:01:55 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 20:01:55 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 20:01:55 [scrapy.extensions.telnet] INFO: Telnet Password: dc27c740ea498972
2019-08-20 20:01:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 20:01:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 20:01:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 20:01:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 20:01:55 [scrapy.core.engine] INFO: Spider opened
2019-08-20 20:01:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 20:01:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:01:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:02:00 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 20:02:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405899,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.680268,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 12, 2, 0, 428802),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 12, 1, 55, 748534)}
2019-08-20 20:02:00 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 20:05:50 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 20:05:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 20:05:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 20:05:50 [scrapy.extensions.telnet] INFO: Telnet Password: 7798fd56d6e9f103
2019-08-20 20:05:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 20:05:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 20:05:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 20:05:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 20:05:51 [scrapy.core.engine] INFO: Spider opened
2019-08-20 20:05:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 20:05:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:05:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 20:05:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405890,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.918281,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 12, 5, 55, 935272),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 12, 5, 51, 16991)}
2019-08-20 20:05:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 20:09:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 20:09:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 20:09:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 20:09:54 [scrapy.extensions.telnet] INFO: Telnet Password: 3b05cceae16a5f6a
2019-08-20 20:09:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 20:09:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 20:09:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 20:09:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 20:09:54 [scrapy.core.engine] INFO: Spider opened
2019-08-20 20:09:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 20:09:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:09:59 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 20:09:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405874,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.760273,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 12, 9, 59, 103181),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 12, 9, 54, 342908)}
2019-08-20 20:09:59 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-20 20:28:41 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-20 20:28:41 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-20 20:28:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-20 20:28:41 [scrapy.extensions.telnet] INFO: Telnet Password: cd95d700f4c0a7ed
2019-08-20 20:28:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-20 20:28:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-20 20:28:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-20 20:28:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-20 20:28:41 [scrapy.core.engine] INFO: Spider opened
2019-08-20 20:28:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-20 20:28:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in <lambda>
    item_detail = scrapy.Request(url=player_url, meta={'item': item}, callback=lambda response, item_a=item: self.parse_stat(response, item_a))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-20 20:28:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-20 20:28:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 405616,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.798274,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 20, 12, 28, 46, 188646),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 20, 12, 28, 41, 390372)}
2019-08-20 20:28:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:22:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:22:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:23:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:23:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:23:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:23:22 [scrapy.extensions.telnet] INFO: Telnet Password: f916ec523f2658fb
2019-08-21 09:23:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:23:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:23:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:23:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:23:22 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:23:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:23:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:23:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 29, in parse
    yield scrapy.Request(url=url, meta={'item': item}, callback=self.parse_content, dont_filter=False)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 63, in _set_url
    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
TypeError: Request url must be str or unicode, got Selector:
2019-08-21 09:23:23 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:23:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.271016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 23, 23, 205991),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 1, 23, 22, 934975)}
2019-08-21 09:23:23 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:24:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:24:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:24:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:24:52 [scrapy.extensions.telnet] INFO: Telnet Password: 613956aed07d7aae
2019-08-21 09:24:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:24:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:24:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:24:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:24:52 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:24:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:24:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:24:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 30, in parse
    yield scrapy.Request(url=url, meta={'item': item}, callback=self.parse_content, dont_filter=False)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 63, in _set_url
    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
TypeError: Request url must be str or unicode, got Selector:
2019-08-21 09:24:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:24:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.270016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 24, 53, 241141),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 1, 24, 52, 971125)}
2019-08-21 09:24:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:25:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:25:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:25:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:25:56 [scrapy.extensions.telnet] INFO: Telnet Password: af0689d66dd42fe5
2019-08-21 09:25:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:25:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:25:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:25:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:25:56 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:25:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:25:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:25:56 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 30, in parse
    yield scrapy.Request(url=url, meta={'item': item}, callback=self.parse_content, dont_filter=False)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 63, in _set_url
    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
TypeError: Request url must be str or unicode, got Selector:
2019-08-21 09:25:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:25:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.279016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 25, 56, 888781),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 1, 25, 56, 609765)}
2019-08-21 09:25:56 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:26:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:26:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:26:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:26:32 [scrapy.extensions.telnet] INFO: Telnet Password: b177e11dce93e773
2019-08-21 09:26:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:26:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:26:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:26:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:26:32 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:26:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:26:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:26:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 31, in parse
    yield scrapy.Request(url=url, meta={'item': item}, callback=self.parse_content, dont_filter=False)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 63, in _set_url
    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
TypeError: Request url must be str or unicode, got Selector:
2019-08-21 09:26:33 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:26:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.276015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 26, 33, 69850),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 1, 26, 32, 793835)}
2019-08-21 09:26:33 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:26:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:26:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:26:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:26:59 [scrapy.extensions.telnet] INFO: Telnet Password: 749368176dde6c56
2019-08-21 09:26:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:26:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:26:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:26:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:26:59 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:26:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:26:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:26:59 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    print(url.data)
AttributeError: 'Selector' object has no attribute 'data'
2019-08-21 09:26:59 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:26:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.266015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 26, 59, 867383),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 1, 26, 59, 601368)}
2019-08-21 09:26:59 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:27:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:27:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:27:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:27:39 [scrapy.extensions.telnet] INFO: Telnet Password: b807f7f081d032a0
2019-08-21 09:27:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:27:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:27:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:27:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:27:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:27:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:27:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:27:39 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 31, in parse
    yield scrapy.Request(url=url, meta={'item': item}, callback=self.parse_content, dont_filter=False)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 63, in _set_url
    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
TypeError: Request url must be str or unicode, got Selector:
2019-08-21 09:27:39 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:27:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.269015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 27, 39, 733663),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 1, 27, 39, 464648)}
2019-08-21 09:27:39 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:29:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:29:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:29:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:29:12 [scrapy.extensions.telnet] INFO: Telnet Password: ded417deb6f91f28
2019-08-21 09:29:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:29:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:29:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:29:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:29:12 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:29:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:29:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:13 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:29:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.891051,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 29, 13, 884048),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 29, 12, 992997)}
2019-08-21 09:29:13 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:29:49 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:29:49 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:29:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:29:49 [scrapy.extensions.telnet] INFO: Telnet Password: a842be22513c9bbd
2019-08-21 09:29:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:29:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:29:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:29:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:29:49 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:29:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:29:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:29:50 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:29:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.87305,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 29, 50, 694154),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 29, 49, 821104)}
2019-08-21 09:29:50 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:30:43 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:30:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:30:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:30:43 [scrapy.extensions.telnet] INFO: Telnet Password: c35e44c90aeb87c3
2019-08-21 09:30:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:30:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:30:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:30:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:30:43 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:30:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:30:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:30:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:30:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:30:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.44514,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 30, 45, 734302),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 30, 43, 289162)}
2019-08-21 09:30:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:31:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:31:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:31:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:31:30 [scrapy.extensions.telnet] INFO: Telnet Password: bd4ad117023a9849
2019-08-21 09:31:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:31:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:31:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:31:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:31:30 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:31:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:31:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:31:30 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:30 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:30 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:30 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:31:33 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:31:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411008,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 3.30719,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 31, 33, 580039),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 31, 30, 272849)}
2019-08-21 09:31:33 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:34:00 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:34:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:34:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:34:00 [scrapy.extensions.telnet] INFO: Telnet Password: 30d47d08be44d05e
2019-08-21 09:34:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:34:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:34:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:34:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:34:00 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:34:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:34:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:34:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:01 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:03 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:07 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:07 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:34:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 6.783388,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 34, 7, 199560),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 34, 0, 416172)}
2019-08-21 09:34:07 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:34:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:34:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:34:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:34:23 [scrapy.extensions.telnet] INFO: Telnet Password: 54aa6d16fbddf8d6
2019-08-21 09:34:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:34:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:34:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:34:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:34:23 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:34:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:34:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:26 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:34:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.62315,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 34, 26, 561667),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 34, 23, 938517)}
2019-08-21 09:34:26 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:34:42 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:34:42 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:34:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:34:42 [scrapy.extensions.telnet] INFO: Telnet Password: 65802ddfe9eef5d2
2019-08-21 09:34:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:34:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:34:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:34:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:34:42 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:34:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:34:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:43 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:34:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:34:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 4.602263,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 34, 47, 327855),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 34, 42, 725592)}
2019-08-21 09:34:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:35:48 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:35:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:35:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:35:48 [scrapy.extensions.telnet] INFO: Telnet Password: 6cdaf4c6ea6e6a57
2019-08-21 09:35:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:35:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:35:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:35:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:35:48 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:35:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:35:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:49 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:50 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:35:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:35:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 3.392194,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 35, 52, 298571),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 35, 48, 906377)}
2019-08-21 09:35:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 09:37:43 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 09:37:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 09:37:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 09:37:43 [scrapy.extensions.telnet] INFO: Telnet Password: e2e3a2a877f9ab77
2019-08-21 09:37:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 09:37:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 09:37:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 09:37:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 09:37:43 [scrapy.core.engine] INFO: Spider opened
2019-08-21 09:37:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 09:37:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:46 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 09:37:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 09:37:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 3.791217,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 1, 37, 47, 235145),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 1, 37, 43, 443928)}
2019-08-21 09:37:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:55:34 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:55:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:55:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:55:34 [scrapy.extensions.telnet] INFO: Telnet Password: d5b60d8b1f76c502
2019-08-21 10:55:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:55:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:55:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:55:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:55:35 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:55:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:55:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:55:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://product.china-pub.com/robots.txt>: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:55:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html>
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:55:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:55:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,
 'downloader/request_bytes': 1440,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'elapsed_time_seconds': 0.233013,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 55, 35, 391148),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 8, 21, 2, 55, 35, 158135)}
2019-08-21 10:55:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:55:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:55:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:55:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:55:52 [scrapy.extensions.telnet] INFO: Telnet Password: b7c5e4921320a44a
2019-08-21 10:55:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:55:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:55:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:55:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:55:52 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:55:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:55:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:55:52 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://product.china-pub.com/robots.txt>: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:55:52 [scrapy.core.scraper] ERROR: Error downloading <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html>
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:55:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:55:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,
 'downloader/request_bytes': 1440,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'elapsed_time_seconds': 0.226012,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 55, 52, 911150),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 8, 21, 2, 55, 52, 685138)}
2019-08-21 10:55:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:56:15 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:56:15 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:56:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:56:15 [scrapy.extensions.telnet] INFO: Telnet Password: a2c7a5177834d3c4
2019-08-21 10:56:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:56:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:56:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:56:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:56:15 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:56:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:56:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:56:15 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://product.china-pub.com/robots.txt>: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:56:15 [scrapy.core.scraper] ERROR: Error downloading <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html>
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:56:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:56:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,
 'downloader/request_bytes': 1440,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'elapsed_time_seconds': 0.226013,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 56, 15, 788459),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 8, 21, 2, 56, 15, 562446)}
2019-08-21 10:56:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:56:35 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:56:35 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:56:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:56:35 [scrapy.extensions.telnet] INFO: Telnet Password: 4f4c2ee0c027a4e6
2019-08-21 10:56:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:56:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:56:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:56:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:56:35 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:56:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:56:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:56:35 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://product.china-pub.com/robots.txt>: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:56:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html>
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 1416, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "d:\spider_test\venv\lib\site-packages\twisted\python\failure.py", line 512, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\downloader\middleware.py", line 44, in process_request
    defer.returnValue((yield download_func(request=request, spider=spider)))
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\endpoints.py", line 982, in startConnectionAttempts
    "no results for hostname lookup: {}".format(self._hostStr)
twisted.internet.error.DNSLookupError: DNS lookup failed: no results for hostname lookup: product.china-pub.com.
2019-08-21 10:56:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:56:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 6,
 'downloader/request_bytes': 1440,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'elapsed_time_seconds': 0.225012,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 56, 35, 570590),
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'retry/count': 4,
 'retry/max_reached': 2,
 'retry/reason_count/twisted.internet.error.DNSLookupError': 4,
 "robotstxt/exception_count/<class 'twisted.internet.error.DNSLookupError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 8, 21, 2, 56, 35, 345578)}
2019-08-21 10:56:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:57:36 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:57:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:57:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:57:36 [scrapy.extensions.telnet] INFO: Telnet Password: f1ce0b4d1558436d
2019-08-21 10:57:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:57:37 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:57:37 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:57:37 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:57:37 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:57:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:57:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:57:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    item['game_play'] = game_play
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: game_play'
2019-08-21 10:57:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:57:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 404546,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.641266,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 57, 41, 699373),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/KeyError': 40,
 'start_time': datetime.datetime(2019, 8, 21, 2, 57, 37, 58107)}
2019-08-21 10:57:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:57:44 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:57:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:57:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:57:44 [scrapy.extensions.telnet] INFO: Telnet Password: 463dafccab7a637e
2019-08-21 10:57:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:57:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:57:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:57:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:57:44 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:57:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:57:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:57:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:57:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.897051,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 57, 45, 574594),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 2, 57, 44, 677543)}
2019-08-21 10:57:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:58:03 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:58:03 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:58:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:58:03 [scrapy.extensions.telnet] INFO: Telnet Password: 60b48f5bfe04264f
2019-08-21 10:58:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:58:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:58:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:58:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:58:04 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:58:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:58:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:04 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:05 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:06 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:06 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:06 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:06 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 10:58:06 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:58:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.226127,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 58, 6, 307780),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 2, 58, 4, 81653)}
2019-08-21 10:58:06 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:58:29 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:58:29 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:58:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:58:29 [scrapy.extensions.telnet] INFO: Telnet Password: 2d46d96bacc432a8
2019-08-21 10:58:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:58:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:58:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:58:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:58:30 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:58:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:58:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:58:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:58:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.283016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 58, 30, 302153),
 'item_scraped_count': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 2, 58, 30, 19137)}
2019-08-21 10:58:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 10:58:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 10:58:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 10:58:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 10:58:58 [scrapy.extensions.telnet] INFO: Telnet Password: 8de09a643b87af55
2019-08-21 10:58:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 10:58:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 10:58:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 10:58:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 10:58:58 [scrapy.core.engine] INFO: Spider opened
2019-08-21 10:58:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 10:58:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 10:58:59 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 10:58:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.286017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 2, 58, 59, 77799),
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 2, 58, 58, 791782)}
2019-08-21 10:58:59 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:01:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:01:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:01:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:01:22 [scrapy.extensions.telnet] INFO: Telnet Password: 0b87139060bcfa4a
2019-08-21 11:01:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:01:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:01:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:01:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:01:22 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:01:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:01:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:01:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:23 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:24 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:25 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:01:33 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-08-21 11:01:33 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-08-21 11:01:33 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-08-21 11:01:33 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in http://product.china-pub.com/8057288. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2019-08-21 11:01:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 387883,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 11.502658,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 1, 33, 709643),
 'item_scraped_count': 20,
 'log_count/ERROR': 15,
 'log_count/INFO': 12,
 'log_count/WARNING': 1,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 17,
 'retry/count': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 1,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 18,
 'scheduler/enqueued/memory': 18,
 'spider_exceptions/KeyError': 15,
 'start_time': datetime.datetime(2019, 8, 21, 3, 1, 22, 206985)}
2019-08-21 11:01:33 [scrapy.core.engine] INFO: Spider closed (shutdown)
2019-08-21 11:04:47 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:04:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:04:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:04:47 [scrapy.extensions.telnet] INFO: Telnet Password: ec77cba39b276766
2019-08-21 11:04:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:04:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:04:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:04:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:04:47 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:04:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:04:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:04:47 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 31, in parse
    yield scrapy.Request(url=url, meta={'item': json.dumps(item)}, callback=self.parse_content, dont_filter=False)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:04:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:04:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.308018,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 4, 47, 913751),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 4, 47, 605733)}
2019-08-21 11:04:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:26:41 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:26:41 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:26:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:26:41 [scrapy.extensions.telnet] INFO: Telnet Password: 1364e69a97b5fe06
2019-08-21 11:26:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:26:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:26:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:26:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:26:42 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:26:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:26:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:26:42 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 31, in parse
    yield scrapy.Request(url=url, meta={'item': json.dumps(item)}, callback=self.parse_content, dont_filter=False)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:26:42 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:26:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.326018,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 26, 42, 491940),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 26, 42, 165922)}
2019-08-21 11:26:42 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:28:51 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:28:51 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:28:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:28:51 [scrapy.extensions.telnet] INFO: Telnet Password: 4c05c63d336cd282
2019-08-21 11:28:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:28:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:28:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:28:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:28:51 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:28:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:28:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:28:51 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 38, in parse_content
    item['content'] = content
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'MyspiderItem does not support field: content'
2019-08-21 11:28:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:28:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.117121,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 28, 53, 487433),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/KeyError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 28, 51, 370312)}
2019-08-21 11:28:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:32:36 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:32:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:32:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:32:36 [scrapy.extensions.telnet] INFO: Telnet Password: b6a94e91a5de2cb0
2019-08-21 11:32:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:32:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:32:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:32:36 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:32:36 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:32:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:32:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:32:36 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 31, in parse
    yield scrapy.Request(url=url, meta={'item': json.dumps(item)}, callback=self.parse_content, dont_filter=False)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:32:36 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:32:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.303017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 32, 36, 918212),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 32, 36, 615195)}
2019-08-21 11:32:36 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:33:42 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:33:42 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:33:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:33:42 [scrapy.extensions.telnet] INFO: Telnet Password: ce2ab3df713d447a
2019-08-21 11:33:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:33:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:33:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:33:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:33:42 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:33:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:33:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:33:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pelicans> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in parse_detail
    item_detail = scrapy.Request(url=player_url, meta={'item': json.dumps(item)}, callback=self.parse_stat)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type NBA_Item is not JSON serializable
2019-08-21 11:33:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in parse_detail
    item_detail = scrapy.Request(url=player_url, meta={'item': json.dumps(item)}, callback=self.parse_stat)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type NBA_Item is not JSON serializable
2019-08-21 11:33:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/grizzlies> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 78, in parse_detail
    item_detail = scrapy.Request(url=player_url, meta={'item': json.dumps(item)}, callback=self.parse_stat)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type NBA_Item is not JSON serializable
2019-08-21 11:33:46 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:33:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1269,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 33291,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'elapsed_time_seconds': 3.509201,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 33, 46, 43166),
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 5,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/TypeError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 3, 33, 42, 533965)}
2019-08-21 11:33:46 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:35:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:35:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:35:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:35:12 [scrapy.extensions.telnet] INFO: Telnet Password: 771537e043884131
2019-08-21 11:35:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:35:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:35:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:35:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:35:13 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:35:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:35:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:35:13 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 31, in parse
    yield scrapy.Request(url=url, meta={'item': json.dumps(item)}, callback=self.parse_content, dont_filter=False)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:35:13 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:35:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.269016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 35, 13, 329159),
 'item_scraped_count': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 35, 13, 60143)}
2019-08-21 11:35:13 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:35:31 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:35:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:35:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:35:31 [scrapy.extensions.telnet] INFO: Telnet Password: b0c3851df8126933
2019-08-21 11:35:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:35:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:35:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:35:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:35:31 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:35:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:35:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:35:31 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:35:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.278016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 35, 31, 860219),
 'item_scraped_count': 20,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 35, 31, 582203)}
2019-08-21 11:35:31 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:36:27 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:36:27 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:36:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:36:27 [scrapy.extensions.telnet] INFO: Telnet Password: 435117cea232cea2
2019-08-21 11:36:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:36:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:36:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:36:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:36:27 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:36:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:36:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:36:27 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    item = json.dumps(item)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:36:27 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:36:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.269015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 36, 27, 992429),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 36, 27, 723414)}
2019-08-21 11:36:27 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:36:45 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:36:45 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:36:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:36:45 [scrapy.extensions.telnet] INFO: Telnet Password: 8044b01708cda650
2019-08-21 11:36:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:36:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:36:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:36:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:36:45 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:36:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:36:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:36:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    item = json.dumps(item)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:36:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:36:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.276016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 36, 45, 751445),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 36, 45, 475429)}
2019-08-21 11:36:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:37:02 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:37:02 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:37:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:37:02 [scrapy.extensions.telnet] INFO: Telnet Password: 9bf5b9d318f55caf
2019-08-21 11:37:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:37:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:37:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:37:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:37:02 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:37:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:37:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:37:02 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    item_1 = json.dumps(item)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:37:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:37:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.265015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 37, 2, 685413),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 37, 2, 420398)}
2019-08-21 11:37:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:37:20 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:37:20 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:37:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:37:20 [scrapy.extensions.telnet] INFO: Telnet Password: a1df43298c993838
2019-08-21 11:37:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:37:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:37:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:37:20 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:37:20 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:37:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:37:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:37:20 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    item_1 = json.dumps(item)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:37:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:37:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.256015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 37, 20, 493432),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 37, 20, 237417)}
2019-08-21 11:37:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:37:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:37:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:37:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:37:39 [scrapy.extensions.telnet] INFO: Telnet Password: 7b4825be863aeb8d
2019-08-21 11:37:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:37:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:37:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:37:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:37:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:37:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:37:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:37:39 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    item_1 = json.dumps(item)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:37:39 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:37:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.259015,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 37, 39, 898542),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 37, 39, 639527)}
2019-08-21 11:37:39 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:37:53 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:37:53 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:37:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:37:53 [scrapy.extensions.telnet] INFO: Telnet Password: 83abb264b2d103ae
2019-08-21 11:37:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:37:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:37:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:37:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:37:53 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:37:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:37:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:37:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:37:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.288017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 37, 53, 866341),
 'item_scraped_count': 20,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 37, 53, 578324)}
2019-08-21 11:37:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:38:14 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:38:14 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:38:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:38:14 [scrapy.extensions.telnet] INFO: Telnet Password: 26fce1a81b6edcd8
2019-08-21 11:38:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:38:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:38:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:38:14 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:38:14 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:38:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:38:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:38:14 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:38:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.296017,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 38, 14, 966548),
 'item_scraped_count': 20,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 38, 14, 670531)}
2019-08-21 11:38:14 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:38:44 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:38:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:38:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:38:44 [scrapy.extensions.telnet] INFO: Telnet Password: fb11aeeabfa3efac
2019-08-21 11:38:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:38:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:38:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:38:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:38:44 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:38:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:38:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:38:45 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/cache/browse2/59/1_2_59_0.html> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 25, in parse
    print(json.dumps(item))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type MyspiderItem is not JSON serializable
2019-08-21 11:38:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:38:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.275016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 38, 45, 263281),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 38, 44, 988265)}
2019-08-21 11:38:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:41:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:41:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:41:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:41:12 [scrapy.extensions.telnet] INFO: Telnet Password: 0417032d31cdc701
2019-08-21 11:41:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:41:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:41:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:41:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:41:13 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:41:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:41:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:41:13 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:41:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.33602,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 41, 13, 385753),
 'item_scraped_count': 20,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 41, 13, 49733)}
2019-08-21 11:41:13 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:41:47 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:41:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:41:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:41:47 [scrapy.extensions.telnet] INFO: Telnet Password: 9617fc2a39ad6498
2019-08-21 11:41:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:41:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:41:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:41:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:41:47 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:41:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:41:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:41:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:41:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 480,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 5555,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.287016,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 41, 47, 590709),
 'item_scraped_count': 20,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 8, 21, 3, 41, 47, 303693)}
2019-08-21 11:41:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:42:21 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:42:21 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:42:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:42:21 [scrapy.extensions.telnet] INFO: Telnet Password: aaf16cd80475dfd6
2019-08-21 11:42:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:42:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:42:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:42:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:42:21 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:42:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:42:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 37, in parse_content
    item = json.loads(response.meta['item'])
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:42:22 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:42:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.768044,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 42, 22, 613712),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/JSONDecodeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 42, 21, 845668)}
2019-08-21 11:42:22 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:44:25 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:44:25 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:44:25 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:44:25 [scrapy.extensions.telnet] INFO: Telnet Password: 1a83e43a19fdb29a
2019-08-21 11:44:25 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:44:25 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:44:25 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:44:25 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:44:25 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:44:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:44:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:26 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:44:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.788045,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 44, 26, 501798),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 44, 25, 713753)}
2019-08-21 11:44:26 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:44:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:44:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:44:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:44:52 [scrapy.extensions.telnet] INFO: Telnet Password: 08b0c5dac77a9a45
2019-08-21 11:44:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:44:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:44:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:44:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:44:52 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:44:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:44:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + eval(response.meta['item']) + "的信息。")
TypeError: can only concatenate str (not "dict") to str
2019-08-21 11:44:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:44:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.759043,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 44, 53, 682353),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 44, 52, 923310)}
2019-08-21 11:44:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:45:17 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:45:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:45:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:45:17 [scrapy.extensions.telnet] INFO: Telnet Password: de49f9f9b594015c
2019-08-21 11:45:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:45:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:45:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:45:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:45:18 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:45:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:45:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + json.loads(response.meta['item']) + "的信息。")
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\__init__.py", line 348, in loads
    return _default_decoder.decode(s)
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\json\decoder.py", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
2019-08-21 11:45:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:45:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.771044,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 45, 18, 839792),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/JSONDecodeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 45, 18, 68748)}
2019-08-21 11:45:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:45:33 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:45:33 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:45:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:45:33 [scrapy.extensions.telnet] INFO: Telnet Password: 9c0c8c1029b397f8
2019-08-21 11:45:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:45:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:45:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:45:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:45:33 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:45:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:45:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:45:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:33 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:45:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:45:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.829047,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 45, 34, 179669),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 45, 33, 350622)}
2019-08-21 11:45:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:45:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:45:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:45:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:45:56 [scrapy.extensions.telnet] INFO: Telnet Password: 87cc4423ec571f55
2019-08-21 11:45:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:45:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:45:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:45:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:45:56 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:45:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:45:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 36, in parse_content
    print("正在爬取书:" + response.meta['item']['name'] + "的信息。")
TypeError: string indices must be integers
2019-08-21 11:45:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:45:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.819047,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 45, 57, 504003),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 45, 56, 684956)}
2019-08-21 11:45:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 11:46:08 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 11:46:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 11:46:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 11:46:08 [scrapy.extensions.telnet] INFO: Telnet Password: ecbe3d31a632d542
2019-08-21 11:46:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 11:46:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 11:46:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 11:46:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 11:46:08 [scrapy.core.engine] INFO: Spider opened
2019-08-21 11:46:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 11:46:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 11:46:09 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 11:46:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.938054,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 3, 46, 9, 713702),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 3, 46, 8, 775648)}
2019-08-21 11:46:09 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:37:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:37:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:37:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:37:30 [scrapy.extensions.telnet] INFO: Telnet Password: 74fdf7e1b548b46c
2019-08-21 13:37:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:37:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:37:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:37:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:37:31 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:37:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:37:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = content
TypeError: 'str' object does not support item assignment
2019-08-21 13:37:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:37:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411008,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.919053,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 37, 32, 20908),
 'item_scraped_count': 20,
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 5, 37, 31, 101855)}
2019-08-21 13:37:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:38:04 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:38:04 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:38:04 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:38:04 [scrapy.extensions.telnet] INFO: Telnet Password: 4c39776f5749ca7f
2019-08-21 13:38:04 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:38:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:38:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:38:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:38:05 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:38:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:38:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:38:06 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:38:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411008,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.948054,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 38, 6, 122858),
 'item_scraped_count': 36,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 38, 5, 174804)}
2019-08-21 13:38:06 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:39:36 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:39:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:39:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:39:36 [scrapy.extensions.telnet] INFO: Telnet Password: c0bbce0bb9c25f77
2019-08-21 13:39:36 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:39:36 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:39:36 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:39:36 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:39:36 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:39:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:39:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:39:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:39:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.861049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 39, 37, 240070),
 'item_scraped_count': 36,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 39, 36, 379021)}
2019-08-21 13:39:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:39:50 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:39:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:39:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:39:50 [scrapy.extensions.telnet] INFO: Telnet Password: 9168dc14c6ca0b48
2019-08-21 13:39:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:39:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:39:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:39:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:39:51 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:39:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:39:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:39:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:39:51 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: book.json
2019-08-21 13:39:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.852049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 39, 51, 923910),
 'item_scraped_count': 36,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 39, 51, 71861)}
2019-08-21 13:39:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:42:07 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:42:07 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:42:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:42:07 [scrapy.extensions.telnet] INFO: Telnet Password: bee76325b8665e7e
2019-08-21 13:42:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:42:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:42:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:42:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:42:08 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:42:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:42:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:42:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:42:08 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:42:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.848049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 42, 8, 961748),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 42, 8, 113699)}
2019-08-21 13:42:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:42:33 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:42:33 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:42:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:42:33 [scrapy.extensions.telnet] INFO: Telnet Password: bbfff0d94d03b1b1
2019-08-21 13:42:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:42:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:42:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:42:34 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:42:34 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:42:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:42:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:42:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:42:35 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:42:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.885051,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 42, 35, 113244),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 42, 34, 228193)}
2019-08-21 13:42:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:44:16 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:44:16 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:44:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:44:16 [scrapy.extensions.telnet] INFO: Telnet Password: 1edaf132df18966f
2019-08-21 13:44:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:44:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:44:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:44:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:44:16 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:44:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:44:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:44:16 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:16 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:16 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:16 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:17 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:18 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r|\\n|\\u3000", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:44:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:44:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.499143,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 44, 18, 820175),
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 5, 44, 16, 321032)}
2019-08-21 13:44:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:45:08 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:45:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:45:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:45:08 [scrapy.extensions.telnet] INFO: Telnet Password: 142792434bcc3174
2019-08-21 13:45:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:45:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:45:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:45:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:45:08 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:45:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:45:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:45:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:08 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 40, in parse_content
    item['content'] = ''.join(re.split(r"\\r", content))
  File "C:\Users\user\AppData\Local\Programs\Python\Python37-32\lib\re.py", line 213, in split
    return _compile(pattern, flags).split(string, maxsplit)
TypeError: expected string or bytes-like object
2019-08-21 13:45:09 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:45:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.86705,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 45, 9, 178056),
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/TypeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 5, 45, 8, 311006)}
2019-08-21 13:45:09 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:45:29 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:45:29 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:45:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:45:29 [scrapy.extensions.telnet] INFO: Telnet Password: 938a6bf1b0425980
2019-08-21 13:45:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:45:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:45:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:45:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:45:29 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:45:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:45:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:45:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:45:30 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:45:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.834047,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 45, 30, 800292),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 45, 29, 966245)}
2019-08-21 13:45:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:45:53 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:45:53 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:45:53 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:45:53 [scrapy.extensions.telnet] INFO: Telnet Password: bc27b1e773e8f805
2019-08-21 13:45:53 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:45:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:45:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:45:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:45:53 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:45:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:45:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:45:54 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:45:54 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:45:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.990057,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 45, 54, 726661),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 45, 53, 736604)}
2019-08-21 13:45:54 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:48:43 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:48:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:48:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:48:43 [scrapy.extensions.telnet] INFO: Telnet Password: 98dd5ae8c3a673ea
2019-08-21 13:48:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:48:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:48:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:48:44 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:48:44 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:48:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:48:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:48:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:48:45 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:48:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.572089,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 48, 45, 689439),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 48, 44, 117350)}
2019-08-21 13:48:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:50:31 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:50:31 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:50:31 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:50:31 [scrapy.extensions.telnet] INFO: Telnet Password: 50bb5f0f1364b3ff
2019-08-21 13:50:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:50:31 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:50:31 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:50:31 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:50:31 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:50:31 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:50:31 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:31 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.strip()
AttributeError: 'list' object has no attribute 'strip'
2019-08-21 13:50:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:50:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.860049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 50, 32, 136528),
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/AttributeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 5, 50, 31, 276479)}
2019-08-21 13:50:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:51:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:51:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:51:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:51:54 [scrapy.extensions.telnet] INFO: Telnet Password: b40ea801f0d2fff1
2019-08-21 13:51:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:51:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:51:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:51:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:51:55 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:51:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:51:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057274> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057284> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057273> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057290> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057288> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057285> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057250> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057272> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057287> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057286> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057243> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057249> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057241> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057280> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057239> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.scraper] ERROR: Spider error processing <GET http://product.china-pub.com/8057277> (referer: http://product.china-pub.com/cache/browse2/59/1_2_59_0.html)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\book_spider_test.py", line 39, in parse_content
    content = content.replace('\r', '').replace('\n', '').replace('\t', '')
AttributeError: 'list' object has no attribute 'replace'
2019-08-21 13:51:55 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:51:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.860049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 51, 55, 940321),
 'log_count/ERROR': 16,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/AttributeError': 16,
 'start_time': datetime.datetime(2019, 8, 21, 5, 51, 55, 80272)}
2019-08-21 13:51:55 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:53:26 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:53:26 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:53:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:53:26 [scrapy.extensions.telnet] INFO: Telnet Password: d75804a6ff3f2036
2019-08-21 13:53:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:53:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:53:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:53:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:53:26 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:53:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:53:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:53:27 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:53:27 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:53:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.859049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 53, 27, 500558),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 53, 26, 641509)}
2019-08-21 13:53:27 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:55:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:55:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:55:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:55:32 [scrapy.extensions.telnet] INFO: Telnet Password: 746b265a78a18bae
2019-08-21 13:55:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:55:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:55:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:55:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:55:32 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:55:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:55:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:55:33 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:55:33 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:55:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.87705,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 55, 33, 383758),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 55, 32, 506708)}
2019-08-21 13:55:33 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:57:03 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:57:03 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:57:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:57:03 [scrapy.extensions.telnet] INFO: Telnet Password: 8d3568108f7538f4
2019-08-21 13:57:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:57:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:57:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:57:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:57:03 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:57:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:57:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:57:04 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:57:04 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:57:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.865049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 57, 4, 103947),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 57, 3, 238898)}
2019-08-21 13:57:04 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:58:19 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:58:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:58:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:58:19 [scrapy.extensions.telnet] INFO: Telnet Password: 64d2c10796bdc656
2019-08-21 13:58:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:58:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:58:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:58:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:58:19 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:58:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:58:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:58:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:58:20 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:58:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.850048,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 58, 20, 442313),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 58, 19, 592265)}
2019-08-21 13:58:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 13:59:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 13:59:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 13:59:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 13:59:32 [scrapy.extensions.telnet] INFO: Telnet Password: d9635876851f8cd7
2019-08-21 13:59:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 13:59:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 13:59:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 13:59:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 13:59:32 [scrapy.core.engine] INFO: Spider opened
2019-08-21 13:59:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 13:59:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 13:59:33 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 13:59:33 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 13:59:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.857049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 5, 59, 33, 386486),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 5, 59, 32, 529437)}
2019-08-21 13:59:33 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:00:15 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:00:15 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:00:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:00:15 [scrapy.extensions.telnet] INFO: Telnet Password: 69901c039eeacef5
2019-08-21 14:00:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:00:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:00:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:00:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:00:16 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:00:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:00:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:00:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:00:17 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 14:00:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411007,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.850049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 0, 17, 27982),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 6, 0, 16, 177933)}
2019-08-21 14:00:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:00:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:00:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:00:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:00:55 [scrapy.extensions.telnet] INFO: Telnet Password: 5bc7384a353d1087
2019-08-21 14:00:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:00:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:00:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:00:55 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:00:55 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:00:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:00:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:00:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:00:57 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 14:00:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411010,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.407137,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 0, 57, 650305),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 6, 0, 55, 243168)}
2019-08-21 14:00:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:01:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:01:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:01:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:01:30 [scrapy.extensions.telnet] INFO: Telnet Password: e082fe980baa312d
2019-08-21 14:01:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:01:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:01:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:01:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:01:30 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:01:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:01:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:01:31 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:01:31 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 14:01:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411008,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.917052,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 1, 31, 588246),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 6, 1, 30, 671194)}
2019-08-21 14:01:31 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:02:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:02:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:02:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:02:40 [scrapy.extensions.telnet] INFO: Telnet Password: 27dd4ea7ceb8cf24
2019-08-21 14:02:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:02:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:02:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:02:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:02:40 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:02:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:02:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:02:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:02:41 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: book.json
2019-08-21 14:02:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.914053,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 2, 41, 178227),
 'item_scraped_count': 36,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 6, 2, 40, 264174)}
2019-08-21 14:02:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:03:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:03:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:03:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:03:39 [scrapy.extensions.telnet] INFO: Telnet Password: 6cc4edfd4a1413e2
2019-08-21 14:03:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:03:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:03:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:03:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:03:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:03:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:03:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:03:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:03:40 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 14:03:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411006,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.897051,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 3, 40, 374612),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 6, 3, 39, 477561)}
2019-08-21 14:03:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:04:40 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:04:40 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:04:40 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:04:40 [scrapy.extensions.telnet] INFO: Telnet Password: de2cbcecacf93e31
2019-08-21 14:04:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:04:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:04:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:04:40 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:04:40 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:04:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:04:40 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:04:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:04:41 [scrapy.extensions.feedexport] INFO: Stored json feed (16 items) in: book.json
2019-08-21 14:04:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5232,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 411005,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 0.849049,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 4, 41, 130088),
 'item_scraped_count': 16,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 4,
 'request_depth_max': 1,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'start_time': datetime.datetime(2019, 8, 21, 6, 4, 40, 281039)}
2019-08-21 14:04:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:08:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:08:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:08:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:08:39 [scrapy.extensions.telnet] INFO: Telnet Password: 6b62e1d460bf969d
2019-08-21 14:08:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:08:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:08:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:08:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:08:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:08:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:08:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:08:43 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-08-21 14:08:43 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-08-21 14:08:43 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-08-21 14:08:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:08:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:08:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:08:52 [scrapy.extensions.telnet] INFO: Telnet Password: 59dee519bf38f4a4
2019-08-21 14:08:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:08:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:08:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:08:53 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:08:53 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:08:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:08:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_stat
    print("爬虫工作中，正在爬取" + str(eval(response.meta['item'])['player_name']) + "信息。")
TypeError: eval() arg 1 must be a string, bytes or code object
2019-08-21 14:08:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:08:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 403518,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.525259,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 8, 57, 573755),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/TypeError': 40,
 'start_time': datetime.datetime(2019, 8, 21, 6, 8, 53, 48496)}
2019-08-21 14:08:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:09:16 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:09:16 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:09:16 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:09:16 [scrapy.extensions.telnet] INFO: Telnet Password: d1192e93e12cb9d2
2019-08-21 14:09:16 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:09:16 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:09:16 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:09:16 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:09:16 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:09:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:09:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:09:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:09:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 403518,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.448255,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 9, 20, 679077),
 'item_scraped_count': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'start_time': datetime.datetime(2019, 8, 21, 6, 9, 16, 230822)}
2019-08-21 14:09:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:11:58 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:11:58 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:11:58 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:11:58 [scrapy.extensions.telnet] INFO: Telnet Password: e1a5ff7a280cf283
2019-08-21 14:11:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:11:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:11:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:11:58 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:11:58 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:11:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:11:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:12:03 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:12:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 403579,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.764272,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 12, 3, 476388),
 'item_scraped_count': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'start_time': datetime.datetime(2019, 8, 21, 6, 11, 58, 712116)}
2019-08-21 14:12:03 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:12:48 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:12:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:12:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:12:48 [scrapy.extensions.telnet] INFO: Telnet Password: f8e22f01db28822e
2019-08-21 14:12:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:12:48 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:12:48 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:12:48 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:12:48 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:12:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:12:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:12:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:12:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 403575,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.71827,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 12, 53, 106227),
 'item_scraped_count': 33,
 'log_count/ERROR': 7,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/IndexError': 7,
 'start_time': datetime.datetime(2019, 8, 21, 6, 12, 48, 387957)}
2019-08-21 14:12:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:13:54 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:13:54 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:13:54 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:13:54 [scrapy.extensions.telnet] INFO: Telnet Password: ff3a28733a194e88
2019-08-21 14:13:54 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:13:54 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:13:54 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:13:54 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:13:54 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:13:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:13:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:13:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:13:59 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:13:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 402734,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.681267,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 13, 59, 148004),
 'item_scraped_count': 33,
 'log_count/ERROR': 7,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/IndexError': 7,
 'start_time': datetime.datetime(2019, 8, 21, 6, 13, 54, 466737)}
2019-08-21 14:13:59 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:14:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:14:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:14:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:14:59 [scrapy.extensions.telnet] INFO: Telnet Password: 3595086a97f75db2
2019-08-21 14:14:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:14:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:14:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:14:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:14:59 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:14:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:14:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jjredick-1213.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '61.50'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/andreiguodala-642.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '73.25'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '75.08'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/lonzoball-150429.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '49.50'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jahlilokafor-150045.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '36.33'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshhart-150488.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '65.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/frankjackson-150473.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '30.50'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/deanthonymelton-150985.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '50.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandoningram-150164.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '63.33'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/yutawatanabe-151204.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '15.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ivanrabb-150507.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '42.50'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/dillonbrooks-150497.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '50.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brunocaboclo-4947.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '11.57'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/joshjackson-150432.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '78.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/graysonallen-150946.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '38.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jarenjackson-151072.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '58.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaecrowder-3671.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '64.09'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tyusjones-150038.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '65.80'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/kyleanderson-4917.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '62.33'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/solomonhill-4800.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '45.29'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jonasvalanciunas-3558.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '61.50'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/milesplumlee-3663.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '42.18'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '51.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '28.00'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '37.75'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '57.82'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '48.60'
2019-08-21 14:15:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '61.50'
2019-08-21 14:15:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '21.00'
2019-08-21 14:15:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '59.45'
2019-08-21 14:15:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '67.73'
2019-08-21 14:15:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '58.17'
2019-08-21 14:15:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 87, in parse_stat
    item['game_play'] = '职业生涯场均出场'+ str(int(game_play)) + '场'
ValueError: invalid literal for int() with base 10: '76.09'
2019-08-21 14:15:04 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:15:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 402716,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.684268,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 15, 4, 100719),
 'log_count/ERROR': 40,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/IndexError': 7,
 'spider_exceptions/ValueError': 33,
 'start_time': datetime.datetime(2019, 8, 21, 6, 14, 59, 416451)}
2019-08-21 14:15:04 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:17:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:17:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:17:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:17:52 [scrapy.extensions.telnet] INFO: Telnet Password: 344a1329723b6de8
2019-08-21 14:17:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:17:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:17:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:17:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:17:52 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:17:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:17:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:17:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    game_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:17:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:17:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 402776,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.715269,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 17, 57, 153617),
 'item_scraped_count': 33,
 'log_count/ERROR': 7,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/IndexError': 7,
 'start_time': datetime.datetime(2019, 8, 21, 6, 17, 52, 438348)}
2019-08-21 14:17:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:20:47 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:20:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:20:47 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:20:47 [scrapy.extensions.telnet] INFO: Telnet Password: 07d92d6764f75694
2019-08-21 14:20:47 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:20:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:20:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:20:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:20:47 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:20:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:20:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:20:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jaxsonhayes-151702.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/zionwilliamson-151669.html> (referer: https://nba.hupu.com/players/pelicans)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/brandonclarke-151716.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamorant-151686.html> (referer: https://nba.hupu.com/players/grizzlies)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:20:52 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:20:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 12868,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 402188,
 'downloader/response_count': 45,
 'downloader/response_status_count/200': 45,
 'elapsed_time_seconds': 4.745272,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 20, 52, 324637),
 'item_scraped_count': 33,
 'log_count/ERROR': 7,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 45,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 44,
 'scheduler/enqueued/memory': 44,
 'spider_exceptions/IndexError': 7,
 'start_time': datetime.datetime(2019, 8, 21, 6, 20, 47, 579365)}
2019-08-21 14:20:52 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:21:10 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:21:10 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:21:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:21:10 [scrapy.extensions.telnet] INFO: Telnet Password: cdf2c4cb1253e701
2019-08-21 14:21:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:21:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:21:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:21:10 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:21:10 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:21:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:21:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:21:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: None)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 22, in parse
    yield scrapy.Request(url=team_url, meta={'player_team': player_team}, callback=self.parse_detail)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 25, in __init__
    self._set_url(url)
  File "d:\spider_test\venv\lib\site-packages\scrapy\http\request\__init__.py", line 69, in _set_url
    raise ValueError('Missing scheme in request url: %s' % self._url)
ValueError: Missing scheme in request url: h
2019-08-21 14:21:11 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:21:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 447,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 9302,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.251014,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 21, 11, 110711),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 6, 21, 10, 859697)}
2019-08-21 14:21:11 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:21:24 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:21:24 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:21:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:21:24 [scrapy.extensions.telnet] INFO: Telnet Password: 20d608ff5e780647
2019-08-21 14:21:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:21:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:21:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:21:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:21:24 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:21:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:21:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:21:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:21:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:21:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:21:26 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:21:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 160192,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.980113,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 21, 26, 413586),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 21, 24, 433473)}
2019-08-21 14:21:26 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:23:33 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:23:33 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:23:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:23:33 [scrapy.extensions.telnet] INFO: Telnet Password: 50bfa179e847cffa
2019-08-21 14:23:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:23:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:23:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:23:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:23:33 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:23:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:23:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:23:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:23:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:23:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:23:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:23:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 160218,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.989114,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 23, 35, 324960),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 23, 33, 335846)}
2019-08-21 14:23:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:24:38 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:24:38 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:24:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:24:38 [scrapy.extensions.telnet] INFO: Telnet Password: c50731f275d07f91
2019-08-21 14:24:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:24:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:24:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:24:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:24:38 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:24:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:24:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:24:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:24:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:24:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:24:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:24:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 160800,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.947111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 24, 40, 633695),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 24, 38, 686584)}
2019-08-21 14:24:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:26:29 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:26:29 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:26:29 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:26:29 [scrapy.extensions.telnet] INFO: Telnet Password: e78f71b2189e2fdc
2019-08-21 14:26:29 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:26:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:26:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:26:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:26:29 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:26:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:26:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:26:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:26:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:26:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:26:31 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:26:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 160815,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.968112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 26, 31, 355028),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 26, 29, 386916)}
2019-08-21 14:26:31 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:27:45 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:27:45 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:27:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'nba.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:27:46 [scrapy.extensions.telnet] INFO: Telnet Password: 9d3fdb8e4e22a590
2019-08-21 14:27:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:27:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:27:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:27:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:27:46 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:27:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:27:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:27:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:27:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:27:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:27:48 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:27:48 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: nba.json
2019-08-21 14:27:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 160796,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.942111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 27, 48, 146420),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 27, 46, 204309)}
2019-08-21 14:27:48 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:34:50 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:34:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:34:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:34:50 [scrapy.extensions.telnet] INFO: Telnet Password: cf5af05dfaa4171b
2019-08-21 14:34:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:34:51 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:34:51 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:34:51 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:34:51 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:34:51 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:34:51 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:34:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    if response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]:
IndexError: list index out of range
2019-08-21 14:34:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    if response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]:
IndexError: list index out of range
2019-08-21 14:34:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    if response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]:
IndexError: list index out of range
2019-08-21 14:34:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:34:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161099,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.032116,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 34, 53, 78725),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 34, 51, 46609)}
2019-08-21 14:34:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:35:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:35:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:35:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:35:23 [scrapy.extensions.telnet] INFO: Telnet Password: 1c8690fac3852a2d
2019-08-21 14:35:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:35:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:35:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:35:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:35:23 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:35:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:35:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:35:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:35:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:35:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 88, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:35:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:35:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161314,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.93311,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 35, 25, 341570),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 35, 23, 408460)}
2019-08-21 14:35:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:36:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:36:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:36:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:36:23 [scrapy.extensions.telnet] INFO: Telnet Password: 0c2aaee1b41bcfa4
2019-08-21 14:36:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:36:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:36:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:36:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:36:23 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:36:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:36:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:36:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:36:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:36:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:36:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:36:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161294,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.952112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 36, 25, 865032),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 36, 23, 912920)}
2019-08-21 14:36:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:39:59 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:39:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:39:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:39:59 [scrapy.extensions.telnet] INFO: Telnet Password: 9e00ec7e7e1e6d90
2019-08-21 14:39:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:39:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:39:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:39:59 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:39:59 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:39:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:39:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:40:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:40:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:40:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 85, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:40:01 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:40:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161306,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.959112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 40, 1, 682376),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 39, 59, 723264)}
2019-08-21 14:40:01 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:41:20 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:41:20 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:41:20 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:41:20 [scrapy.extensions.telnet] INFO: Telnet Password: 99bfa2dcd3d408a0
2019-08-21 14:41:20 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:41:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:41:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:41:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:41:21 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:41:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:41:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]'.extract()))
AttributeError: 'str' object has no attribute 'extract'
2019-08-21 14:41:22 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:41:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161357,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.911109,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 41, 22, 941024),
 'log_count/ERROR': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/AttributeError': 15,
 'start_time': datetime.datetime(2019, 8, 21, 6, 41, 21, 29915)}
2019-08-21 14:41:22 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:42:06 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:42:06 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:42:06 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:42:06 [scrapy.extensions.telnet] INFO: Telnet Password: a29e7825e1dda575
2019-08-21 14:42:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:42:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:42:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:42:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:42:07 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:42:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:42:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0])
IndexError: list index out of range
2019-08-21 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0])
IndexError: list index out of range
2019-08-21 14:42:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_stat
    print(response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0])
IndexError: list index out of range
2019-08-21 14:42:09 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:42:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161391,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.954112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 42, 9, 94664),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 42, 7, 140552)}
2019-08-21 14:42:09 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:42:26 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:42:26 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:42:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:42:26 [scrapy.extensions.telnet] INFO: Telnet Password: a2b42599767130ef
2019-08-21 14:42:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:42:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:42:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:42:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:42:26 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:42:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:42:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:42:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:42:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:42:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 86, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:42:28 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:42:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161384,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.060118,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 42, 28, 708786),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 42, 26, 648668)}
2019-08-21 14:42:28 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:43:49 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:43:49 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:43:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:43:49 [scrapy.extensions.telnet] INFO: Telnet Password: 2178b1fb6c9ec0ab
2019-08-21 14:43:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:43:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:43:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:43:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:43:49 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:43:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:43:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:43:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:43:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:43:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:43:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:43:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161373,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.010115,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 43, 51, 725534),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 43, 49, 715419)}
2019-08-21 14:43:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:44:21 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:44:21 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:44:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:44:21 [scrapy.extensions.telnet] INFO: Telnet Password: 3544680eeea4c683
2019-08-21 14:44:21 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:44:21 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:44:21 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:44:21 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:44:21 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:44:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:44:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:44:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:44:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:44:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:44:23 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:44:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161373,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 1.990113,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 44, 23, 625358),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 44, 21, 635245)}
2019-08-21 14:44:23 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:50:08 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:50:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:50:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:50:08 [scrapy.extensions.telnet] INFO: Telnet Password: a474da7a20708700
2019-08-21 14:50:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:50:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:50:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:50:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:50:08 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:50:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:50:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:50:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:50:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:50:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 89, in parse_stat
    career_minute = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[2]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 14:50:10 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:50:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 160169,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.116121,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 50, 10, 990227),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 6, 50, 8, 874106)}
2019-08-21 14:50:10 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:50:38 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:50:38 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:50:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:50:38 [scrapy.extensions.telnet] INFO: Telnet Password: 8a5bb81939bd27c4
2019-08-21 14:50:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:50:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:50:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:50:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:50:38 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:50:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:50:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:50:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:50:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136030,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.944111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 50, 40, 314904),
 'item_scraped_count': 12,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 6, 50, 38, 370793)}
2019-08-21 14:50:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:52:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:52:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:52:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:52:39 [scrapy.extensions.telnet] INFO: Telnet Password: fb87046f7c830f0c
2019-08-21 14:52:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:52:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:52:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:52:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:52:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:52:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:52:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:52:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_detail
    item['career_play'] = '该球员为新秀，从未上场过。'
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: career_play'
2019-08-21 14:52:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:52:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136031,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.91611,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 52, 41, 91812),
 'item_scraped_count': 12,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 6, 52, 39, 175702)}
2019-08-21 14:52:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:53:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:53:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:53:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'nba.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:53:32 [scrapy.extensions.telnet] INFO: Telnet Password: 75f21ed4fba71556
2019-08-21 14:53:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:53:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:53:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:53:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:53:32 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:53:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:53:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:53:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 83, in parse_detail
    item['career_play'] = '该球员为新秀，从未上场过。'
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: career_play'
2019-08-21 14:53:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:53:34 [scrapy.extensions.feedexport] INFO: Stored json feed (12 items) in: nba.json
2019-08-21 14:53:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136030,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.865106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 53, 34, 648875),
 'item_scraped_count': 12,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 6, 53, 32, 783769)}
2019-08-21 14:53:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:54:28 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:54:28 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:54:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:54:28 [scrapy.extensions.telnet] INFO: Telnet Password: 64d4ef33c2569e7d
2019-08-21 14:54:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:54:29 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:54:29 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:54:29 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:54:29 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:54:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:54:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:54:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_detail
    item['career_play'] = '该球员为新秀，从未上场过。'
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: career_play'
2019-08-21 14:54:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:54:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136013,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.948111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 54, 30, 965096),
 'item_scraped_count': 12,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 6, 54, 29, 16985)}
2019-08-21 14:54:30 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:56:23 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:56:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:56:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:56:23 [scrapy.extensions.telnet] INFO: Telnet Password: 4dccad5b8a149c3e
2019-08-21 14:56:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:56:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:56:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:56:23 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:56:23 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:56:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:56:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:56:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:56:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137530,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.875107,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 56, 25, 652656),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 6, 56, 23, 777549)}
2019-08-21 14:56:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 14:56:43 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 14:56:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 14:56:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 14:56:43 [scrapy.extensions.telnet] INFO: Telnet Password: c0d22dcbf4d32f8e
2019-08-21 14:56:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 14:56:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 14:56:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 14:56:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 14:56:43 [scrapy.core.engine] INFO: Spider opened
2019-08-21 14:56:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 14:56:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 14:56:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 14:56:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137530,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.882108,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 6, 56, 45, 702803),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 6, 56, 43, 820695)}
2019-08-21 14:56:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:00:03 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:00:03 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:00:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:00:03 [scrapy.extensions.telnet] INFO: Telnet Password: 001e3e3b74dd47e6
2019-08-21 15:00:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:00:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:00:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:00:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:00:03 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:00:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:00:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:00:12 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:00:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 78674,
 'downloader/request_count': 264,
 'downloader/request_method_count/GET': 264,
 'downloader/response_bytes': 6299687,
 'downloader/response_count': 264,
 'downloader/response_status_count/200': 264,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 8.303474,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 0, 12, 18603),
 'item_scraped_count': 249,
 'log_count/INFO': 10,
 'offsite/domains': 1,
 'offsite/filtered': 31,
 'request_depth_max': 14,
 'response_received_count': 264,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 263,
 'scheduler/dequeued/memory': 263,
 'scheduler/enqueued': 263,
 'scheduler/enqueued/memory': 263,
 'start_time': datetime.datetime(2019, 8, 21, 7, 0, 3, 715129)}
2019-08-21 15:00:12 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:00:28 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:00:28 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:00:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'book.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:00:28 [scrapy.extensions.telnet] INFO: Telnet Password: 32bdfe34044e62ba
2019-08-21 15:00:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:00:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:00:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:00:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:00:28 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:00:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:00:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:00:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:00:40 [scrapy.extensions.feedexport] INFO: Stored json feed (249 items) in: book.json
2019-08-21 15:00:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 78674,
 'downloader/request_count': 264,
 'downloader/request_method_count/GET': 264,
 'downloader/response_bytes': 6299699,
 'downloader/response_count': 264,
 'downloader/response_status_count/200': 264,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 11.816676,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 0, 40, 795249),
 'item_scraped_count': 249,
 'log_count/INFO': 11,
 'offsite/domains': 1,
 'offsite/filtered': 31,
 'request_depth_max': 14,
 'response_received_count': 264,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 263,
 'scheduler/dequeued/memory': 263,
 'scheduler/enqueued': 263,
 'scheduler/enqueued/memory': 263,
 'start_time': datetime.datetime(2019, 8, 21, 7, 0, 28, 978573)}
2019-08-21 15:00:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:17:45 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:17:45 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:17:45 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:17:45 [scrapy.extensions.telnet] INFO: Telnet Password: 28fcf2226d5eb573
2019-08-21 15:17:45 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:17:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:17:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:17:45 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:17:45 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:17:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:17:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:17:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_detail
    item['career_play'] = {'该球员为新秀，从未上场过。'}
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: career_play'
2019-08-21 15:17:47 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:17:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137459,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.936111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 17, 47, 858994),
 'item_scraped_count': 12,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 7, 17, 45, 922883)}
2019-08-21 15:17:47 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:18:12 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:18:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:18:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:18:12 [scrapy.extensions.telnet] INFO: Telnet Password: 8d9108e6f136808d
2019-08-21 15:18:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:18:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:18:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:18:12 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:18:12 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:18:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:18:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:18:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 84, in parse_detail
    item['career_play'] = player_salary
  File "d:\spider_test\venv\lib\site-packages\scrapy\item.py", line 73, in __setitem__
    (self.__class__.__name__, key))
KeyError: 'NBA_Item does not support field: career_play'
2019-08-21 15:18:14 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:18:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137459,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.857106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 18, 14, 213501),
 'item_scraped_count': 12,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'spider_exceptions/KeyError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 7, 18, 12, 356395)}
2019-08-21 15:18:14 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:19:05 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:19:05 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:19:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:19:05 [scrapy.extensions.telnet] INFO: Telnet Password: 18b13005210de893
2019-08-21 15:19:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:19:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:19:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:19:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:19:05 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:19:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:19:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:19:07 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:19:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137459,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.988114,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 19, 7, 409544),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 19, 5, 421430)}
2019-08-21 15:19:07 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:20:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:20:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:20:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:20:56 [scrapy.extensions.telnet] INFO: Telnet Password: 5974d378d0145d51
2019-08-21 15:20:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:20:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:20:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:20:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:20:57 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:20:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:20:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:20:59 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:20:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137438,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.952111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 20, 59, 63930),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 20, 57, 111819)}
2019-08-21 15:20:59 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:24:15 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:24:15 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:24:15 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:24:15 [scrapy.extensions.telnet] INFO: Telnet Password: fbba10890bce7394
2019-08-21 15:24:15 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:24:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:24:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:24:15 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:24:15 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:24:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:24:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:24:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:24:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137445,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.989114,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 24, 17, 612287),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 24, 15, 623173)}
2019-08-21 15:24:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:26:38 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:26:38 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:26:38 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:26:38 [scrapy.extensions.telnet] INFO: Telnet Password: 0e2f3c51ed81261f
2019-08-21 15:26:38 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:26:38 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:26:38 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:26:38 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:26:38 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:26:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:26:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:26:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:26:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137452,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.817104,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 26, 40, 587464),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 26, 38, 770360)}
2019-08-21 15:26:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:27:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:27:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:27:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:27:32 [scrapy.extensions.telnet] INFO: Telnet Password: 2e580b95486b5c91
2019-08-21 15:27:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:27:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:27:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:27:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:27:32 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:27:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:27:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:27:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:27:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137434,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.856106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 27, 34, 226532),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 27, 32, 370426)}
2019-08-21 15:27:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:27:42 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:27:42 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:27:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:27:42 [scrapy.extensions.telnet] INFO: Telnet Password: 624b24fa2865b17f
2019-08-21 15:27:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:27:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:27:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:27:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:27:42 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:27:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:27:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:27:44 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:27:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137434,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.828104,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 27, 44, 509120),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 27, 42, 681016)}
2019-08-21 15:27:44 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:33:08 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:33:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:33:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:33:08 [scrapy.extensions.telnet] INFO: Telnet Password: 15a3cc25bce27d54
2019-08-21 15:33:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:33:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:33:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:33:08 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:33:08 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:33:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:33:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:33:10 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:33:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137430,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.899109,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 33, 10, 686777),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 33, 8, 787668)}
2019-08-21 15:33:10 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:34:00 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:34:00 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:34:00 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:34:00 [scrapy.extensions.telnet] INFO: Telnet Password: 5ccbda2010522840
2019-08-21 15:34:00 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:34:00 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:34:00 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:34:00 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:34:00 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:34:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:34:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:34:02 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:34:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137414,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.850106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 34, 2, 715753),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 34, 0, 865647)}
2019-08-21 15:34:02 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:34:32 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:34:32 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:34:32 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:34:32 [scrapy.extensions.telnet] INFO: Telnet Password: 3f65780412dbedbd
2019-08-21 15:34:32 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:34:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:34:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:34:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:34:32 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:34:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:34:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:34:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:34:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137414,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.862107,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 34, 34, 330561),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 34, 32, 468454)}
2019-08-21 15:34:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:38:26 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:38:26 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:38:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:38:26 [scrapy.extensions.telnet] INFO: Telnet Password: e831cd6c32b00330
2019-08-21 15:38:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:38:26 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:38:26 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:38:26 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:38:26 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:38:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:38:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:38:28 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:38:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137428,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.864107,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 38, 28, 450952),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 38, 26, 586845)}
2019-08-21 15:38:28 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:38:48 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:38:48 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:38:48 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:38:48 [scrapy.extensions.telnet] INFO: Telnet Password: da6193e927617009
2019-08-21 15:38:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:38:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:38:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:38:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:38:49 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:38:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:38:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:38:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:38:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137428,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.91111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 38, 51, 16243),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 38, 49, 105133)}
2019-08-21 15:38:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:41:18 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:41:18 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:41:18 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:41:18 [scrapy.extensions.telnet] INFO: Telnet Password: c93f42fd11b4a7d2
2019-08-21 15:41:18 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:41:18 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:41:18 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:41:18 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:41:18 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:41:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:41:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/shamorieponds-151745.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 94, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 15:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/williammcdowellwhite-151879.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 94, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 15:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/chrisclemons-151877.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 94, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 15:41:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:41:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5059,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 161418,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'elapsed_time_seconds': 2.034116,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 41, 20, 665802),
 'item_scraped_count': 12,
 'log_count/ERROR': 3,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 18,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 17,
 'scheduler/dequeued/memory': 17,
 'scheduler/enqueued': 17,
 'scheduler/enqueued/memory': 17,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2019, 8, 21, 7, 41, 18, 631686)}
2019-08-21 15:41:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:41:56 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:41:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:41:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:41:56 [scrapy.extensions.telnet] INFO: Telnet Password: aa3546eceedc619e
2019-08-21 15:41:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:41:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:41:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:41:56 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:41:56 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:41:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:41:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:41:58 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:41:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 137429,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.91211,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 41, 58, 614973),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 41, 56, 702863)}
2019-08-21 15:41:58 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:43:07 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:43:07 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:43:07 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:43:07 [scrapy.extensions.telnet] INFO: Telnet Password: d8e8d339c3c821aa
2019-08-21 15:43:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:43:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:43:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:43:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:43:07 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:43:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:43:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:43:09 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:43:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136594,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.853106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 43, 9, 248013),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 43, 7, 394907)}
2019-08-21 15:43:09 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:45:46 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:45:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:45:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:45:46 [scrapy.extensions.telnet] INFO: Telnet Password: 082f8835640b96ff
2019-08-21 15:45:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:45:46 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:45:46 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:45:46 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:45:46 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:45:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:45:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:45:48 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:45:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136592,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.92411,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 45, 48, 174103),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 45, 46, 249993)}
2019-08-21 15:45:48 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:50:22 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:50:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:50:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:50:22 [scrapy.extensions.telnet] INFO: Telnet Password: e3051b23500b3857
2019-08-21 15:50:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:50:22 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:50:22 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:50:22 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:50:22 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:50:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:50:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:50:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/rockets> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 79, in parse_detail
    print(i + player_contract)
TypeError: unsupported operand type(s) for +: 'int' and 'str'
2019-08-21 15:50:24 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:50:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17573,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.713097,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 50, 24, 289895),
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 7, 50, 22, 576798)}
2019-08-21 15:50:24 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:50:43 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:50:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:50:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:50:43 [scrapy.extensions.telnet] INFO: Telnet Password: 54ef23072a4fbe20
2019-08-21 15:50:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:50:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:50:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:50:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:50:43 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:50:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:50:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:50:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:50:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136179,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.853106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 50, 45, 92085),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 50, 43, 238979)}
2019-08-21 15:50:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:52:43 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:52:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:52:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:52:43 [scrapy.extensions.telnet] INFO: Telnet Password: e92c64473bed6cf6
2019-08-21 15:52:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:52:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:52:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:52:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:52:43 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:52:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:52:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:52:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:52:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4180,
 'downloader/request_count': 15,
 'downloader/request_method_count/GET': 15,
 'downloader/response_bytes': 136546,
 'downloader/response_count': 15,
 'downloader/response_status_count/200': 15,
 'elapsed_time_seconds': 1.838106,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 52, 45, 243958),
 'item_scraped_count': 15,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 15,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 14,
 'scheduler/dequeued/memory': 14,
 'scheduler/enqueued': 14,
 'scheduler/enqueued/memory': 14,
 'start_time': datetime.datetime(2019, 8, 21, 7, 52, 43, 405852)}
2019-08-21 15:52:45 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:55:13 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:55:13 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:55:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:55:13 [scrapy.extensions.telnet] INFO: Telnet Password: 193e09b93e40e736
2019-08-21 15:55:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:55:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:55:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:55:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:55:13 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:55:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:55:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:55:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:55:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17571,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.469084,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 55, 15, 320541),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 7, 55, 13, 851457)}
2019-08-21 15:55:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:56:27 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:56:27 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:56:27 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:56:27 [scrapy.extensions.telnet] INFO: Telnet Password: 6be1563c18f59b57
2019-08-21 15:56:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:56:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:56:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:56:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:56:27 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:56:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:56:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:56:29 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:56:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17573,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.456083,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 56, 29, 62759),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 7, 56, 27, 606676)}
2019-08-21 15:56:29 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:57:13 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:57:13 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:57:13 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:57:13 [scrapy.extensions.telnet] INFO: Telnet Password: 73d1ed209f990c65
2019-08-21 15:57:13 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:57:13 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:57:13 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:57:13 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:57:13 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:57:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:57:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:57:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:57:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17573,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.434082,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 57, 15, 318405),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 7, 57, 13, 884323)}
2019-08-21 15:57:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:58:02 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:58:02 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:58:02 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:58:02 [scrapy.extensions.telnet] INFO: Telnet Password: f0cb9873e1a97ad9
2019-08-21 15:58:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:58:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:58:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:58:02 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:58:02 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:58:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:58:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:58:04 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:58:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17577,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.456084,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 58, 4, 373211),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 7, 58, 2, 917127)}
2019-08-21 15:58:04 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 15:59:19 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 15:59:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 15:59:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 15:59:19 [scrapy.extensions.telnet] INFO: Telnet Password: b2f56f211fd2aaac
2019-08-21 15:59:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 15:59:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 15:59:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 15:59:19 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 15:59:19 [scrapy.core.engine] INFO: Spider opened
2019-08-21 15:59:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 15:59:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 15:59:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 15:59:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17577,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.429082,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 7, 59, 20, 992593),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 7, 59, 19, 563511)}
2019-08-21 15:59:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:00:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:00:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:00:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:00:39 [scrapy.extensions.telnet] INFO: Telnet Password: bc91911c69c31dc8
2019-08-21 16:00:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:00:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:00:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:00:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:00:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:00:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:00:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:00:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:00:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17577,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.451083,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 0, 40, 997169),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 8, 0, 39, 546086)}
2019-08-21 16:00:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:01:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:01:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:01:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:01:39 [scrapy.extensions.telnet] INFO: Telnet Password: e11c9bdfd399c8ef
2019-08-21 16:01:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:01:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:01:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:01:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:01:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:01:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:01:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:01:40 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:01:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 720,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 17577,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'elapsed_time_seconds': 1.387079,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 1, 40, 650581),
 'log_count/INFO': 10,
 'request_depth_max': 1,
 'response_received_count': 3,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2019, 8, 21, 8, 1, 39, 263502)}
2019-08-21 16:01:40 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:08:05 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:08:05 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:08:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:08:05 [scrapy.extensions.telnet] INFO: Telnet Password: f0d5e2cbd45e81c7
2019-08-21 16:08:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:08:05 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:08:05 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:08:05 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:08:05 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:08:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:08:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:08:08 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 95, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:08:08 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:08:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153697,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 2.025116,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 8, 8, 25738),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 8, 6, 622)}
2019-08-21 16:08:08 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:10:26 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:10:26 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:10:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:10:26 [scrapy.extensions.telnet] INFO: Telnet Password: 37715e55d42124a3
2019-08-21 16:10:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:10:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:10:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:10:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:10:27 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:10:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:10:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:10:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 98, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:10:29 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:10:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153336,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.960112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 10, 29, 73805),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 10, 27, 113693)}
2019-08-21 16:10:29 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:11:24 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:11:24 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:11:24 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:11:24 [scrapy.extensions.telnet] INFO: Telnet Password: 629d339564f0664b
2019-08-21 16:11:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:11:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:11:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:11:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:11:24 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:11:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:11:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:11:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 101, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:11:26 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:11:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153336,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.973113,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 11, 26, 880112),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 11, 24, 906999)}
2019-08-21 16:11:26 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:14:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:14:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:14:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:14:39 [scrapy.extensions.telnet] INFO: Telnet Password: 91ae12ccffbc7cff
2019-08-21 16:14:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:14:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:14:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:14:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:14:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:14:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:14:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:14:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 104, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:14:41 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:14:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153620,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.960112,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 14, 41, 757258),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 14, 39, 797146)}
2019-08-21 16:14:41 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:15:33 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:15:33 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:15:33 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:15:33 [scrapy.extensions.telnet] INFO: Telnet Password: c113b4767cc101f7
2019-08-21 16:15:33 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:15:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:15:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:15:33 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:15:33 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:15:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:15:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:15:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 104, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:15:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:15:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153620,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.919109,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 15, 35, 369324),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 15, 33, 450215)}
2019-08-21 16:15:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:17:41 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:17:41 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:17:41 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:17:41 [scrapy.extensions.telnet] INFO: Telnet Password: 19a15a963e267909
2019-08-21 16:17:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:17:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:17:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:17:41 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:17:41 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:17:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:17:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:17:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 101, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:17:43 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:17:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153633,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.937111,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 17, 43, 537655),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 17, 41, 600544)}
2019-08-21 16:17:43 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:24:03 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:24:03 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:24:03 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:24:03 [scrapy.extensions.telnet] INFO: Telnet Password: c75c1e7085fcb2c4
2019-08-21 16:24:03 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:24:04 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:24:04 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:24:04 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:24:04 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:24:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:24:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:24:05 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 101, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:24:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153705,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.897108,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 24, 5, 926526),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 24, 4, 29418)}
2019-08-21 16:24:05 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:24:30 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:24:30 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:24:30 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:24:30 [scrapy.extensions.telnet] INFO: Telnet Password: 1b448f0d0dad3874
2019-08-21 16:24:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:24:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:24:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:24:30 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:24:30 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:24:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:24:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:24:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 101, in parse_stat
    career_play = response.xpath('//*[@id="in_box"]/div/div[1]/table[1]/tbody/tr[3]/td[1]/text()').extract()[0]
IndexError: list index out of range
2019-08-21 16:24:32 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:24:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153705,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.892109,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 24, 32, 802064),
 'item_scraped_count': 18,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 24, 30, 909955)}
2019-08-21 16:24:32 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:24:52 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:24:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:24:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:24:52 [scrapy.extensions.telnet] INFO: Telnet Password: bf7cc2407d82828a
2019-08-21 16:24:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:24:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:24:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:24:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:24:52 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:24:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:24:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/russellwestbrook-3016.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/danuelhouse-150386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/austinrivers-3647.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/garyclark-151168.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/tysonchandler-326.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/anthonybennett-4786.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/isaiahhartenstein-151386.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/geraldgreen-1028.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/pjtucker-1258.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/benmclemore-4826.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/clintcapela-4908.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/jamesharden-3306.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/michaelfrazier-151698.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 111, in parse_stat
    print(item)
UnboundLocalError: local variable 'item' referenced before assignment
2019-08-21 16:24:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nba.hupu.com/players/ericgordon-3015.html> (referer: https://nba.hupu.com/players/rockets)
Traceback (most recent call last):
  File "d:\spider_test\venv\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\spider_test\venv\lib\site-packages\scrapy\core\spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "d:\spider_test\venv\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\spider_test\mySpider\mySpider\spiders\nba_spider.py", line 107, in parse_stat
    item['career_play'] = '职业生涯每赛季场均出场'+ str(int(float(career_play))) + '场'
TypeError: float() argument must be a string or a number, not 'list'
2019-08-21 16:24:54 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:24:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153705,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 1.883108,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 24, 54, 245290),
 'item_scraped_count': 5,
 'log_count/ERROR': 14,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'spider_exceptions/TypeError': 13,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2019, 8, 21, 8, 24, 52, 362182)}
2019-08-21 16:24:54 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 16:27:49 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 16:27:49 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 16:27:49 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 16:27:49 [scrapy.extensions.telnet] INFO: Telnet Password: e70e61141f0adcd9
2019-08-21 16:27:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 16:27:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 16:27:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 16:27:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 16:27:49 [scrapy.core.engine] INFO: Spider opened
2019-08-21 16:27:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 16:27:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 16:27:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 16:27:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153724,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 2.256129,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 8, 27, 51, 678439),
 'item_scraped_count': 19,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'start_time': datetime.datetime(2019, 8, 21, 8, 27, 49, 422310)}
2019-08-21 16:27:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 17:00:42 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 17:00:42 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 17:00:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 17:00:42 [scrapy.extensions.telnet] INFO: Telnet Password: bf16348ded54e5ae
2019-08-21 17:00:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 17:00:42 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 17:00:42 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 17:00:42 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 17:00:42 [scrapy.core.engine] INFO: Spider opened
2019-08-21 17:00:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 17:00:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 17:00:44 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 17:00:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153873,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 2.099121,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 9, 0, 44, 672288),
 'item_scraped_count': 19,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'start_time': datetime.datetime(2019, 8, 21, 9, 0, 42, 573167)}
2019-08-21 17:00:44 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 17:03:35 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 17:03:35 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 17:03:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 17:03:35 [scrapy.extensions.telnet] INFO: Telnet Password: 65f539f082e7dc45
2019-08-21 17:03:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 17:03:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 17:03:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 17:03:35 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 17:03:35 [scrapy.core.engine] INFO: Spider opened
2019-08-21 17:03:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 17:03:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 17:03:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 17:03:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 4759,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 153800,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'elapsed_time_seconds': 2.065118,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 9, 3, 37, 744187),
 'item_scraped_count': 19,
 'log_count/INFO': 10,
 'request_depth_max': 2,
 'response_received_count': 17,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'start_time': datetime.datetime(2019, 8, 21, 9, 3, 35, 679069)}
2019-08-21 17:03:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 17:04:26 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 17:04:26 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 17:04:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'csv', 'FEED_URI': 'nba.csv', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 17:04:26 [scrapy.extensions.telnet] INFO: Telnet Password: 75575d11f8bde0b3
2019-08-21 17:04:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 17:04:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 17:04:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 17:04:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 17:04:27 [scrapy.core.engine] INFO: Spider opened
2019-08-21 17:04:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 17:04:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 17:05:17 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 17:05:17 [scrapy.extensions.feedexport] INFO: Stored csv feed (527 items) in: nba.csv
2019-08-21 17:05:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 150627,
 'downloader/request_count': 524,
 'downloader/request_method_count/GET': 524,
 'downloader/response_bytes': 4767898,
 'downloader/response_count': 524,
 'downloader/response_status_count/200': 524,
 'elapsed_time_seconds': 50.232873,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 9, 5, 17, 385886),
 'item_scraped_count': 527,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 524,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 523,
 'scheduler/dequeued/memory': 523,
 'scheduler/enqueued': 523,
 'scheduler/enqueued/memory': 523,
 'start_time': datetime.datetime(2019, 8, 21, 9, 4, 27, 153013)}
2019-08-21 17:05:17 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 17:09:46 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 17:09:46 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 17:09:46 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'csv', 'FEED_URI': 'nba.csv', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 17:09:46 [scrapy.extensions.telnet] INFO: Telnet Password: 89a78a40c8073c72
2019-08-21 17:09:46 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 17:09:47 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 17:09:47 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 17:09:47 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 17:09:47 [scrapy.core.engine] INFO: Spider opened
2019-08-21 17:09:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 17:09:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 17:10:37 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 17:10:37 [scrapy.extensions.feedexport] INFO: Stored csv feed (527 items) in: nba.csv
2019-08-21 17:10:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 150627,
 'downloader/request_count': 524,
 'downloader/request_method_count/GET': 524,
 'downloader/response_bytes': 4760372,
 'downloader/response_count': 524,
 'downloader/response_status_count/200': 524,
 'elapsed_time_seconds': 50.434885,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 9, 10, 37, 487195),
 'item_scraped_count': 527,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 524,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 523,
 'scheduler/dequeued/memory': 523,
 'scheduler/enqueued': 523,
 'scheduler/enqueued/memory': 523,
 'start_time': datetime.datetime(2019, 8, 21, 9, 9, 47, 52310)}
2019-08-21 17:10:37 [scrapy.core.engine] INFO: Spider closed (finished)
2019-08-21 17:18:39 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: mySpider)
2019-08-21 17:18:39 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-7-6.1.7601-SP1
2019-08-21 17:18:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'mySpider', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_FORMAT': 'json', 'FEED_URI': 'nba.json', 'LOG_FILE': 'ChinaPubSpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'mySpider.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['mySpider.spiders']}
2019-08-21 17:18:39 [scrapy.extensions.telnet] INFO: Telnet Password: 8c871ba53750f3e8
2019-08-21 17:18:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2019-08-21 17:18:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-08-21 17:18:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-08-21 17:18:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-08-21 17:18:39 [scrapy.core.engine] INFO: Spider opened
2019-08-21 17:18:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-08-21 17:18:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-08-21 17:19:30 [scrapy.core.engine] INFO: Closing spider (finished)
2019-08-21 17:19:30 [scrapy.extensions.feedexport] INFO: Stored json feed (527 items) in: nba.json
2019-08-21 17:19:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 150627,
 'downloader/request_count': 524,
 'downloader/request_method_count/GET': 524,
 'downloader/response_bytes': 4773712,
 'downloader/response_count': 524,
 'downloader/response_status_count/200': 524,
 'elapsed_time_seconds': 50.52689,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 8, 21, 9, 19, 30, 449678),
 'item_scraped_count': 527,
 'log_count/INFO': 11,
 'request_depth_max': 2,
 'response_received_count': 524,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 523,
 'scheduler/dequeued/memory': 523,
 'scheduler/enqueued': 523,
 'scheduler/enqueued/memory': 523,
 'start_time': datetime.datetime(2019, 8, 21, 9, 18, 39, 922788)}
2019-08-21 17:19:30 [scrapy.core.engine] INFO: Spider closed (finished)
